5353
resilience :  the capacity to withstand or to recover quickly from difficulties
              the ability of something to return to its original size and shape after being compressed or deformed
              To increase the resiliency of the application the solutions architect can use Auto Scaling groups to launch and terminate instances across
		  multiple availability zones based on demand. An application load balancer (ALB) can be used to direct traffic to the web application 
persistent :  continuing to exist or occur over a prolonged period
optimized :   make the best or most effective use of (a situation or resource).
circumvent :  find a way around ; overcome (a problem or difficulty), typically in a clever and surreptitious way.
proprietary : held as property of a private owner ;  Proprietary File System  = EBS
Remediate : process of improving or correcting a situation ; restore by reversing or stopping environmental damage.
Leverage : to use something that you already have in order to achieve something new or better
concurrent : operating or occurring at the same time : running parallel : (of two or more prison sentences) to be served at the same time.


*********************************************************************************************************************************************************
Path :  dns server (route53) - cloudfront (edge location) (- global accelerator) -APi gateway(contains lambda) or custom origin (  s3 or ELB ) - ELB(ec2's)
-global accelerator
- throttling
- DAX
- Dynamodb
- STS 
- AWS datapipeline :

- OPSworks  : 
 AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. AWS OpsWorks for Chef Automate is a fully-managed
 configuration management service that hosts Chef Automate, a suite of automation tools from Chef for configuration management, compliance and security,
 and continuous deployment.OpsWorks for Chef Automate is completely compatible with tooling and cookbooks from the Chef community and automatically 
 registers new nodes with your Chef server.

 Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. OpsWorks lets you use Chef and Puppet 
  to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments. OpsWorks has three  
  offerings, AWS Opsworks for Chef Automate, AWS OpsWorks for Puppet Enterprise, and AWS OpsWorks Stacks.
 
- cloudformation
  AWS CloudFormation uses templates to deploy infrastructure as code. It is not a PaaS service like Elastic Beanstalk and is more focused on infrastructure
  than applications and management of applications.

- AWS Serverless Application Model (AWS SAM) is an extension of AWS CloudFormation that is used to package, test, and deploy serverless applications.

- AWS IoT Core is a managed cloud service that lets connected devices easily and securely interact with cloud applications and other devices. AWS IoT Core can support billions of devices and trillions of messages, and can process and route those messages to AWS endpoints and to other devices reliably and securely.

- Enhanced networking provides higher bandwidth, higher packet-per-second (PPS) performance, and consistently lower inter-instance latencies. If your packets-per-second rate appears to have reached its ceiling, you should consider moving to enhanced networking because you have likely reached the upper thresholds of the VIF driver



-  the application will use a replicated database between both regions with synchronized data : to replicate table data between multi Regions = Use Amazon
   DynamoDB global tables.
- we can use client side script(static) websites in s3. not server side scripts(dynamic)
- concurrently access , strong data consistency and file locking” = Amazon EFS

- codedeploy :
 AWS CodeDeploy is a deployment service that automates application (not infrastructure) deployments to Amazon EC2 instances, on-premises
 instances, or serverless Lambda functions. This would be a good fit if we were talking about an application environment where code changes need to be
 managed but not for infrastructure services

- customer gateway

  The only single point of failure in this architecture is the customer gateway device in the on-premises data center. A customer gateway device is the 
  on-premises (client) side of the connection into the VPC. The customer gateway configuration is created within AWS, but the actual device is a physical
  or virtual device running in the on-premises data center. If this device is a single device, then if it fails the VPN connections will fail. The AWS side
  of the VPN link is the virtual private gateway, and this is a redundant device


- virtual private gateway : 
   An organization is extending a secure development environment into AWS. They have already secured the VPC including removing the Internet Gateway and
   setting up a Direct Connect connection. What else needs to be done to add encryption :  Setup a Virtual Private Gateway (VPG)

   A VPG is used to setup an AWS VPN which you can use in combination with Direct Connect to encrypt all data that traverses the Direct Connect link. This 
   combination provides an IPsec-encrypted private connection that also reduces network costs, increases bandwidth throughput, and provides a more 
   consistent network experience than internet-based VPN connections.


DynamoDB best practices include:
– Keep item sizes small.
– If you are storing serial data in DynamoDB that will require actions based on data/time use separate tables for days, weeks, months.
– Store more frequently and less frequently accessed data in separate tables.
– If possible compress larger attribute values.
– Store objects larger than 400KB in S3 and use pointers (S3 Object ID) in DynamoDB
- DynamoDB can be used for storing session state data

- Amazon SQS offers buffer capabilities to smooth out temporary volume spikes without losing messages or increasing latency.
  Amazon Kinesis - Amazon Kinesis is a fully managed, scalable service that can ingest, buffer, and process streaming data in real-time.

- EMR :
 Access Logs can be enabled on ALB and configured to store data in an S3 bucket. Amazon EMR is a web service that enables businesses, researchers, data 
  analysts, and developers to easily and cost-effectively process vast amounts of data. EMR utilizes a hosted Hadoop framework running on Amazon EC2 and
  Amazon S3


- Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. 

- AWS Batch : AWS Batch is used for running batch computing jobs across a fleet of EC2 instance
  An application that runs a computational fluid dynamics workload uses a tightly-coupled HPC architecture that uses the MPI protocol and runs across many
   nodes. A service-managed deployment is required to minimize operational overhead. : Use AWS Batch to deploy a multi-node parallel job

  AWS Batch Multi-node parallel jobs enable you to run single jobs that span multiple Amazon EC2 instances. With AWS Batch multi-node parallel jobs,you can
  run large-scale, tightly coupled, high performance computing applications and distributed GPU model training without the need to launch, configure, and
  manage Amazon EC2 resources directly

- QLDB : 
  The solution requires that the storage layer be immutable. This immutability can only be delivered by Amazon Quantum Ledger Database (QLDB), as Amazon
  QLDB has a built-in immutable journal that stores an accurate and sequenced entry of every data change. The journal is append-only, meaning that data can
  only be added to a journal, and it cannot be overwritten or deleted.




- tightly coupled & EFA : 
  It is recommended to use either enhanced networking or an Elastic Fabric Adapter (EFA) for the nodes of an HPC application. This will assist with 
   decreasing latency. Additionally, a cluster placement group packs instances close together inside an Availability Zone

  Using a cluster placement group enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication
  that is typical of HPC applications.

  A legacy tightly-coupled High Performance Computing (HPC) application will be migrated to AWS.: Elastic Fabric Adapter (EFA)

  An Elastic Fabric Adapter is an AWS Elastic Network Adapter (ENA) with added capabilities. The EFA lets you apply the scale, flexibility, and elasticity 
  of the AWS Cloud to tightly-coupled HPC apps. It is ideal for tightly coupled app as it uses the Message Passing Interface (MPI)

  The ENA, which provides Enhanced Networking, does provide high bandwidth and low inter-instance latency but it does not support the features for a 
  tightly-coupled app that the EFA does.

- S3 Select is used to query data in S3 using SQL to retrieve subset of the data using server side filtering
-  the best security option is to configure the inbound rule of the database security group to refer to the security group of ELB.
- ELB connection draining can be used to stop sending requests to instances that are de-registering or unhealthy while keeping the existing connections  
  open.


-Amazon DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns. This is the most efficient and cost-effective solution to optimizing for cost.


-  Create NAT Gateway in the public subnet of each Availability Zone, Configure each private subnet's route table to include a route from 0.0.0.0/0 to the 
   NAT Gateway in the same Availability Zone.

- Query API : 
   A Solutions Architect needs to work programmatically with IAM. Which feature of IAM allows direct access to the IAM web service using HTTPS to call 
   service actions and what is the method of authentication that must be used : Query API ,  Access key ID and secret access key

  AWS recommend that you use the AWS SDKs to make programmatic API calls to IAM. However, you can also use the IAM Query API to make direct calls to the
  IAM web service. An access key ID and secret access key must be used for authentication when using the Query API.



- Amazon FSX lustre for High performance file system. It is a high performance parallel storage.
- ALias - an alias record in route53 HZ that points to cloudfront distribution by resolving to the application's url domain name
        - create an Alias record in Route 53 that resolves the applications URL to the CloudFront distribution endpoint.
- Private link :A VPC endpoint enables private connections between your VPC and supported AWS services and VPC endpoint services powered by AWS PrivateLink

A company hosts statistical data in an Amazon S3 bucket that users around the world download from their website using a URL that resolves to a domain 
   name. The company needs to provide low latency access to users and plans to use Amazon Route 53 for hosting DNS records : 

- create a web distribution on amazon cloudfront pointing to an Amazon S3 origin.Create an alias record in route53 HZ that points to cloudfront distributio
  , resolving to the application's url domain name

Run Command is designed to support a wide range of enterprise scenarios including installing software, running ad hoc scripts or Microsoft PowerShell
   commands, configuring Windows Update settings, and more.
- Run Command can be used to implement configuration changes across Windows instances on a consistent yet ad hoc basis and is accessible from the AWS
  Management Console, the AWS Command Line Interface (CLI), the AWS Tools for Windows PowerShell, and the AWS SDKs.



*********************************************************************************************************************************************************


Hemanthc1234
cd /var/log
ssh root@ipaddress  connect to server in command prompot

cd ~/Desktop takes cli to desktop

ls / List all directories

open .    :open where our terminal is currently associated with

chmod 400 ec2-key.epm    :400 protects it by making it read only for the owner

ssh -i ec2-key.pem ec2-user@43.205.96.3


sudo su -     : if we need administrator access we run this command(O/P: We are now root)  .
                Root has all the permissions over the linux

sudo amazon-linux-extras install nginx1   : this will install nginx web serve software and packages in server

systemctl status nginx   : This will check status of webserver software of nginx


systemctl start nginx : This will start nginx

netstat -ntlp  : shows the active internet connections in server ( shows software ports associated with port 80:nginx sofyware , port 22:ssh software

in ec2 security groups : ec2 is listening on port 22 ,and nginx is listening on port 80. so to interact with nginx we have to open port 80

cd /usr/share/nginx/html   : path to customize our website

cat index.html    : this open the file content

index.html  : nginx server will save the contents in this file

echo > index.html     : To remove all the contents from index.html

echo "Hii" > index.html this will store in index.html

lsblk : list of directories with size  i.e; Lsblk is used to display details about block devices

Touch notes.txt    : create a file notes.txt in that directory

tracert google.com : shows how many hopsods it takes between server and customer

apt-get : for ubuntu
yum : for RPM (redhat linux).

*****************************************************************************************************************************************

INFRASTRUCTURE AND NETWORKING :

*******************************************************************************************************************************************************
Security Group: 

Port : Ex : if we install a ssh software on ec2 instance it will open a opening of 22(port for ssh software)  to connect from client to ec2(server)
            if we install a nginx software on ec2 instance it will open a opening of 80(port for nginx software) to connect from client to ec2(server)
 
 If we install a specific software on EC2 ; how do we know which opening will it open   : netstat -ntlp
    
       netstat -ntlp: this command will show which software is associated with opening  (ssh software has a opening of 22)
 Not all the software that create a opening on server.  Only few server that require a external connection from user or client will create opening.

 Now port 22 or 80 is open.any attacker in the world can attack server using port or opening (22 or 80) to prevent this we make use of firewall(S.G)

Firewall is a bridge between user and server that contains rules that has been set by administrator.
   if we have 2 openings. we can decide which can open(deny connect  to 22 or allow connect 80)
   -> if anyoone tries to connect server using port 22, firewall check its rules whether port 22 is allowed or not.based on that it will deny or allwed.
 The primary aim of firewall or S.G is to allow from trusted and deny from hacker.

** we can't allow 22 port from everyone we should allow it from our ip address only.
  opening 22 is connection to server  ; opening 80 is connection to browser or website in the server.   always deny 22 , allow 80 

*********************************************************************************************************************************************************

VPC : isolated network in AWS that can store servers.

 VPC is divided into Subnets.where few servers in each subnet
 
 -> servers from subnet 1 can communicate to internet using routes. for subnet 2 servers cannot connect to internet because of no routes.

if servers from diff subnets want to communicate each other then we can create a gate between that subnets.
we can partition our vpc to multiple subnets.
- the amount of partition size of subnet differs on size(Ip addresses for servers) we provided.
- routes is route tables. local target is to connect to servers on other subnet using gate.

VPC CIDR block defines the private ip address that will be assigned to resources.

Every default VPC in any region will have IPV4 cidr block = 172.31.0.0/16

** VPC spans across every AZ in region
-> we can't directly launch ec2 in VPC.	We divide multiple subnets in VPC. Each subnet associated with each availability zone.
-> we can add multiple subnets in a AZ. But we can't add one subnet in multiple AZ's.
-> each subnet has its own set of range of that is derived from larger VPC Range  (EX: VPC(1-100 IP's : Sub 1 = (1-50)   Sub 2 = (51-100)

Ip address : VPC(10.72.0.0/16) contains( 2^(32-16) = 65536 )  = sub1(10,72.0.0/24) contains 2^32-24 = 2^8 = 256 ip addresses starting from 10.72.0.(0-255).
                                                                sub2(10,72.1.0/24) contains 2^32-24 = 2^8 = 256 ip addresses starting from 10.72.1.(0-255)
** eventhough we launched an ec2 in subnet inside VPC we dont have access to internet. we can't connect to ec2 from internet eventhough port 22is open.

IG : It allows communication between resources in the VPC and the Internet. we have to attach it to vPC for internet.
     Manual Vpc doest cotain IG by default.we have to create a custom IG and we have to attach it.
     Without IG we can't even connect to ec2 through ssh.

Route Tables : here two main things    straight(destination)=dharmavaram(target); 0.0.0.0/0(destination)=igw-f5c97kskjskgd(IG-it goes to internet)
                                             172.30.3.0/24(destination)=local(will connect to other ec2's in diff subnets in same vpc(don't need internet)

by default when ever a vpc is created,the route table also created


Public Subnet: It is associated with an Internet Gateway.

  it is recommended if you want to run a public-facing web application.

Overall Security Risk:  High   along with users,hackers also get access to website so they can attack.



Private Subnet: it does not have an Internet Gateway attached to it.

New connections from the Internet cannot reach to the EC2 instances within the private subnet. so attackers cannot attack.

*** Instances in private subnet do not have a Public IP / Elastic IP attached. Eventhough we get public ip for ec2 but we can't connect to ec2 using that.


Example Use-Case:Database Servers : Servers who do not directly interact with Internet resources.
 Even though the ec2 in private subnet ,the local level communication for ec2 between public and private will still work using private IP.
      Public ip's won't work for local communication.Because public ip's go through internet.
Cons : Attacker can still attack ec2 instance in private subnet using ec2 instances in public subnet. he first login to publiuc ec2 using public ip then he
       perform ssh for private ec2 using private ip. 

NAT Gateway : since there is no IG in private subnet the ec2's present in that can't connect to internet for any updates or patches.
 
->we use network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet
-> new connections from internet cannot connect to instances in private subnet.

** Path : Private EC2 -> NAT gateway -> Internet gateway -> Internet.  outgoing to internet.

** path : Private EC2 -> NAT gateway <-> Internet gateway <- Internet : from nat gateway internet doesnot go to private ec2
-> NAT gateway should be in a public subnet.

***PORTS : 22 for SSH, 3389 for RDP, 21 for FTP, 25 for SMTP, 80 for HTTP and 443 for HTTPS.

**********************************************************************

VPC Peering : it is a netwrok on between two VPC that enables the communication between instances of both the VPC.instances communicate via the private IP

 -> VPC peering is not a transit(VPC A & B Peered ; VPC B &C peered. doesn't mean that VPC A comunicate to VPC C.
 -> VPC should have different cidr(No overlapping of ipv4 addresses)

NACL(Network access control List) : SG works at ec2 intsance level
                                    NACL works at Subnet Level

NACL is an optional layer of our security for vpc that acts as firewall for controlling in and out of subnets(more than one)
we can associate NACL at an entire subnet level.
each subnet in vpc associated with NACL.if we don't explicitely associate subnet with any NACL then the subnet is automatically associate with default NACL
Deafult NACL will allow all traffic inbound and outbound,if applicable ipv6 also.
we can associate multiple subnets with NACL.BUt a subnet can only associate with one NACL.

Prefix Lists(PL) : a pl is a set of one or more CIDR blocks.
we can create a PL from the IP addresses that we use frequently,and reference them as a security group rules and routes insteadof referencing them indvd

2 types of PL :
 1) customer managed PL : set of ip addresses range that we define and manage
 2) AWS Managed PL : set of ip addresses range for AWS services.

-> Pl support a single type of ip adresses(IPV4 or IPV6).we cannot combine both ipv4 and ipv6.
-> a PL can applies to only one region.

EC2 PRICING : 

1)On DEmand instance : 

With On-demand instances, we pay for compute capacity per hour or per second depending on the instances which is being run.

2.DEDICATED INSTANCES : Dedicated Instances are Amazon EC2 instances that run in a VPC on hardware that’s dedicated to a single customer. Your Dedicated
 instances are physically isolated at the host hardware level from instances that belong to other AWS accounts. Dedicated instances allow automatic
 instance placement and billing is per instance

3.DEDICATED HOST : An Amazon EC2 Dedicated Host is a physical server with EC2 instance capacity fully dedicated to your use. Dedicated Hosts can help you 
address compliance requirements and reduce costs by allowing you to use your existing server-bound software licenses. With dedicated hosts billing is on a
 per-host basis (not per instance).


Q : 

* A VPC automatically comes with a default network ACL which allows all inbound/outbound traffic. A custom NACL denies all traffic both inbound and 
  outbound by default

- In a default VPC instances will be assigned a public and private DNS hostname  
- In a non-default VPC instances will be assigned a private but not a public DNS hostname

- You can use VPC Flow Logs to capture detailed information about the traffic going to and from your Elastic Load Balancer. Create a flow log for each   
  network interface for your load balancer. There is one network interface per load balancer subnet

- VPCs can be shared among multiple AWS accounts. Resources can then be shared amongst those accounts. However, to restrict access so that consumers 
  cannot connect to other instances in the VPC the best solution is to use PrivateLink to create an endpoint for the application. The endpoint type will be
  an interface endpoint and it uses an NLB in the shared services VPC

- An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet.
  An internet gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic, and to perform network address 
  translation (NAT) for instances that have been assigned public IPv4 addresses.
  An internet gateway supports IPv4 and IPv6 traffic. It does not cause availability risks or bandwidth constraints on your network traffic.


SUBNET : 

- To prevent direct connectivity to the EC2 instances from the internet you can deploy your EC2 instances in a private subnet and have the ELB in a public
  subnet. To configure this you must enable a public subnet in the ELB that is in the same AZ as the private subnet.


NACL : 

- You can only create deny rules with network ACLs, it is not possible with security groups. Network ACLs process rules in order from the lowest numbered
  rules to the highest until they reach and allow or deny.

- Security group : supports allow rules only.it is statful.applies to an instance only if associated with group.
- NACL : operates at subnet level.supports allow and deny rules.Stateless.Automatically applies to all instances in the subnets associated with.
  Stateful: This means any changes applied to an incoming rule will be automatically applied to the outgoing rule. Example: If you allow an incoming
  port 80, the outgoing port 80 will be automatically opened. Stateless: This means any changes applied to an incoming rule will not be applied to the
  outgoing rule.

- When you create a new subnet, it is automatically associated with the main route table. Therefore, the EC2 instance will not have a route to the Intern
  The Architect should associate the new subnet with the custom route table



Create a new launch configuration that uses the AMI and update the ASG to use the new launch configuration   

- A launch configuration is the template used to create new EC2 instances and includes parameters such as instance family, instance type, AMI, key pair and
  security groups.
- You cannot edit a launch configuration once defined. In this case you can create a new launch configuration that uses the new AMI and any new instances 
  that are launched by the ASG will use the new AMI.




***********************************************************************************************************************************************************

STORAGE :

***********************************************************************************************************************************************************

NEW : 


EBS :

Use-Case: External HDD :  can be attached to the Workstation and detached whenever required (portable)
- it come with a feature of portability. tdy we can attatch it to one laptop tmrw we can connect to another laptop. the data in hdd remains same unless 
  and untiull we change.

EBS : Depending on the use-case, you can buy external storage of different configuration based on size, performance and others.
- by default whenever we create a ec2, the storage volme in which all of the data will be stored is the ebs volume.
- pros : These volumes can be attached and detached from EC2 instance.
   ex : suppose i terminate the ec2.since the vol's can be attatched and detatched from ec2, even if the instance is terminated the storage volume(ebs) can
   contineues to stay if required.

Demo : launch a ec2 with default.in storage section we can see one block device is connected(EBS) to ec2 and size of vulume is 8gib.
  if we click on that ebs vol. we can see monitoring(write bandwith,read bandwidth, ipos).

- EBS volumes are attached to EC2 instance through Network.(when we take a example of external hdd we need a usb cable to connect to laptop).However in 
  the case of EBS we need to have a network and through the network these volumes are attatched to ec2(network mounted).now since network that is involved
  there can also be a little amount of delay (latency) that involved.

- in diagram there are 4 ebs volumes are there.however only one volume is attatched to the ec2 instance.incase if we would like we can attatch all the 4 
  depending upon the use case.

- go to volumes console in the AZ where we launched ec2. we can see one ebs attatched to ec2. create a new ebs vol of 1gb.select az same as of ec2.create.
- now i have one extra vol. if i want i conncet to the ec2 i created.click on extra ebs and select attatch vol. specify the ec2 inst and click on attatch.
  it states that ebs is in use.tmrw we can detatch it and it states that it is available.

- EBS vol's are AZ specific : You create an EBS volume in a specific Availability Zone, and then attach it to an instance in that same Availability Zone
 ex : i have 2 ebs vol's in AZ(1a). i can attatch these to the instances which are running in the same AZ(1a).if the ec2 is running in diff AZ we won't be
  able to attatch.

- EC2 and EBS Attachment : A single ec2 can have multiple volumes.let's say when i launch a ec2 i had a ebs storage of 10Gib.npow i am out of storage.we
  can create one more vol of 100gib and attatch it to ec2. and config our appl to store data into 100gib vol.

-Proprietary File System  = EBS

***********************************************************************************************************************************************************

EBS SNAPSHOTS : 

- whatever the data that is stored in ebs vol we can go and take a backup or a snapshot of that specific vol whenever req.so these are also reffred to as
  pointing snapshot.
- You can back up the data on your Amazon EBS volumes to Amazon S3 by taking point-in-time snapshots.
- i have a 10gib vol. i taken a snapshot of that.now from that snapshot i can go ahead and create one more vol.and whatever vol that would be created from
  this snapshot will have the exact same data as the original vol.


DEMO : 
- go to ebs console and click and create vol. create a  1gib vol in AZ(1a).now it is available. if we want to take backup of this ;click on create snapshot
  and give name it will be created.now got to snaphot console we can see snapshot. from this snapshot we can create a ebs vol. click on snapshot and select
  create ebs volume.it will create a ebs vol from snapshot.

* Snapshots are extensively used when we want to copy the data of vol from one AZ to another or may be to a diff region or may be to a diff accounts.

- let's say that i have taken a snapshot in aws acnt 1, i can coppy that snapshot to aws acnt 2.and from the snapsot of aws acnt 2 we can cretae a new ebs
  vol and attatch it to the ec2 instance. Also note that ebs vol are AZ specific.

- incase if we want to create a diff ec2 in diff AZ with same data as of the original one we will have to copy the diff snapshot to that AZ.

- Migrating data Across region : we want to migrate data associated with ebs volume to a ec2 instance in another region. there is one vol associated with
  ec2 in singap region.if we want to have this data associated with instance in mumb region.then first we can take a snapshot of ebs volume.from the snapsh
  ot we can go and copy and restore operation , from that we will create a ebs voil in mum region.and then from that we can attatch the ebs to the ec2 in
  mum region.

- Amazon EBS fast snapshot restore (FSR) enables you to create a volume from a snapshot that is fully initialized at creation. This eliminates the latency
  of I/O operations on a block when it is accessed for the first time. Volumes that are created using fast snapshot restore instantly deliver all of their
  provisioned performance.
The following are a few reasons why an instance might immediately terminate:
– You’ve reached your EBS volume limit.
– An EBS snapshot is corrupt.

- With EBS you can encrypt your volume and automate volume-level backups using snapshots that are run by Data Lifecycle Manager.
- Block Storage +  fault-tolerant up to the loss of an instance = EBS
- to backup EBS volumes into another region, just create a snapshot and copy to another region
- Only the most recent snapshot. Snapshots are incremental, but the deletion process will ensure that no data is lost 

***********************************************************************************************************************************************************

Delete on Termination Attribute Associated with ebs volume: 

- Since EBS and EC2 are separate set of entities, they can live independently of each other.
 ex : we have an ec2 and there are 2 ebs vol's associated with it.when this ec2 gets terminated the volumes that are present are they live seperately. we
 can configure that volumes contineus to stay there irrespective of ec2 is running or terminated.

- when we disscuss about independence of ec2 and ebs volume, there is one important attribute of deletionon termination that we need to know.
- When an instance terminates, the value of the DeleteOnTermination attribute for each attached EBS volume determines whether to preserve or delete volume


- demo : create an ec2. while creating an ec2 in storage section go to advanvce configuration of storage there we can see "delete on termination option: if
  we put that value to yes means the ebs vol will be deleted when ec2 gets terminated. if we give no the ebs volume will still be there if we delete ec2.

.When you hibernate an instance, Amazon EC2 signals the operating system to perform hibernation (suspend-to-disk). Hibernation saves the contents from 
the instance memory (RAM) to your Amazon Elastic Block Store (Amazon EBS) root volume. Amazon EC2 persists the instance's EBS root volume and any attached 
EBS data volumes. When you start your instance:
- The EBS root volume is restored to its previous state
- The RAM contents are reloaded
- The processes that were previously running on the instance are resumed
(Also :  When an instance is stopped the operating system is shut down and the contents of memory will be lost.)


***********************************************************************************************************************************************************


-  IOPS is a count of the read/write operations per second
-  Throughput is the actual measurement of read/write bits per second that are transferred over a network


- SSD: Optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS
- HDD : Optimized for large streaming workloads where the dominant performance attribute is throughput


1.general Purpose SSD(GP1) : it again divided into 2 types
  - GP2: 3 IOPS/GiB (minimum 100 IOPS) to a maximum of 16,000 IOPS
         Throughput between 128 MiB/s and 250 MiB/s, depending on the volume size.
         Older

  - GP3:  Baseline of 3000 IOPS 
          125 MiB/s, Maximum throughput = 1000 MiB/s
          newer.

- gp3 offers SSD-performance at a 20% lower cost per GB than gp2 volumes.
* IOPS: ex : GP2 : 3 iops per gb(100min IOPS)
                   100 GB : 300IOPS
             
             GP3 : here even if we create 50Gb volume we will be getting a baseline performance of 3000IOPS.

General Purpose SSD (GP2)  : general purpose wrklds which balances the price and performance for norma wrklds. Used fordev,test,prod normal workloads.
                             Volume Size from 1Gib to 16TiB . Max 10000 IOPS, 160Mib/Sec. We can use only 3iops/Gib.(Ex:100iops for 33 gb.  102 for 34 gib

-Legacy Application + Low workload = General Purpose SSD (gp2)


2. Provisioned IOPS SSD : Highest performance SSD volume designed for mission critical application workloads.

* Provisoned IOPS is again diveided into 3 types : 1.IO2 BLOCK EXPRESS          2.IO2               3. IO1
- Refer page no 23 in PPT.

 Provisioned IOPS SSD (IO1) : Highest Performance SSD. Used for MongoDB, Postegress SQL, MySQL. Used for high sensetive workloads. 
                             Volume from 4GiB to 16TiB.  Max 20000 IOPS , 320Mib/sec.  we can use any no of iops per gib(Ex : 1000 iops for 100gib)

- Amazon EC2 Nitro-based systems are not required for this solution but do offer advantages in performance that will help to maximize the usage of the EBS
 volume. For the data storage volume an i01 volume can support up to 64,000 IOPS so a single volume with sufficient capacity (50 IOPS per GiB) can be
 deliver the requirements.

***********************************************************************************************************************************************************


Hard Disk Drive(HDD) : Optimized for large streaming workloads where the dominant performance attribute is throughput.

Cold HDD (SC1) : Lowest(Cheapest HDD) Designed for less frequenty access workloads. Performance and speed is slow.
                 Used in scenarios where lowest cost storage is required. 500Gib To 16 TiB
                 Max 250 IOPS. 250 Mib/Sec
- infrequently accessed data +  store large + sequential data = Cold HDD (sc1)

- Cold HDD (sc1) volumes provide low-cost magnetic storage that defines performance in terms of throughput rather than IOPS. With a lower throughput limit
  than st1, sc1 is a good fit for large, sequential cold-data workloads. If you require infrequent access to your data and are looking to save costs, sc1
  provides inexpensive block storage. Bootable sc1 volumes are not supported.

- Cold HDD (sc1) volumes, though similar to Throughput Optimized HDD (st1) volumes, are designed to support infrequently accessed data.



ThroughPut Optimized HDD (St1) : Lowest Cost HDD used for frequently accessed throughput workload.Used for applications like Data Ware House, Big data 
  Log Processing.  500Gib to 16Tib	
- Max 500 IOPS/Sec . 500Mib/Sec. 
- The instance needs to support a MapReduce process that requires high throughput for a large dataset with large I/O sizes:A.EBS throuhtput optimized.

- EBS Throughput Optimized HDD is good for the following:
-  Frequently accessed, throughput intensive workloads with large datasets and large I/O sizes, such as MapReduce, Kafka, log processing, data warehouse,
 and ETL workloads
*** Both HDD's are low cost. But Cold HDD used for less frequently and ThroughPut Optimized used for frequently used access Workloads. 
     We cannot install Operating System  on HDD's

- log processing + sequentially + throughput rate 500 MB/s = EBS Throughput Optimized HDD (st1)
- frequently accessed + full-volume scan + throughput intensive workloads + large datasets and large I/O sizes = Throughput Optimized HDD (st1)
- Throughput Optimized HDD is the most cost-effective storage option and for a small DB with low traffic volumes it may be sufficient. Note that the volume
 must be at least 500 GB in size


Magnetic 

Volume : EBS Magnetic. Workloads where data is infrequently accessed. 1Gib To 16 TiB Performance MAX : 40-200 IOPS . 40-90MIB/sec




EBS ENCRYPTION (Disk level encryption) : 
- Disk Level encryption involves encrypting all the files that are part of the storage device.
- depending upon the OS that we use there are multiple diff set of technologies avalable for encryption.
  For windows we have Bitlocker for Disklevel Encryption
 for Linux Lux 
Ex : one of my disks is encrypted.if we want to decrypt it we want to provide the password only then only we will be able to fetch contents.

- Amazon EBS encryption uses AWS KMS keys when creating encrypted volumes and snapshots.
- For an encrypted EBS volume that is attached to a supported instance type, the following types of data are encrypted:
  Data at rest inside the volume(whatever data we stored at the ebs volume)
- All data moving between the volume and the instance(EBS is not directly part of Host.it is the volume that is mounted through network. so the data travel
- Through network withing the AZ is Encrypted)
- All snapshots created from the Encrypted volume 
- All volumes created from those encrypted snapshots
  
IMP Points : 
- Enabling EBS Encryption has minimal effect on latency.
- Encryption and decryption are handled transparently, and they require no additional action from you or your applications.
- Amazon EBS encrypts your volume using industry-standard AES-256 data encryption

-All EBS types and all instance families support encryption but not all instance types support encryption.There is no direct way to change the encryption
 state of a volume.
– Not all instance types support encryption.
– Data in transit between an instance and an encrypted volume is also encrypted (data is encrypted in trans.
– You can have encrypted an unencrypted EBS volumes attached to an instance at the same time.
– Snapshots of encrypted volumes are encrypted automatically.
– EBS volumes restored from encrypted snapshots are encrypted automatically.
– EBS volumes created from encrypted snapshots are also encrypted.




OLD : 


Block :
- file is split and stored in fixed size blocks
- suitable for applications which require high IOPS,transactionsal data

EBS : Elastic block store :EBS Volumes are mounted via the network to the ec2 Instances
- Persistent block storage to use with ec2. i.e; when ec2 stops and start the data will remain same(Persistent)
- 99.9^4 availability , automatically replicated in its availability zone.
- EBS is elastic in nature. supports dynamic increase in capacity and performance.
- EBS is based on a network attach storage.
- EBS is portable.This allows users to attach and detach ebs to ec2 instances.
* EBS volumes are associated to one AZ only.if we want to use it in another AWS AZ we can make a EBS snapshot and use it.

Instance Store :

                Provides temporary block storage volumes for ec2 instances. Storage is located on the disks that are physicaly atteced to host computer
                Instance store size based on instance type. If we are using instance store, better to backup our data to central storage places like S3.
  Data in instance store lost if :
                                   Instance Terminates
					     Instance Stops, Underlying Disk Drive fails.

- The company will require around 5 TB of storage for video processing with the maximum possible I/O performance : Instance store volume.
The best I/O performance can be achieved by using instance store volumes for the video processing. This is safe to use for use cases where the data can
 be recreated from the source files so this is a good use case.

- You can specify the instance store volumes for your instance only when you launch an instance. You can’t attach instance store volumes to an instance
  after you’ve launched it.
- Each instance that you launch has an associated root device volume, either an Amazon EBS volume or an instance store volume.
- You can also attach additional EBS volumes to a running instance

- A High Performance Computing (HPC) application needs storage that can provide 135,000 IOPS. The storage layer is replicated across all instances in a 
  cluster : Use Amazon Instance Store

- Instance stores offer very high performance and low latency. As long as you can afford to lose an instance, i.e. you are replicating your data, these
  can be a good solution for high performance/low latency requirements. Also, the cost of instance stores is included in the instance charges so it can
  also be more cost-effective than EBS Provisioned IOPS
	
*********************************************************************************************************************************************************


Object :
- we can call object using https interface
- files are distributed in physical nodes

S3:

 S3 : 99.9^11 Durability.  99.99 Availability. Durability is same for every s3 storage class.
     
     AWS Cloud storage service. S3 is an object storage service designed to store and retrieve any amount of data from anywhere.
     Buckets = Folders , Objects = files. Bucket names should be unique across the AWS Namespace.  

S3 storage classes : 

  S3 Standard : 99.9^11 durability of objects. Offers high availability,durability and performance object storage for frequently accessed data
		    $24/month for 1tb data.99.99% availability

  S3 Standard IA : It also provides 99.9^11 durability.  S3 S IA is for data that is infrequently accessed data,but that require rapid access when needed
                  1tb/month = $24.5 . If 50% is accessed : $18 . if 0% occupied = $12, 99.9% availability.

  S3 Standard ,Standard IA,every storage type except One zone -IA stores data in minimum 3 Availabilty zones. Data replicated.

  S3 Glacier : Glacier is meant for archiving and for storing long term back-ups. means data that need to be archived for years without requirement acces
               $4/Month.We can use glacier for storing past years logs. But for retrieval of data it takes hours. Pay extra money for fast retrieval.
               S3 glacier is of 2 types S3 glacier and S3 Glacier Deep Archive.
               retrieval time 1-5 min= expedited. 3-5 hours standard. 5-12 hours bulk (retrieval time 1min - 12 hours) 

S3 Intelligent tiering : designed to optimize cost by automatically moving data to most cost effective tier. Ex : Standardcost $24,IA cost $12-0% access
              Ex: if we have 30 objects in s3 standard .if one of them is not accessd for 1 month it will automatically moved to S-IA.
                  Vice versa : in standard-IA any of the object is accessed it will automatically moves to standard type.
                         
S3 -One Zone Infrequent access : Stores data in single availability zone.Costs 20% lesser than Standard IA.
   	 	  		  	   Good choice for storing secondary backup of on premises data or easily recreatable data.	
                                 Data Will be lost incase of avilability zone destruction. 1tb data = $10.24/Month

S3 Glacier Deep Archive : 1Tb/month = $1.02 .we can access data within 12 hours if it is standard and 48 hours if it is bulk.(Retrieval time 12-48hrs) 
                          S3 Glacier Deep Archive for long term backup cycle data that might infrequently need to restore within 12 hours.

S3 Versioning : we can keep multiple variants of objects in same s3 bucket.
                versioning allows us to preserve,retrieve,restore every version of every object stored in s3 bucket.
                Versioning applies to all the objects in the bucket.

S3 Lifecycle Policies : Depending upon the policies,access patterns, data should be transiytioned to appropriate storage classes.
                        we can store 1 month of logs in s3 standard. after one month move one month older logs into Standard-IA.
                        Move the logs older than 6 months to S3 Glacier.

S3 cross region replication : it allows data from s3 buckets to be replicated across regions.
                              Both source and destination must have versioning enabled.
                                   
S3 Static Website Hosting : before this feature if we want to host a basic static website we need to create and manage the entire server infrastructure
                            on the cloud provider
                            With amazon S3 we can now directly host a static website through S3
 On Staitic websites individual web pages include static content. They might also include client side scripts.
 Dynamic websites relies on server side processing, include server side scripts such as PHP,asp.net, JSP. S3 is not used for Dynamic website.

Presigned URL's : All objects in s3 are private by default.
                  Object owner can optionally share a presigned url for the objects without making them public. 

S3 MultiPArt upload : it is a way to upload an entire file in form of small individual chunks to storage device.
                      While uploading data via multipart we need to specify the part number and it's position in the uploaded object.
                      

S3 Object Lock : We can write objects using Write once read only Model(WORM).
                 we can use it to prevent an object from being deleted or overwritten for a fixed amount of time or indefenitely.
     Retention modes :
           Governance mode: When deployed in GM mode aws account with specific iam permissions are able to remove objects lock from objects
           Compliance Mode: The protection cannot be removed by any user including root account.
We can apply a specific retention mode for each object in the bucket.

S3 requester pays : With this the requester instead of bucket owner pays the cost of request and data download from the bucket
                    Bucket owner will always pay the amount for storing the data.

Reduced Redundancy Storage (RRS) : 99.99% Availabilty and durability
                                   is an Amazon S3 storage option that enables customers to store noncritical, 
                                   reproducible data at lower levels of redundancy than Amazon S3’s standard storage
Q : 

- Use an S3 lifecycle policy with object expiration configured to automatically remove objects that are more than 60 days old 
  two types of actions in s3 life cycle policy: 
  Transition actions—Define when objects transition to another storage class. For example, you might choose to transition objects to the S3 Standard-IA 
  storage class 30 days after you created them, or archive objects to the S3 Glacier storage class one year after creating them.
  Expiration actions—Define when objects expire. Amazon S3 deletes expired objects on your behalf.

- Object versioning is means of keeping multiple variants of an object in the same Amazon S3 bucket. Versioning provides the ability to recover from both
  unintended user actions and application failures. You can use versioning to preserve, retrieve, and restore every version of every object stored in your 
  Amazon S3 bucket.

- The two solutions for these requirements are versioning and MFA delete. Versioning will retain a copy of each version of the document and multi-factor 
  authentication delete (MFA delete) will prevent any accidental deletion as you need to supply a second factor when attempting a delete.

- We can configure a lifecycle action that then transitions the objects to INTELLIGENT_TIERING, STANDARD_IA, or ONEZONE_IA.
  when there is no standard_IA option we can use onezone_IA.

- The transition should be to Standard-IA rather than One Zone-IA. Though One Zone-IA would be cheaper, it also offers lower availability and the question
  states the objects “must remain immediately available”. Therefore the availability is a consideration.
  Though there is no minimum duration when storing data in S3 Standard, you cannot transition to Standard IA within 30 days. This can be seen when trying to
  create a lifecycle rule: Means we can't transfer to any other s3 standard before 30 days using life cycle rules.

- A Financial Services company currently stores data in Amazon S3. Each bucket contains items which have different access patterns. The Chief Financial
  officer of the organization wants to reduce costs, as they have noticed a sharp increase in their S3 bill. The Chief Financial Officer wants to reduce
  the S3 spend as quickly as possible.What is the quickest way to reduce the S3 spend with the LEAST operational overhead ? 
  transition objects to appropriate storage class by using s3 lifecycle configuration.

- Uploading using a pre-signed URL allows you to upload the object without having any AWS security credentials/permissions. Pre-signed URLs can be 
  generated programmatically and anyone who receives a valid pre-signed URL can then programmatically upload an object. This solution bypasses the web 
  server avoiding any performance bottlenecks.

- In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation.

- Using the Range HTTP header in a GET Object request, you can fetch a byte-range from an object, transferring only the specified portion. You can use
  concurrent connections to Amazon S3 to fetch different byte ranges from within the same object. This helps you achieve higher aggregate throughput versus
  a single whole-object request.Fetching smaller ranges of a large object also allows your application to improve retry times when requests are interrupted.


  Amazon S3 Transfer Acceleration is used for speeding up uploads of data to Amazon S3 by using the CloudFront network. It is  used for downloading data

  AWS Global Accelerator is used for improving availability and performance for Amazon EC2 instances or Elastic Load Balancers (ALB and NLB). It is not
  used for improving Amazon S3 performance




******************************************************************************************************************************************************
IMP : 
- all storage gateway types (volume gateway, tape gateway, file gateway ) cloud storage destination is aws s3 or glacier or ebs snapshots (nothing else)
- volume gateway uses ISCSI protocol.
- Tape gateway uses ISCSI-VTL protocol.
- File Gateway uses NFS or SMB protocol.
- FSX windows file system uses SMB protocol. also replacement to the NAS file share 
- Datsync is not a hybrid storage.DataSync makes it simple and fast to move large amounts of data online between on-premises storage and  S3,Amazon EFS), 
  Amazon FSx for Windows File Server
-EFS makes use of NFS.




- AWS Storage Gateway: AWS stotrage gateway is a hybrid storage service that allows the on-premise application to easily use the cloud storage.
- AWS Storage Gateway”as this service is primarily used for connecting on-premises storage to cloud storage. It consists of a software device installed 
  on-premises and can be used with SMB shares but it actually stores the data on S3. It is also used for migration   

 
PATH: Application server ~ Aws Storage gateway Appliance(Like a drive in Local machine) ~ Direct connect(Through internet) ~ AWS Storage gateway backend 

                         Connected to AWS(S3 or Glacier or EBS Snap Shots)

***Storage gateway h/w appliance uses standard storage protocol like NFS(network File System) ,iSCSI which the application connects to and stores the data.
- The hardware appliance supports File Gateway with NFS and SMB interfaces, Volume Gateway cached volumes with iSCSI, and Tape Gateway with iSCSI-VTL


Three types of storage gateway connections : 1) Gateway stored volume
                                             2) Gateway cached volume
                                             3) Gateway virtual tape library

1)Gateway stored volume: Stores Primary data locally(On premises) while asynchronously(in time interval) backing data to aws.
                       in storage gateway volume storage is present for storing data and upload buffer is present to upload data to AWS 

PATH : Application server(Initiator) ~ AWS Gateway stored volume(volume storage + Upload Buffer) ~ AWS Storage gateway service ~ AWS(S3 or EBS snapshot)

*** Gateway stored volume is very beneficial when customer wants to store large amounts of data on premises at very fast rate.
-The hardware appliance supports File Gateway with NFS and SMB interfaces, Volume Gateway cached volumes with iSCSI, and Tape Gateway with iSCSI-VTL


2)Gateway cached volume: Nothing is stored on premises except the cache of recently read or written data stored . Primary data is stored in AWS S3.
                          In this no volume storage is present on premises.
       Ex: A client wants to store a large amounts of data but he will be accessing only certain portions of data	then Gateway cached volume is used.

PATH : Application server(Initiator) ~ AWS Storage gateway VM(Cached Storage + Upload Buffer) ~ AWS Storage 
       gateway service ~Volume Storage backed by AWS S3 ~ Amazon EBS Snapshots
*** The AWS Storage Gateway Volume Gateway in cached volume mode is a block-based (not file-based) solution so you cannot mount the storage with the SMB 
    or NFS protocol

******************************************************************************************************************************************************


3)Gateway virtual tape library(Tape gateway) : virtual tapes stored in S3 with frequently accsed data stores on-premise.
 Tape storage solution	 remains most cost effective solution till date

 tape storage is backup for any primary data.

 It is just like gateway cached volume but the data stored in aws s3 in the form of virtual tapes.

PATH : Application server(Initiator) ~ (tapeDrive,media changer)AWS Storage gateway VM(Cached Storage + Upload Buffer) ~ AWS Storage 
       gateway service ~ gateway virtual tape library storage backed by aws S3 ~ Virtual tapes Storage backed by amazon galacier

- The AWS Storage Gateway Tape Gateway enables you to replace using physical tapes on premises with virtual tapes in AWS without changing existing backup
 workflows.


FILE GATEWAY : One of the storage gateway types (Volume gateway , tape gateway, file gateway)

   Stores files as objects in S3 with a local cache for low latency to most recently visited data.
 AWS file Storage gateway (file gateway) interface or file gateway provides an interface via NFS Or SMB(Server message block) for synchronization
 of data from on-premises to AWS S3

DEmo : File gateway(Storage gateway appliance) is running on ec2 instance and it is integrated with s3. we have to connect to ec2 using NFS or SMB 
    ** Storage gateway(In ec2 instance) is acting as a middle man between On premises and S3.
NFS supported by File Gateway only and No visualization means Hardware appliance


Q :
The solution must use NFS file shares to access the migrated data without code modification. This means you can use either Amazon EFS or AWS Storage
  Gateway – File Gateway. Both of these can be mounted using NFS from on-premises applications.
- However, EFS is the wrong answer as the solution asks to maximize availability and durability. The File Gateway backs off of Amazon S3 which has much 
  higher availability and durability than EFS which is why it is the best solution for this scenario.

*** The AWS Storage Gateway Hardware Appliance is a physical, standalone, validated server configuration for on-premises deployments. It comes pre-loaded
  with Storage Gateway software, and provides all the required CPU, memory, network, and SSD cache resources for creating and configuring File Gateway, 
  Volume Gateway, or Tape Gateway. (Neal davis set -1 -63q)

- A file gateway is the correct type of appliance to use for this use case as it is suitable for mounting via the NFS and SMB protocols.

-AWS Storage Gateway is a set of hybrid cloud storage services that provide on-premises access to virtually unlimited cloud storage.

***********************************************************************************************************************************************************


AMAZON FSX :

- in the industry there are multiple types of files systems that are available for different set of use cases.now if we are using windows one of the common
  files systems that we will generally use is FET32,NTFS.
- we will see that there are multiple set of file systems and depending upon the device the file system requirments will also change.apart from that in the
  industry there are diff set of file systems are req for diff workloads this is where amazon fsx comes into picture.

- there are multiple popular set of file systems/storage platforms available in the industry that are used extensively based on specific use case.
  ex : types
  Lustre : Parallel distributed file system generally used for High scale cluster computing.
- file systems that are in laptop drives are NTFS.
Now challenges : many org's has use case to leverage the rich feature sets and fast performance of widely used open source and commercially licensed FS's.
  This would lead to lot of time consuming administrative tasks like Hardware provisioning,s/w configuration,patching and backups.
  to overcome these set of challenges for various popular files systems aws has released a service called as FSX. 
- Amzon FSX makes it easy and cost effective to launch and run popular file systems.


Benefits of FSX :
- Simple and fully managed.

- Secure and fully compliant : Amazon fsx automatically encrypts your data at rest and transit.complies with ISO,Soc certificates.

- Integrate with aws services like aws s3,kms,workspaces.

Demo : go to fsx console.one file system is running of type Lustre.it's capacity is 1.2 tb. I mounted this on amazon linux machine.in linux machine if i 
       run mount command , there we can see specific lustre file system that is monted on the path of /fsx.also shows ip address of lustre.
- once lustre file system is ready it will give you a specific endpoint.like dns endpoint we can conect this to ec2.

AMAZON FSX FOR LUSTRE :
Amazon FSX for lusture : MAkes it easy and cost effective to launch and run the world's most popular high performance system
 
 - Allows your workloads to process data with consistent sub-milli second latencies , upto 100's of GB's per second of throughput and upto millions IOPS


-  provides cost effective,High performance,scalable file storage for compute workloads such as Ml,High performance computing(HPC),vedio
 processing and financial modeling.( at any use case related to HPC lustre FS is recommended).
- Integrates seamlessly with S3,EKS,sage maker.

EX: i have an s3 bucker names kplabs.This bucket is connected with the lustre fs that we created.whatever data that we upload in s3 bucket will be availabl
    in our FS.Similiarly whatever data that we add as part of FS(lustre) will be available in the s3 bucket.

- Amazon FSx for Lustre makes it easy and cost effective to launch and run the world’s most popular high-performance file system. Use it for workloads 
  where speed matters, such as machine learning, high performance computing (HPC), video processing, and financial modeling.

- Amazon FSx for Lustre provides a high-performance file system optimized for fast processing of workloads such as machine learning, high-performance 
  computing (HPC), video processing, financial modeling, and electronic design automation (EDA)
- FSX lustre Allows your workloads to process data with consistent sub-millisecond latencies, up to hundreds of gigabytes per second of throughput, and up
 to millions of IOPS

-tiered storage on premises with hot high-performance parallel storage to support the application during periodic runs of the application, and more 
economical cold storage to hold the data when the application is not actively running. : 
 A. Amazon S3 for cold data storage
    Amazon FSx for Lustre for high-performance parallel storage


***********************************************************************************************************************************************************


AMAZON FSX FOR WINDOWS FILE SERVER(this is primarily for windows server related use cases):

- this is fully managed highly reliable file system storage that is accessable over the industry standard Server message Block(SMB) protocol.
- it built on windows server,it provides Full SMB support and a wide range of administrative features like user quotas,data deduplication , and end user 
  file restore
- Although it is built on windows server, it can still be accessabvle from Windows,Linux,MacOS.
* It integrates with Microsoft Active Directory(AD) to support windows environments and enterprises.

- The destination filesystem should be Amazon FSx for Windows File Server. This supports DFSN and is the most suitable storage solution for Microsoft 
  filesystems. AWS DataSync supports migrating to the Amazon FSx and automates the process

-applications stores files on a Windows file server farm that uses Distributed File System Replication (DFSR) to keep data in sync : Amazon FSX
-Amazon FSx for Windows File Server provides fully managed, highly reliable file storage that is accessible over the industry-standard Server Message 
 Block (SMB) protocol.

Amazon FSx is built on Windows Server and provides a rich set of administrative features that include end-user file restore, user quotas, and Access 
Control Lists (ACLs).
Additionally, Amazon FSX for Windows File Server supports Distributed File System Replication (DFSR) in both Single-AZ and Multi-AZ deployments as can be
 seen in the feature comparison table below.               

A website runs on a Microsoft Windows server in an on-premises data center. The web server is being migrated to Amazon EC2 Windows instances in multiple
Availability Zones on AWS. The web server currently uses data stored in an on-premises network-attached storage (NAS) device.
Which replacement to the NAS file share is MOST resilient and durable

A.Migrate the file share to Amazon FSx for Windows File Server
-Amazon FSx for Windows File Server provides fully managed, highly reliable file storage that is accessible over the industry-standard Server Message 
Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and
 Microsoft Active Directory (AD) integration. It offers single-AZ and multi-AZ deployment options, fully managed backups, and encryption of data at rest
 and in transit.

The destination filesystem should be Amazon FSx for Windows File Server. This supports DFSN and is the most suitable storage solution for Microsoft 
filesystems. AWS DataSync supports migrating to the Amazon FSx and automates the process


***********************************************************************************************************************************************************


FSX FOR OPENZFS: 
- it is simple, high performance,cost effective built on the Open ZFS file system accessable over the Industry standars NFS protocol
- there are multiple capabilities that this file system has related to compression ,instant time point in snapshots,data cloning and others.

AMAZON FSX FOR FSX NetApp ONTAP : It is high performance,highly reliable storage built on NetApp's popular ONTAP file system and fully managed by AWS.
- It is accessable via the industry standard NFS,SMB,iSCSI protocols.
- It integrates with Microsoft Active directory(AD) to support windows based environments and enterprises.


***********************************************************************************************************************************************************


Amazon EFS(Elastic File System) : It provides a simple,serverless elastic file system to use with cloud services and on premise resources. 
           
           It is built to scale on demand to petabytes without disrupting applications growing and shrinking automatically as you add and remove files
           
           We just have to mount EFS on to Ec2.We can store tera or peta bytes of data it will scale automatically.
    
  * we can Attach EFS to multiple targets like Amazon EC2, Amazon ECS ,EKS, AWS lambda . WE can attach EFS to multiple targets symultaniously.
  * We can consider it as some kind of shared storage.

  - To access EFS from instance inside VPC we need to create mount target in the VPC for each availability zone.

  - EFS is just like NFS. NFS is a networking protocol for distributed file sharing. EFS uses the NFS version 4 Protocol
  
  -EfS is expensive when compared to EBS and S3.  1TB EFS with 80% frequently accessed data $250,  1TB EBS storage $102 , 1TB S3 Storage $24.

** If performance is your concern always use EBS.
   
   EFS can even be accessed from on premise datacenter using an AWS Direct connect or AWS VPN Connection
   
   With EFS you pay only for what you use.

Q:
- To increase the resiliency of the application the solutions architect can use Auto Scaling groups to launch and terminate instances across multiple
  availability zones based on demand. An application load balancer (ALB) can be used to direct traffic to the web application running on the EC2 instances.

- Lastly, the Amazon Elastic File System (EFS) can assist with increasing the resilience of the application by providing a shared file system that can be
  mounted by multiple EC2 instances from multiple availability zones

- Amazon EFS file systems in the Max I/O mode can scale to higher levels of aggregate throughput and operations per second with a tradeoff of slightly 
higher latencies for file operations. You can also mount EFS filesystems to up to thousands of EC2 instances across multiple AZs.

- A Solutions Architect needs a storage solution for a fleet of Linux web application servers. The solution should provide a file system interface and be
  able to support millions of files : Amazon EFS

A: The Amazon Elastic File System (EFS) is the only storage solution in the list that provides a file system interface. It also supports millions of files
    as requested

- After creating a file system(EFS), by default, only the root user (UID 0) has read-write-execute permissions. For other users to modify the file system,
  the root user must explicitly grant them access.

- One common use case is to create a “writable” subdirectory under this file system root for each user you create on the EC2 instance and mount it on the 
  user’s home directory. All files and subdirectories the user creates in their home directory are then created on the Amazon EFS file system



***********************************************************************************************************************************************************


AWS DataSync(storage service) : 
 * DataSync makes it simple and fast to move large amounts of data online between on-premises storage and  S3,Elastic File System (Amazon EFS), or Amazon 
   FSx for Windows File Server

- Datasync provides end to end security, including encryption and integrity validation, to help ensure that your data arrives securely, intact and ready
  to use.

Datasync supported endpoints : 
 - Nfs file servers, smb file servers,Hadoop distributed file system (HDFS),Object storage systems, all s3 bucket types ,EFS, All amazon FSX file server 
   types,all aws snowcone devices, google cloud storage buckets,azure files.

- when we discuss about datasync there are 2 types of architectures : 
  Moving data b/w aws services
  Moving data b/w aws and on premises.
  Moving data b/w aws and diff cloud env's like GCP or Azure.

- when we transfer data b/w aws and on premises and aws, there is a requirment of datasync agent. 

Alaso when we transfer data between aws storage services in such case no datasync agent is required.

Demo : there is an s3 bucket(source) and fsx( destination) . connect to ec2 instance.wher i have fsx mounted.we have a specific directory called as datasyn
       this is the folder where the datasync will be uploading the files to from s3 buclket as part of task.
- as part of datasync we can set a schedule where after every 2 hiurs the data gets synced again.

Transferring data b/w GCP and AWS s3.
 : deploy datasync agent in gcp env. agent reads your GC stoirage bucket(permission needs to be present). objects from your google cloud storage bucket
   move securely through TLS 1.2 into the AWS cloud by using public endpoint. The datasync service writes the data to s3 bucket.


Path : shared file system(using nfs or smb network) - connected to aws datasync agent ( deployed as vm and connected to our NAS or file system to copy dat
       to aws or write from aws - Network transfers trough AWS DX or internet - AWS datasync seamlesly and securrely connects to AWS s3 or AWS EFS AWS FSX
       for windows file server to copy data to and from AWS. 

DEMO : first we need datasync agent on premises.multiple ways to deploy datasync agent.easiest way is readymade ami available which contains datasync agen
       datasync configured successfully.datasync agent interacts with nfs and smb.so to replicate this we are using AW EFS.which basicslly makes use ofNFS.

       so our shared filed system suppports nfs is going through the EFS. we created efs.now efs is using default security group.So to mount efs to ourec2 
       instance, the sg associate with efs shloud allow mount. WE allow port 2049 from total vpc cidr(cidr defaut vpc=172.31.0.0/16(NFS PORT NUM : 2049) 
       now we create a datasync in aws and we give address as ec2 public address(which is datasync agent).

   till now we created on premises sared file system(NFS) we connected it to Datasync agent(ec2-instance) and we connected that to aws datasync.	 
   Now we want to perform some operation like copying files from s3 to efs. this operation datasync is referred as TASK. we create a task and we give sour
   as s3 and destination as EFS.	

- AWS DataSync is a secure, online service that automates and accelerates moving data between on-premises and AWS storage service(b/w storage systems and
  services)
- is not designed as a hybrid storage service.

Q:
- The destination filesystem should be Amazon FSx for Windows File Server. This supports DFSN and is the most suitable storage solution for Microsoft 
  filesystems. AWS DataSync supports migrating to the Amazon FSx and automates the process

- AWS DataSync can be used to move large amounts of data online between on-premises storage and Amazon S3 or Amazon Elastic File System (Amazon EFS). 
  DataSync eliminates or automatically handles many of these tasks, including scripting copy jobs, scheduling and monitoring transfers, validating data, 
  and optimizing network utilization. The source datastore can be Server Message Block (SMB) file servers


***********************************************************************************************************************************************************



AWS BACKUP : 

- AWS has lots of services where data can be stored.
- For production environment, data backup is one of the critical task.
- Taking backup at individual service level can take lot of time and require customization.
in order to overcome this aws has launched a service called as AWS backup.
- AWS Backup is a fully-managed service that allows customers to configure backup policies in one central place
benefit of this service is that we can go ahead and configure all of the backup and its associated policies at central place.
we have a aws backup service where we go ahead and configure the backup policies and other retention related configuration and aws backup will take care 
 of the backup of all of the prod env's at a ascheduled time also takes care ofthe retention.

 Benefits of using AWS Backup
- Easily create backup rules for daily, monthly backups.
- Backup Process is automated at a scheduled time.
- Supports Cross-Region, Cross-Account Backups.
- AWS Backup can back up on-premises Storage Gateway volumes and VMware virtual machines
- Supports Retention Period that tells how long to store backup.


***********************************************************************************************************************************************************

AWS Snowball Family : 
use case : Organization A has hosted all of it’s storage infrastructure in data-center. 
Total Storage: 500 TB.
They have now decided to use S3 due to the benefits that it provides.
since they are planing to use s3. They need to migrate all 500Tb to s3.

- AWS has released some of the offerings as part of the snowball family.the main aim of snowball related device is to Allows customers to Accelerate
 moving offline data or remote storage to the cloud.
- here AWS team will be sending some storage devices to the customer.customer will load the storage data in the device and they will ship back storage
  device to AWS.AWS team they have internal network to the AWS services.and from internal network they will load the data to Services like S3.

-Vice versa : lot of data in the s3 bucket.and the customer wants the data into their onpremise location.in a similiar way the aws team can download the 
 data from s3 ship it back to customer through storage device.customer download the data locally and deletes it from the AWS device and ship back to aws.

- Along with the primary aim of moving the data across the aws and customer networks the snowball devices also come upwith new set of features called as th
  edge computing.

- with edge computing we can run our applications in EC2 instances that are running inside the devices so that you can work in edge environments with
 limited connectivity.(means benefit is that let's say we are working in remote networks like desert or underground mines ,oil fields,which doesn't have
  a internet connectivity at all.if we have some custom applications that we want to run may be i want to perform some image or video processing,ML etc
  in such cases not only storage will help you but also local computing also imp. This type of functionality also comes with snowball set of devices.

- maybe we can process the data locally.once we have the internet connectivity back after months or year we can plug it back to the network and upload the
  data back to aws. 

Snowcone :
AWS Snowcone is a small, rugged, and secure device offering edge computing, data storage, and data transfer on-the-go, in severe environment with little or
 no connectivity.
- Can carry in backpack, drones and others.
- 8 TB of usable storage
 
Snowball Edge
- AWS Snowball Edge is a type of Snowball device with on-board storage and compute power for select AWS capabilities
- Available in Multiple Storage Capacity like 100 TB, 40 TB and others.
- the overall compute power of snowball edge is much more higher when compare with snowcone.

TYPES : 
- Snowball Edge Storage Optimized (for data transfer) :This option has a 100 TB (80 TB usable) storage capacity.

- Snowball Edge Storage Optimized (with EC2 compute functionality) : This option has up to 80 TB of usable storage space, 24 vCPUs, and 80 GB of memory for
                                                                   compute functionality
                 We can run applications in ec2 instances in these devices without need of any internet connectivity.

- Snowball Edge Compute Optimized : Most compute functionality, with 104 vCPUs, 416 GB of memory, and 28 TB of dedicated NVMe SSD for compute instances.

- Snowball Edge Compute Optimized with GPU :Identical to the Compute Optimized option, except for an installed GPU
  incase orgs GPU for image or video analysis and others then we have snowball edge compute optimized with GPU.

- AWS Snowmobile: AWS Snowmobile moves extremely large amounts of data to AWS. 
  Transfer up to 100 PB per Snowmobile, a 45-foot-long ruggedized shipping container pulled by a semi-trailer truck.

- AWS OpsHub is a graphical user interface you can use to manage your AWS Snowball devices.

OLD :

AWS SnowBall : 50Tb = $200 ; 80Tb = $250   ; first 10 days is free. and after that each day costs $15
    Data transfer into S3 is free ; data transfer out from S3 is charged per region.
 
- With snowball we can transfer 100's of terabytes or petabytes of data between on premises and S3
- in us region 50tb and 80tb options available. In other regions only 80tb is available.
- data is encrypted in transit
- if we want to transfer less than 10tb of data between on premises and s3, snow ball is not economical option



***********************************************************************************************************************************************************
    

NETWORKING : 


***********************************************************************************************************************************************************


VPC ENDPOINTS: without vpc endpoints if we want to connect to other services like s3 or dynamodb from ec2's we have to use internet.
               using internet we might face high latency, and transferring sensitive data over might be insecure.That's where VPC endpoint come into pictre

 using vpc endpoint we can connect VPC to other aws services(S3,Dynamodb any aws service) or any other supported services over aws private network.
 here instead of internet gateway we have VPC endpoint.  


** AWS PrivateLink is a technology that enables you to privately access services by using private IP addresses.(aws private link the name itslf will say it
  is a link privately inside aws network) To use AWS PrivateLink, you can create a VPC endpoint for a service in your VPC.
- VPC Endpoint allows us to connect VPC to another AWS services over AWS network.
- Traffic between your VPC and the other service does not leave the Amazon network.

Three types of VPC endpoints :
   1)Gateway endpoint
   2)Interface endpoint(AWS Private Link)
   3)Gateway loadbalancer endpoint

1)Gateway VPC endpoint : it works on a route table that connects to VPC endpoints. in this route table we give destination as vpc endpoint that contains
                         cidr block of the other AWS service
  CONS: vpc endpoint is cresated outside we vpc and routed via route table. this is not possible to use from vpn's or diret connects.
        END points are supported in same region only. we can't connect end point in one vpc to service in anothger aws region.
        End point supports ipv4 only 

2)Interface endpoint(AWS private link): New generation endpoint.
* an interface endpoint is a elastic network interface with a private ip address from the ip address range of your subnet.
* it serves as an entry point for traffic destined to a supported aws service or a vpc endpoint service

- Path : private subnet EC2- interface vpc endpoint - api gateway, cloudformation,cloudwatch

pros : 

* interfac endppoits make use of security groups to restrict access to endpoints
* VPN's and direct connections are supported 
* interface endpoints support lots of services unlike gatyeway vpc endpoint.


VPC endpoint services : 

PRIVATE LINK : Aws private link is good , however it used to be limited with interface and gateway endpoints which aws used to offer.it would have been 
    much more better if service providers can create their own application which consumers will be able to consume (via private link).aws did it like that
 

* private link is a connection between one customer and specific service provider. VPC endpoint service is like this.
  EX: there is a service provider DAtaDog which we need to upload our metrics through internet earlier.now without internet is requirewd.
      if datadog is installed on a ec2 server and it is producing metrics ,we can send metrics manually or we can install a agent to collect metrics then
      we can send that logs to VPC Endpoint service which is connected to NLB in the same region.Now NLB can distribute the metrics to the required places.


Q : 

- An Interface endpoint uses AWS PrivateLink and is an elastic network interface (ENI) with a private IP address that serves as an entry point for 
  traffic destined to a supported service. Using PrivateLink you can connect your VPC to supported AWS services, services hosted by other AWS accounts 
  (VPC endpoint services), and supported AWS Marketplace partner services.

- AWS IoT Core is a managed cloud service that lets connected devices easily and securely interact with cloud applications and other devices. AWS IoT Core
  can support billions of devices and trillions of messages, and can process and route those messages to AWS endpoints and to other devices reliably and 
  securely.

- VPCs can be shared among multiple AWS accounts. Resources can then be shared amongst those accounts. However, to restrict access so that consumers 
  cannot connect to other instances in the VPC the best solution is to use PrivateLink to create an endpoint for the application. The endpoint type will be
  an interface endpoint and it uses an NLB in the shared services VPC

- A company has deployed an API in a VPC behind an internal Network Load Balancer (NLB). An application that consumes the API as a client is deployed in
  a second account in private subnets. Which architectural configurations will allow the API to be consumed without using the public Internet

  Configure a PrivateLink connection for the API into the client VPC. Access the API using the PrivateLink address
  Configure a VPC peering connection between the two VPCs. Access the API using the private address

  You can create your own application in your VPC and configure it as an AWS PrivateLink-powered service (referred to as an endpoint service). Other AWS 
  principals can create a connection from their VPC to your endpoint service using an interface VPC endpoint. You are the service provider, and the AWS 
  principals that create connections to your service are service consumers.

- Interface endpoint ,gate way endpoint, private link.
  A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring
  an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection.

  Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service 
  does not leave the Amazon network.

  With a gateway endpoint you configure your route table to point to the endpoint. Amazon S3 and DynamoDB use gateway endpoints.

  Interface endpoint : 
  - it uses dns entries to redirect traffic.
  - Supported services : Api gateway,cloudformation,cloudwatch
  - Security : security groups.
  - It is a elastic network interface with private IP.

  Gateway endpoint : 
  - it uses prefix lists in the route table to redirect traffic.
  - Supported services :  Amazon S3, Dynamodb
  - Security : vpc endpoint policies.
  - it is a gateway that is a target for specific route.

- A gateway VPC endpoint can be used to access an Amazon S3 bucket using private IP addresses. To further secure the solution an S3 bucket policy can be
  created that restricts access to the VPC endpoint so connections cannot be made to the bucket from other sources.




DIRECT CONNECT : 

         -> internet is good if amount of traffic is within a limit. there are always latencies which can be involved. 
         -> some companies might have hybrid architecture. Datacenter + AWS . for these applications latency become more issue.
 To solve this AWS introduced DX(Direct connect). Using this we can create a dedicatd network connection between datacenter and AWS.we can bypass internet

PROS : Consistent network performance
       reduced bandwidth price
       private connectivity to our aws vpc

path : Premises(CPE router (customer premises equipment) ) -> MPLS Router(Multi protocol label switching) -> DX provider -> private DX -> VPC gateway 


Q :

- A large multinational retail company has a presence in AWS in multiple regions. The company has established a new office and needs to implement a high-
  bandwidth, low-latency connection to multiple VPCs in multiple regions within the same account. The VPCs each have unique CIDR ranges.What would be the
  optimum solution design using AWS technology?

  Create a Direct Connect gateway, and create private VIFs to each region 
  Implement a Direct Connect connection to the closest AWS region  

- The company should implement an AWS Direct Connect connection to the closest region. A Direct Connect gateway can then be used to create private virtual 
  interfaces (VIFs) to each AWS region.Direct Connect gateway provides a grouping of Virtual Private Gateways (VGWs) and Private Virtual Interfaces (VIFs)
  that belong to the same AWS account and enables you to interface with VPCs in any AWS Region (except AWS China Region).

  You can share a private virtual interface to interface with more than one Virtual Private Cloud (VPC) reducing the number of BGP sessions required.

- AWS Direct Connect provides a secure, reliable and private connection. However, lead times are often longer than 1 month so it cannot be used to
  migrate data within the timeframes. Therefore, it is better to use AWS Snowball to move the data and order a Direct Connect connection to satisfy the
  other requirement later on. In the meantime the organization can use an AWS VPN for secure, private access to their VPC

- An organization is extending a secure development environment into AWS. They have already secured the VPC including removing the Internet Gateway and
  setting up a Direct Connect connection. What else needs to be done to add encryption :  Setup a Virtual Private Gateway (VPG)

- A VPG is used to setup an AWS VPN which you can use in combination with Direct Connect to encrypt all data that traverses the Direct Connect link. This 
  combination provides an IPsec-encrypted private connection that also reduces network costs, increases bandwidth throughput, and provides a more 
  consistent network experience than internet-based VPN connections.

  An AWS Direct Connect Gateway is used to connect to VPCs across multiple AWS regions

- Three Amazon VPCs are used by a company in the same region. The company has two AWS Direct Connect connections to two separate company offices and 
  wishes to share these with all three VPCs. A Solutions Architect has created an AWS Direct Connect gateway. How can the required connectivity be 
  configured : Associate the Direct Connect gateway to a transit gateway

  You can manage a single connection for multiple VPCs or VPNs that are in the same Region by associating a Direct Connect gateway to a transit gateway.
  The solution involves the following components:
  – A transit gateway that has VPC attachments.
  – A Direct Connect gateway.
  – An association between the Direct Connect gateway and the transit gateway.
  – A transit virtual interface that is attached to the Direct Connect gateway.

- A Solutions Architect needs to select a low-cost, short-term option for adding resilience to an AWS Direct Connect connection. What is the MOST cost-
  effective solution to provide a backup for the Direct Connect connection : Implement an IPSec VPN connection and use the same BGP prefix

  With this option both the Direct Connect connection and IPSec VPN are active and being advertised using the Border Gateway Protocol (BGP).The Direct 
  Connect link will always be preferred unless it is unavailable.

- Provision a Direct Connect connection between your on-premises location and AWS and create a target group on an ALB to use IP based targets for both 
  your EC2 instances and on-premises servers



TRANSIT GATEWAY (TG) : TG is a network transit hub that we can use to interconnect our vpc's and onpremises networks.

-> we cannot connect DX to TG.  
-> we can attach vpc or vpn to TG.
-> TG route table includes Dynamic or static routes that decide the next hop based on destination ip address of the packet.
    [ Static routes are configured in advance of any network communication. Dynamic routing, on the other hand, requires routers to exchange information 
    with other routers to learn about paths through the network ]
-> attachment can be asociated with a single route table
-> vpc or vpn can dynamically propagate routes to TG route table.
-> with VPC we must create a static routes to TG Route table.
-> with VPN connection routes are propagated from transit gateway to our onpremises router using (Border gateway protocol) BGP>
  
NEW : 


use case : before going to Transit gateway , before when org's wants to communicate with multiple vpc's each other then standard approach is vpc peering.
  but vpc peering is not traversal ( like if a connects to b and b connects to c, then a and b is not conncted)

- more the no of vpc's more the no of vpc peering connection you have to eastablish for inter connectivity related use case.in order to simplify this aws
  has released a feature of transit gateway.
- we have 4 vpcs.instead of using peering we can establish or deploy a central transit gateway and we connect all of the transit gateway to the transit 
  gateway.
- there are multiple type of attatchments that we can to attatch with transit gateway
in the fig : we have two vpcs ,one site to site vpn(to one data center), and one direct connect (to another data center)


Q :

- to simplify AWS infrastructure with providing  IP multicast = Use AWS Transit Gateway

- AWS Transit Gateway is a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single
  gateway. 



EGRESS ONLY INTERNET GATEWAYS: 

* Specially designed for vpc's which has IPV6 enabled.
* ec2 instances can also initiate internet connection using IPV6 addresses. IPV6 addresses are unique and they are public by default.
* Egress only is just like NAT gateway. it allows instances with public IPV6 to connect to internet and ,prevents connecttion from internet to ec2.

PLACEMENT GROUPS : 

* placement groups are recommended for the applications that require low latency and high network throughput
       throughput : amount of material or items passing through a system or process
* a single server can run multiple ec2 instances.this can lead to issues if we are running cluster of instances on a same physical host(sevrver )
* if any 2 ec2's are running on a same underlying host using virtualization this can be problematic.
* with placement group we can explicitly specify that two instances should not be part of same physical host.

Three types of placement groups : 
 1) Custer PG : pack instances close to each other in a availability zone.

 2) Partition PG : aws ensures that each partition within a placement group  has its own set of racks.
    EX: if we have 3 partitions each partition  will have multiple ec2 instances. each partition will reside in a seprate rack inside a data center

 3) Spread PG : it is a group of instances that are each placed on a different rack. 
    EX: if we have 7 instances in a spread PG,then each instance will be placed in separte rack.

* a cluster PG can't span multiple AZ's.
* only specific types of ec2's can be launched.
* max network throughput between two instances is limited by slower of the two instance.(if 10mbps-ec21,20mbps-ec22; then 10mps is max network thoughout) 
* recommended to launch all instance together.launching instance later can lead to capacity errors.in such case stop and start all ec2's in PG.

Q : 

- Cluster placement groups are recommended for applications that benefit from low network latency, high network throughput, or both. They are also 
  recommended when the majority of the network traffic is between the instances in the group. To provide the lowest latency and the highest 
  packet-per-second network performance for your placement group, choose an instance type that supports enhanced networking.

- It is recommended to use either enhanced networking or an Elastic Fabric Adapter (EFA) for the nodes of an HPC application. This will assist with 
  decreasing latency. Additionally, a cluster placement group packs instances close together inside an Availability Zone

  Using a cluster placement group enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication
  that is typical of HPC applications.


*************************************************************************************************************************************************

LOAD BALANCING and it's solutions : 

*************************************************************************************************************************************************
HIGH AVAILABILITY : architecture in such a way that even if one server fails there has to be one more(backup or secondary)  server to take the load if 
                    primary server fails.
* Availability refers to the amount of time that the system is in functioning condition
* General Availability:     100% minus system downtime 
* PINGDOM is a wbsite that we can monitor server how many times it went down and what's availability percentage.
* outages means the no of times that server down.

RTO AND RPO : Highly available and multi AZ  is certainly possible ,however money is associated with it.

RTO(recovery time objective) : amount of time frame it takes to recover your infrastructure after a disaster has struck.

RPO(Recovery Point Objective) : Recovery Point Objective (RPO) is concerned with data and the maximum tolerance period to which data can be lost. 
 
  EX : If RPO is 5 hours for the database, then we should be taking backup of the database every five hours. 

* RPO is more directly related to interval of backup to take , to avoid data loss.





LOAD BALANCING : LB
 
* load balancing popular tools : NGINX and HA Proxy. if we are using these LB's to communicate b/w costomer and server, disadvantages are there.
  CONS : HA of LB's and performance and security issues.we have to manage all these things.
* to overcome all these cons AWS has come with it's own load balancer :ELASTIC LOAD BALANCER(ELB)
* elb has Tight integration with multiple AWS services. like WAF,ASG,Aamazon certificate manager.

LB types :
   1) Application Load Balancers
   2) Network Load Balancers.
   3) Classic Load Balancers
   4) Gateway load balancer .
* LB's might be different but their common duty is to LOAD balance between multiple targets.
 
1) Application Load Balancer : Application LB makes routing at application layer(which is HTTP/HTTPS layer)
  
- Use when you have websites/applications at L7 (HTTP/HTTPS)

- ALB is at layer 7 of OSI model(OPEN SYSTEMS INTERCONNECTION MODEL).
- whenever we search anything using HTTP or HTTPS along with domain it is based on ALB. based on the rules (/payment or /videos) 

- HTTP headers let the client and the server pass additional information with an HTTP request or response.
- The request are routed based on the URI path : /videos  /payment  ( example.com/videos  )
- The request are routed based on the Host Header : ( example.com , kplabs.in ) 


2) Network Load Balancer : NLB makes routing decisions at transport layer(TCP/UDP/SSL)(transmission controlprotocol,user datagramprotoco,secure socket layr
 
* it can handle millions of requests per second.
* probably we can say that which ever application not making use of HTTP/HTTPS we can use NLB.
* NLB is at layer 4 of OSI model(OPEN SYSTEMS INTERCONNECTION MODEL)
* Ultra high performance.

3) Gateway load balancer : GLB allows us to deploy,scale,manage virtual appliances such as firewalls.
* use when u have virtual IDS/IPS  appliances (intrusion detection and prevention systems, and deep packet inspection systems)
Use when you have virtual appliances : IDS/IPS Firewalls

4) Classic Load Balancers : CLB makes routing decisions at either the transport layer(TCP/SSL) or the application layer(HTTP/HTTPS).
 * previous genration CLB.not recommended to use.

Q :

An NLB is ideal for latency-sensitive applications and can listen on UDP for incoming requests. As Elastic Load Balancers are region-specific it is 
   necessary to have an NLB in each Region in front of the EC2 instances.
-  To direct traffic based on optimal performance, AWS Global Accelerator can be used. GA will ensure traffic is routed across the AWS global network to
   the most optimal endpoint based on performance.

-  A company’s staff connect from home office locations to administer applications using bastion hosts in a single AWS Region. The company requires a 
   resilient bastion host architecture that requires minimal ongoing operational overhead:use a Network Load Balancer.Create an Auto Scaling group with 
   EC2 instances in multiple Availability Zones.


AWS WAF (Next Gen firewall) : protect aginst layer 7 attacks	

- usually normal Firewall works on the Layer 3 and  Layer 4 of the OSI model.(means network and transport layer). WAF can really look into layer 7 
  attcks(application) specifically where http operates.
  non-http attacks can already be differe at the standard packet firewall. however http attack cannot be read or stopped by standard packet firewall. this
is why there is a need of firewall for layer 7 attacks where applications or web apps are running.
 This is where WAf introduced

- Main aim of firewall:  Block malicious and unauthorized traffic.
 However what about malicious traffic like SQL Injection attacks, XSS and many more ?

* WAF designed for web applications and these kind of firewall operates at layer 7. it applies set of rules for the HTTP based conversations.
* WAF generally are deployed to protect against attacks targeted towards application, specifically the ones defined in the OWASP Top 10 metrics.
* WAF can associate with ALB,CLOUDFRONT,API GATEWAY.
WAF vendors : akamai, aws waf are some of the commercial vendors that offer WAF related functionalities. CDN network like cloudflare also provide WAF suppo



Understanding AWS WAF Concepts : 
I live in a place A in Bangalore and want to meet my friend living in place B in Bangalore.
Rule Statement:   If traffic is less on the roads?
                  Are there any Uber / OLA available?

Rules:  If traffic is less AND uber ola available then yes or no

WebACL:  Container for all the things + default action.
Association: Zeal 

Rule Statements
Rule Statements define basic characteristics that would be analyzed within a web request.
These can be custom-defined or you can use ready-made ones from AWS and marketplace.

1. Block all the requests which are coming from out of India.
2. Block request which has a URI Path of /admin
You can even build custom condition based on:
Headers, HTTP Method, Query Strings, URI Path, Geo-Location, Body.

Rules in WAF
We can combine multiple statements into rules to precisely target requests.
WAF provides two primary rule types: Regular Rule & Rate-Based rule
Let’s look into sample regular  rule:
1. If a request comes from 172.30.0.50
2. Request has SQL-like code 
Rules with multiple statements can be AND, OR, NOT
Rate-Based rule = Regular Rule + Rate limiting feature
1. If a request comes from 172.30.0.50
2. If requests exceeds 1000  request in 10 minutes 

Web ACL in WAF :Web ACL is a centralized place that contains the rules, rule statements and associated configuration. 
It is used to define the protection strategy.

Association in WAF : Association defines to which entity WAF is associated to.WAF cannot be associated with EC2 instances directly.
Support Association:    ALB and CloudFront, API Gateway


Important Pointers 
Rule Groups can be configured which has multiple rules that can be used across multiple Web 
ACLs.
Customers can decide to use ready-made AWS-Managed rules or even rules from AWS 
Marketplace.
Every Rule has a priority. If a request matches Priority 0 rule, none of the other rules will inspect 
the request
Pricing Aspect:
Web-ACL ($5 per month), Rule ($1 per month), Requests ($0.60 / 1 million )


Q :

An application has been deployed on Amazon EC2 instances behind an Application Load Balancer (ALB). A Solutions Architect must improve the security
   posture of the application and minimize the impact of a DDoS attack on resources : Configure an AWS WAF ACL with rate-based rules. Enable the WAF ACL on
   the Application Load Balancer

- A rate-based rule tracks the rate of requests for each originating IP address, and triggers the rule action on IPs with rates that go over a limit. You
  set the limit as the number of requests per 5-minute time span.

- You can use this type of rule to put a temporary block on requests from an IP address that's sending excessive requests. By default, AWS WAF aggregates
  requests based on the IP address from the web request origin, but you can configure the rule to use an IP address from an HTTP header, like X-Forwarded
  -For, instead.


***********************************************************************************************************************************************************
LOAD BALANCING AND AUTOSCALING QUESTIONS COMBINED : 
*************************************************
- BEST PRACTICE TO KEEP ELB IN PUBLIC SUBNETS

- Connection draining is enabled by default and provides a period of time for existing connections to close cleanly. When connection draining is in action
  an CLB(ELB) will be in the status “InService: Instance deregistration currently in progress”.

  Session stickiness uses cookies and ensures a client is bound to an individual back-end instance for the duration of the cookie lifetime.
  The Proxy Protocol header helps you identify the IP address of a client when you have a load balancer that uses TCP for back-end connections.
  Deletion protection is used to protect the ELB from deletion.

- You can use VPC Flow Logs to capture detailed information about the traffic going to and from your Elastic Load Balancer. Create a flow log for each 
  network interface for your load balancer. There is one network interface per load balancer subnet

- The ELB Application Load Balancer can route traffic based on data included in the request including the host name portion of the URL as well as the path
  in the URL. Creating a rule to route traffic based on information in the path will work for this solution and ALB works well with Amazon ECS.

- The AWS Web Application Firewall (WAF) is available on the Application Load Balancer (ALB). You can use AWS WAF directly on Application Load Balancers
  (both internal and external) in a VPC, to protect your websites and web services.

  Attackers sometimes insert scripts into web requests in an effort to exploit vulnerabilities in web applications. You can create one or more cross-site
  scripting match conditions to identify the parts of web requests, such as the URI or the query string, that you want AWS WAF to inspect for possible
  malicious script


- Typically, the nodes of an Internet-facing load balancer have public IP addresses and must therefore be in a public subnet. To keep your back-end 
  instances secure you can place them in a private subnet. To do this you must associate a corresponding public and private subnet for each availability 
  zone the ELB/instances are in).

- If any health check returns an unhealthy status the instance will be terminated. For the “impaired” status, the ASG will wait a few minutes to see if 
  the instance recovers before taking action. If the “impaired” status persists, termination occurs. Unlike AZ rebalancing, termination of unhealthy
  instances happens first, then Auto Scaling attempts to launch new instances to replace terminated instances.


- An application in an Amazon VPC uses an Auto Scaling Group that spans 3 AZs and there are currently 4 Amazon EC2 instances running in the group. What 
  actions will Auto Scaling take, by default, if it needs to terminate an EC2 instance
 
  Terminate an instance in the AZ which currently has 2 running EC2 instances , Send an SNS notification, (if configured) 

  Auto Scaling can perform rebalancing when it finds that the number of instances across AZs is not balanced. Auto Scaling rebalances by launching new EC2
  instances in the AZs that have fewer instances first, only then will it start terminating instances in AZs that had more instances

  Auto Scaling can be configured to send an SNS email when:
  – An instance is launched.
  – An instance is terminated.
  – An instance fails to launch.
  – An instance fails to terminate

- The failover routing policy is used for active/passive configurations. Alias records can be used to map the domain apex (example.com) to the Elastic 
  Load Balancers.


- Configure the web tier security group to allow only traffic from the Elastic Load Balancer

  The web servers must be kept private so they will be not have public IP addresses. The ELB is Internet-facing so it will be publicly accessible via it’s
  DNS address (and corresponding public IP).

  To restrict web servers to be accessible only through the ELB you can configure the web tier security group to allow only traffic from the ELB. You 
  would normally do this by adding the ELBs security group to the rule on the web tier security group


- To increase the resiliency of the application the solutions architect can use Auto Scaling groups to launch and terminate instances across multiple
  availability zones based on demand. An application load balancer (ALB) can be used to direct traffic to the web application running on the EC2 instances.

  Lastly, the Amazon Elastic File System (EFS) can assist with increasing the resilience of the application by providing a shared file system that can be
  mounted by multiple EC2 instances from multiple availability zones.


- to allow web servers to be public accessed : the ELB must be in the public subnet and all instances must be in the private subnet
  to provide secure internet access : attach internet gateway and use security groups to control communications between the layers.

  You must create public subnets in the same Availability Zones as the private subnets that are used by your private instances. Then associate these 
  public subnets to the internet-facing load balancer.

  Public subnet
   If a subnet’s default traffic is routed to an internet gateway, the subnet is known as a public subnet. For example, an instance launched in this subnet
   is publicly accessible if it has an Elastic IP address or a public IP address associated with it.

  Private subnet
   If a subnet’s default traffic is routed to a NAT instance/gateway or completely lacks a default route, the subnet is known as a private subnet. For 
   example, an instance launched in this subnet is not publicly accessible even if it has an Elastic IP address or a public IP address associated with it.

- since the monolithic application is multi-layer + require high scalability + high performance = Use EC2 instances for Web && application Servers +RDS
  + Application load balancer+ Auto scaling group.

- The default health checks for an Auto Scaling group are EC2 status checks only. If an instance fails these status checks, the Auto Scaling group 
  considers the instance unhealthy and replaces it

  You can attach one or more target groups (Application Load Balancers and Network Load Balancers), one or more load balancers (Classic Load Balancers), or
  both to your Auto Scaling group. However, by default, the group does not consider an instance unhealthy and replace it if it fails the Elastic Load 
  Balancing health checks.

  To ensure that the group can determine an instance’s health based on additional tests provided by the load balancer, you can optionally configure the 
  Auto Scaling group to use Elastic Load Balancing health checks. The load balancer periodically sends pings, attempts connections, or sends requests to 
  test the EC2 instances. These tests are called health checks.

- to terminate and replace the unhealthy instances = configure the Auto Scaling group to use Elastic Load Balancing health checks.



-  A company has created a disaster recovery solution for an application that runs behind an Application Load Balancer (ALB). The DR solution consists of
   a second copy of the application running behind a second ALB in another Region. The Solutions Architect requires a method of automatically updating the
   DNS record to point to the ALB in the second Region : Amazon Route 53 health checks monitor the health and performance of your web applications, web
   servers, and other resources.

  Health checks can be used with other configurations such as a failover routing policy. In this case a failover routing policy will direct traffic to the
  ALB of the primary Region unless health checks fail at which time it will direct traffic to the secondary record for the DR ALB.

ASG :

- An issue has been reported whereby Amazon EC2 instances are not being terminated from an Auto Scaling Group behind an ELB when traffic volumes are low :
  Modify the lower threshold settings on the ASG

  The lower threshold may be set too high. With the lower threshold if the metric falls below this number for the breach duration, a scaling operation is
  triggered. If it’s set too high you may find that your Auto Scaling group does not scale-in when required.

- You can suspend and then resume one or more of the scaling processes for your Auto Scaling group. This can be useful when you want to investigate a 
  configuration problem or other issue with your web application and then make changes to your application, without invoking the scaling processes. 
  You can manually move an instance from an ASG and put it in the standby state.

  Instances in standby state are still managed by Auto Scaling, are charged as normal, and do not count towards available EC2 instance for workload/
  application use. Auto scaling does not perform health checks on instances in the standby state. Standby state can be used for performing updates/
  changes/troubleshooting etc

- You can specify which subnets Auto Scaling will launch new instances into. Auto Scaling will try to distribute EC2 instances evenly across AZs. If only
  one subnet has EC2 instances running in it the first thing to check is that you have added all relevant subnets to the configuration

- The last time a new product was launched, the application experienced a performance issue due to an enormous spike in traffic. Management decided that
  capacity must be doubled this week after the product is launched:Add a Scheduled Scaling action	

* The cooldown period is a configurable setting for your Auto Scaling group that helps to ensure that it doesn’t launch or terminate additional instances
  before the previous scaling activity takes effect so this would help. After the Auto Scaling group dynamically scales using a simple scaling policy, it
  waits for the cooldown period to complete before resuming scaling activities.

  The CloudWatch Alarm Evaluation Period is the number of the most recent data points to evaluate when determining alarm state. This would help as you can 
  increase the number of datapoints required to trigger an alarm

- The best solution is to create an AMI of the EC2 instance, and then use it as a template for which to launch additional instances using an Auto Scaling
  Group. This removes the issues of performance, scalability, and redundancy by allowing the EC2 instances to automatically scale and be launched across
  multiple Availability Zones.


***********************************************************************************************************************************************************


AUTO-SCALING & ELASTICITY :  


***********************************************************************************************************************************************************


AWS Global Accelerator : it improves the availability and performance of our applications with local or global users.

ADV :
 EX : we have have multiple copies of our website in multiple region so that over all latency for our users in minimal. one of copy running in singapore
      and another in north vergenia.Now we have global accelator.ANy user who requests our website request goes through the global accelerator. Now two
      users from india and canada. User from india will be directed to nearby region which is singapore through global accelerator where the overall 
      latency is less.now another user request from canada goes to north verginia region through global accelerator for lower letency.
       So both the user gets traffic served with minimal latency.
   
     * if one of the region goes down then automatically the user will directed to the working region.

    * It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Region.
PATH : User - EDGE location - Aws global accelerator - endpoint groups - endpoint

Q :

-  AWS Global Accelerator is used for improving availability and performance for Amazon EC2 instances or Elastic Load Balancers (ALB and NLB)

- AWS Global Accelerator is a service that improves the availability and performance of applications with local or global users. You can configure the ALB
  as a target and Global Accelerator will automatically route users to the closest point of presence.

- Failover is automatic and does not rely on any client side cache changes as the IP addresses for Global Accelerator are static anycast addresses. Global
  Accelerator also uses the AWS global network which ensures consistent performance.



- AWS Global Accelerator is a service in which you create accelerators to improve availability and performance of your applications for local and global
  users. Global Accelerator directs traffic to optimal endpoints over the AWS global network. This improves the availability and performance of your 
  internet applications that are used by a global audience. Global Accelerator is a global service that supports endpoints in multiple AWS Regions, which
  are listed in the AWS Region Table.

- By default, Global Accelerator provides you with two static IP addresses that you associate with your accelerator. (Or, instead of using the IP addresses
  that Global Accelerator provides, you can configure these entry points to be IPv4 addresses from your own IP address ranges that you bring to Global 
  Accelerator.)

- The static IP addresses are anycast from the AWS edge network and distribute incoming application traffic across multiple endpoint resources in multiple 
  AWS Regions, which increases the availability of your applications. Endpoints can be Network Load Balancers, Application Load Balancers, EC2 instances,
  or Elastic IP addresses that are located in one AWS Region or multiple Regions.

- AWS Global Accelerator uses static IP addresses as fixed entry points for your application. You can migrate up to two /24 IPv4 address ranges and choose
  which /32 IP addresses to use when you create your accelerator.

- This solution ensures the company can continue using the same IP addresses and they are able to direct traffic to the application endpoint in the AWS 
  Region closest to the end user. Traffic is sent over the AWS global network for consistent performance.

- The Architect needs to ensure only 2 IP addresses need to be whitelisted. The solution should intelligently route traffic for lowest latency and provide
  fast regional failover : Launch EC2 instances into multiple regions behind an NLB and use AWS Global Accelerator

- An NLB is ideal for latency-sensitive applications and can listen on UDP for incoming requests. As Elastic Load Balancers are region-specific it is 
  necessary to have an NLB in each Region in front of the EC2 instances.

  To direct traffic based on optimal performance, AWS Global Accelerator can be used. GA will ensure traffic is routed across the AWS global network to
  the most optimal endpoint based on performance.



Elastic Beanstalk :
 
* AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications  and services.
* You can simply upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to 
  application health monitoring. 
** when we create an elasticbeanstack environment,when env created automatically elastic beanstack will go and create the underlying resources for us(ec2,
   asg, all the configuration for our resource)
 * elastic beanstalk not only creates infrastructure, it also create web server configuration ( any lang related packeges,ec2 server softwares) 


Q :

- the fastest and simplest way to deploy your application on AWS using AWS Management Console, a Git repository, or an integrated development environment
  (IDE) without any infrastructure or resource configuration work on your side : Elastic Beanstalk


****************************************************************************************************************************************************

IAM : 
Stephen maarek
- IAM Section
- Advanced Identity in AWS


Q : 

- With MySQL, authentication is handled by AWSAuthenticationPlugin—an AWS-provided plugin that works seamlessly with IAM to authenticate your IAM users
- AUTH command : This is used with Redis databases, not with RDS databases.

- You can only apply one IAM role to a Task Definition so you must create a separate Task Definition. A Task Definition is required to run Docker 
  containers in Amazon ECS and you can specify the IAM role (Task Role) that the task should use for permissions.

- With the EC2 launch type you can apply IAM roles at the container and task level, whereas with Fargate you can only apply at the task level.

- Migrate the account using the AWS Organizations console
- Accounts can be migrated between organizations. To do this you must have root or IAM access to both the member and master accounts. Resources will remain
  under the control of the migrated account.

- IAM roles for ECS tasks enabled you to secure your infrastructure by assigning an IAM role directly to the ECS task rather than to the EC2 container 
  instance. This means you can have one task that uses a specific IAM role for access to S3 and one task that uses an IAM role to access DynamoDB.
  IAM roles can be specified at the container and task level on EC2 launch type and the task level on Fargate launch type.

- With IAM roles for EC2 instances you assign all of the IAM policies required by tasks in the cluster to the EC2 instances that host the cluster.This does
  not allow the secure separation requested.

- The images must be encrypted at rest in Amazon S3. The company does not want to spend time managing and rotating the keys, but it does want to control 
   who can access those keys.

- SSE-KMS requires that AWS manage the data key but you manage the customer master key (CMK) in AWS KMS. You can choose a customer managed CMK or the AWS
  managed CMK for Amazon S3 in your account.

- Customer managed CMKs are CMKs in your AWS account that you create, own, and manage. You have full control over these CMKs, including establishing and 
  maintaining their key policies, IAM policies, and grants, enabling and disabling them, rotating their cryptographic material, adding tags, creating 
  aliases that refer to the CMK, and scheduling the CMKs for deletion.

- For this scenario, the solutions architect should use SSE-KMS with a customer managed CMK. That way KMS will manage the data key but the company can
  configure key policies defining who can access the keys.

  An application will gather data from a website hosted on an EC2 instance and write the data to an S3 bucket. The application will use API calls to 
  interact with the EC2 instance and S3 bucket.Which Amazon S3 access control method will be the MOST operationally efficient?

- Create an IAM policy, Grant programmatic access   
- Policies are documents that define permissions and can be applied to users, groups and roles. Policy documents are written in JSON (key value pair that 
  consists of an attribute and a value).Within an IAM policy you can grant either programmatic access or AWS Management Console access to AWS S3 resources.

   Use Athena for querying the data and writing the results back to the bucket
   Use IAM policies to restrict access to the bucket
-  Athena allows you to easily query encrypted data stored in Amazon S3 and write encrypted results back to your S3 bucket. Both,server-side encryption and
   client-side encryption are supported.
-  AWS IAM policies can be used to grant IAM users’ with fine-grained control to Amazon S3 buckets.


  To allow read access to the S3 video assets from the public-facing web application, you can add a bucket policy that allows s3:GetObject permission with 
  a condition, using the aws:referer key, that the get request must originate from specific webpages. This is a good answer as it fully satisfies the 
  objective of ensuring the that EC2 instance can access the videos but direct access to the videos from other sources is prevented.

- Launching the EC2 instance with an IAM role that is authorized to access the videos is only half a solution as you would also need to create a bucket
  policy that specifies that the IAM role is granted access.


  A Solutions Architect needs to work programmatically with IAM. Which feature of IAM allows direct access to the IAM web service using HTTPS to call 
  service actions and what is the method of authentication that must be used : Query API ,  Access key ID and secret access key

- AWS recommend that you use the AWS SDKs to make programmatic API calls to IAM. However, you can also use the IAM Query API to make direct calls to the
  IAM web service. An access key ID and secret access key must be used for authentication when using the Query API.

- Create an IAM policy that applies folder-level permissions   ;  Create an IAM group and attach the IAM policy

- Create an IAM policy with permissions to DynamoDB and assign It to a task using the taskRoleArn parameter

  To specify permissions for a specific task on Amazon ECS you should use IAM Roles for Tasks. The permissions policy can be applied to tasks when creating
  the task definition, or by using an IAM task role override using the AWS CLI or SDKs. The taskRoleArn parameter is used to specify the policy.

- AWS Security Token Service(STS) that enables you to request temporary, limited privilege credentials for IAM Users or Federated Users).

  AWS Security Token Service (AWS STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access 
  Management (IAM) users or for users that you authenticate (federated users). 

  manager asked you to temporary give trusted AWS accounts access to resources that you control and manage :  AWS STS

- The IAM role can be assigned permissions to the database instance and can be attached to the EC2 instance. The instance will then obtain temporary 
  security credentials from AWS STS which is much more secure.

- IAM permissions-related Access Denied errors and Unauthorized errors need to be analyzed and troubleshooted by a company. AWS CloudTrail has been
  enabled at the company : Search CloudTrail logs with Amazon QuickSight. Create a dashboard to identify the errors

  CloudTrail logs are stored natively within an S3 bucket , which can then be easily integrated with Amazon QuickSight. Amazon QuickSight is a data 
  visualization tool which will show any IAM permissions-related Access Denied errors and Unauthorized errors.

- Set an IAM permissions boundary on the IAM role that explicitly denies attatching the administrator policy.

  The permissions boundary for an IAM entity (user or role) sets the maximum permissions that the entity can have. This can change the effectivepermissions
  for that user or role. The effective permissions for an entity are the permissions that are granted by all the policies that affect the user or role.
 
  Within an account, the permissions for an entity can be affected by identity-based policies, resource-based policies,permissions boundaries,Organizations
  SCPs, or session policies.

- A company is working with a strategic partner that has an application that must be able to send messages to one of the company’s Amazon SQS queues. The
  partner company has its own AWS account : Update the permission policy on the SQS queue to grant the sqs:SendMessage permission to the partner’s AWS 
  account

  IMP Amazon SQS supports resource-based policies. The best way to grant the permissions using the principle of least privilege is to use a resource-base 
  policy attached to the SQS queue that grants the partner company’s AWS account the sqs:SendMessage privilege.



***************************************************************************************************************************************************

SERVERLESS : 

****************************************************************************************************************************************************

* with server based architecture we have so many problems : 

  - we have to Understand the capacity requirements  ;  - we have to take care of security, patching, monitoring, auto-scaling.
  - we have to Launch EC2 instance in HA   ; - Install the python packages, pip, package dependencies etc.

* new startups where they can't hire a experienced people will prefer serverless architecture.
  - that's why serverless architecture and platform as a service got high dimand.

AWS LAMBDA : 

EX : I have a lambda function which basically makes a video to smaller size format. A user uploads a video to s3 That is a event. as soon as video gets 
     uploadto s3  the function will run. it will convert the videos to smaller size.
     - Whenever a configured event(uploading video to s3) occurs, the lambda function runs
     - only gets charged based on the compute time your function consumed

Serverless : it does not mean that the servers are not present. It indicates that the servers are present but are completely managed by cloud provider.
             as a user we only have to focus on function or code. entire server managemnt part is taken care by backend provider.

AWS Lambda is a fully managed compute service that runs your code in response to an event or a time based interval.
   - an event can be various things like someone uploading an object to s3 or deleting an object to s3 is an event
   - time based interval : if we want our lambda function to run for every 15 min interval it can do that with time based interval.
- we only charge for the time lambda function runs(if it runs 1min we only pay for 1min.) we don't pay for the resources which are idle.

* for lambda iam role is req if it wants to connct with s3 or vpc.
* We are charged based on number of request for our functions and time the code executes. 

************************************************************************************************

API : It is generally used for inter-communication between multiple softwares.without api also we can do it .but with api we have many benefits.

EX : let's assume i have a function(antivirous software) and database which contains signatures. and i have user. as discussed we can't allow an ssh acce
     to the user where they can upload their files to scan.if we want user to run that function we create a middle layer(API). Now user will call api or
     make a curl request to api by uploading a file. The api then interact with the backend function,the backend function will process according to
     functionality. and then it will send result back to the user.

- when we create an api we need a backend function(it can be anti virous,image processing,hellow world software.
- whenever we design api the method(GET,POSt,PUT,DELETE) that the api will support depends on the functionality of the backend application.

EX : we created a lambda function which will print "hello from lambda". we created a event which will wakeup lambda function. 
     - now if any user want to executive our lambda function they need an api gateway. where any user when they call api ,the api will forward the request
       to lambda function.
      Different types of api(HTTP Api.rest api private which can be accessed from vpc only , REST API : which can be accessed from internet.) 
     we creatd a api gateway which is integrated with Lambda function (we can also integrate api with HTTP,vpc link,aws service).
     while creating api we are giving permission to api to invoke lambda function. now we get the link to use api which will give output of "Hellofromlamb

API Gateway endpoints : 

1)Edge-optimized API endpoints  : best for geographically distributed clients ; API requests are routed to the nearest CloudFront Point of Presence (POP).
                                  This is the default endpoint type for API Gateway REST APIs

2)Regional API endpoints : A regional API endpoint is intended for clients in the same region

3)Private API endpoints  : it is an API endpoint that can only be accessed from your Amazon Virtual Private Cloud (VPC) using an interface VPC endpoint


Q : 

- A tool needs to analyze data stored in an Amazon S3 bucket. Processing the data takes a few seconds and results are then written to another S3 bucket. 
  Less than 256 MB of memory is needed to run the process :A.lambda fxn

- AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Lambda has a maximum execution time
  of 900 seconds(15min) and memory can be allocated up to 3008 MB. Therefore, the most cost-effective solution will be AWS Lambda.

- Create a VPC Security Group for the ELB and use AWS Lambda to automatically update the CloudFront internal service IP addresses when they change
  As these are updated from time to time, you can use AWS Lambda to automatically update the addresses.This is done using a trigger that is triggered when 
  AWS issues an SNS topic update when the addresses are changed.

- You can use an OAI to restrict access to content in Amazon S3 but not on EC2 or ELB.

- The Architect prefers to focus on value-add activities such as software development and product roadmap development rather than provisioning and
  managing instances: Amazon API Gateway and AWS Lambda

- You are developing an application that uses Lambda functions. You need to store some sensitive data that includes credentials for accessing the database
   tier. You are planning to store this data as environment variables within Lambda. How can you ensure this sensitive information is properly secured

A. Use encryption helpers that leverage AWS Key Management Service to store the sensitive information as Ciphertext

- Environment variables for Lambda functions enable you to dynamically pass settings to your function code and libraries, without making changes to your 
  code. Environment variables are key-value pairs that you create and modify as part of your function configuration, using either the AWS Lambda Console, 
  the AWS Lambda CLI or the AWS Lambda SDK. You can use environment variables to help libraries know what directory to install files in, where to store 
  outputs, store connection and logging settings, and more.

- Amazon CloudFront caches content closer to users at Edge locations around the world. This is the lowest latency option for uploading content.API Gateway
  and AWS Lambda are present in all options. DynamoDB can be used for storing session state data. This is a 100% serverless application.

- You can use a Lambda function to process Amazon Simple Notification Service notifications. Amazon SNS supports Lambda functions as a target for messages
  sent to a topic. This solution decouples the Amazon EC2 application from Lambda and ensures the Lambda function is invoked.

- Every time an item in an Amazon DynamoDB table is modified a record must be retained for compliance reasons :  Enable DynamoDB Streams. Configure an AWS 
  Lambda function to poll the stream and record the modified item data to an Amazon S3 bucket

- AWS Lambda and Amazon RDS Aurora MySQL. The Lambda function must use database credentials to authenticate to MySQL and security policy mandates that
  these credentials must not be stored in the function code : Store the credentials in Systems Manager Parameter Store and update the function code and 
  execution role

  In this case the scenario requires that credentials are used for authenticating to MySQL. The credentials need to be securely stored outside of the 
  function code. Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management.You can
  easily reference the parameters from services including AWS Lambda

  Use the AWSAuthenticationPlugin and associate an IAM user account in the MySQL database” is incorrect. This is a great way to securely authenticate to
  RDS using IAM users or roles. However, in this case the scenario requires database credentials to be used by the function.

- To enable your Lambda function to access resources inside your private VPC, you must provide additional VPC-specific configuration information that 
  includes VPC subnet IDs and security group IDs. AWS Lambda uses this information to set up elastic network interfaces (ENIs) that enable your function

- Configure S3 event notifications to trigger a Lambda function when data is uploaded and use the Lambda function to trigger the ETL job
  Use AWS Glue to extract, transform and load the data into the target data store 

- You can use a Lambda function to process Amazon Simple Notification Service notifications. Amazon SNS supports Lambda functions as a target for messages
  sent to a topic. This solution decouples the Amazon EC2 application from Lambda and ensures the Lambda function is invoked.

  You cannot invoke a Lambda function using Amazon SQS. Lambda can be configured to poll a queue, as SQS is pull-based, but it is not push-based like SNS 
  which is what this solution is looking for.

- to perform processing on a files in S3 bucket = Use S3 event + Amazon Lambda Function

- You can use Lambda to process event notifications from Amazon Simple Storage Service. Amazon S3 can send an event to a Lambda function when an object is
  created or deleted. You configure notification settings on a bucket, and grant Amazon S3 permission to invoke a function on the function’s resource-based
  permissions policy.

- Store the password in AWS Secrets Manager. Associate the Lambda function with a role that can retrieve the password from Secrets Manager given its 
  secret ID. we can use this with database.

- Firstly, S3 is a highly available and durable place to store these JSON documents that will be written once and read many times (WORM). As this
  application runs thousands of times per day, AWS Lambda would be ideal to use as it will scale whenever the application needs to be ran, and Python is a
  runtime environment that is natively supported by AWS Lambda, whenever the events arrive in the S3 bucket, and this could be easily achieved using S3
  event notifications. Finally Amazon Aurora is a highly available and durable AWS managed database. 

- A company has created an application that stores sales performance data in an Amazon DynamoDB table. A web application is being created to display the
  data. A Solutions Architect must design the web application using managed services that require minimal operational maintenance.

  There are two architectures here that fulfill the requirement to create a web application that displays the data from the DynamoDB table.

  The first one is to use an API Gateway REST API that invokes an AWS Lambda function. A Lambda proxy integration can be used, and this will proxy the API
  requests to the Lambda function which processes the request and accesses the DynamoDB table.

  The second option is to use an API Gateway REST API to directly access the sales performance data. In this case a proxy for the DynamoDB query API can
  be created using a method in the REST API.


- A company runs a business-critical application in the us-east-1 Region. The application uses an Amazon Aurora MySQL database cluster which is 2 TB in
  size. A Solutions Architect needs to determine a disaster recovery strategy for failover to the us-west-2 Region. The strategy must provide a recovery
  time objective (RTO) of 10 minutes and a recovery point objective (RPO) of 5 minutes : 

  recreate the database as an Aurora global database with the primary DB cluster in US-east-1 and a Secondary DB cluster in Us-west-2. use an Amazon 
  eventbridge rule that invokes an AWS lambda function to promate the DB cluster in US-west-2 when failure is detected.

  "Create a cross-Region Aurora MySQL read replica in us-west-2 Region. Configure an Amazon EventBridge rule that invokes an AWS Lambda function that 
  promotes the read replica in us-west-2 when failure is detected" is incorrect. This may not meet the RTO objectives as large databases may well take
  more than 10 minutes to promote.


API : 

- Per-client throttling limits are applied to clients that use API keys associated with your usage policy as client identifier. This can be applied to the
  single customer that is issuing excessive API requests. This is the best option to ensure that only one customer is affected.

  Server-side throttling limits are applied across all clients. These limit settings exist to prevent your API—and your account—from being overwhelmed by
  too many requests.
 
  Per-method throttling limits apply to all customers using the same method. This will affect all customers who are using the API.

  Account-level throttling limits define the max steady-state request rate and burst limits for the account. This does not apply to individual customers.



-  An API cache is not enabled for a method, it is enabled for a stage
   You cannot use Amazon ElastiCache to cache API requests.

-  Amazon API Gateway decouples the client application from the back-end application-layer services by providing a single endpoint for API requests.

-  Access keys are a combination of an access key ID and a secret access key and you can assign two active access keys to a user at a time. These can be
   used to make programmatic calls to AWS when using the API in program code or at a command prompt when using the AWS CLI or the AWS PowerShell tools

- You can enable API caching in Amazon API Gateway to cache your endpoint’s responses. With caching, you can reduce the number of calls made to your 
  endpoint and also improve the latency of requests to your API.

  When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds. API Gateway 
  then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint. The default TTL value for
  API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled.

- A company has deployed an API in a VPC behind an internal Network Load Balancer (NLB). An application that consumes the API as a client is deployed in
  a second account in private subnets. Which architectural configurations will allow the API to be consumed without using the public Internet

  Configure a PrivateLink connection for the API into the client VPC. Access the API using the PrivateLink address
  Configure a VPC peering connection between the two VPCs. Access the API using the private address

  You can create your own application in your VPC and configure it as an AWS PrivateLink-powered service (referred to as an endpoint service). Other AWS 
  principals can create a connection from their VPC to your endpoint service using an interface VPC endpoint. You are the service provider, and the AWS 
  principals that create connections to your service are service consumers.

- APi gateway is supported by Interface endpoint.

- The key requirement is to limit the number of requests per second that hit the application. This can only be done by implementing throttling rules on 
  the API Gateway. Throttling enables you to throttle the number of requests to your API which in turn means less traffic will be forwarded to your 
  application server

- The individual microservices are not designed to scale. Therefore, the best way to ensure they are not overwhelmed by requests is to decouple the 
  requests from the microservices. An Amazon SQS queue can be created, and the API Gateway can be configured to add incoming requests to the queue. The
  microservices can then pick up the requests from the queue when they are ready to process them.

- The application is writing the files using API calls which means it will be compatible with Amazon S3 which uses a REST API. S3 is a massively scalable
   key-based object store that is well-suited to allowing concurrent access to the files from many instances.


************************************************************************************************************

Amazon Athena : is a service that allows us to analyze various log files from a data source using  standard SQL 
  
  
  EX : if i have large amount of data in s3 that i want to analyze.now we can write a sql query and send it to athena,athena depending on sql query it 
       will go and analyze data in s3 and give result.


Q :

-  Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no
   infrastructure to manage, and you pay only for the queries that you run. Amazon Athena supports encrypted data for both the source data and query 
   results, for example, using Amazon S3 with AWS KMS.

- Use Athena for querying the data and writing the results back to the bucket
  Use IAM policies to restrict access to the bucket

- Athena allows you to easily query encrypted data stored in Amazon S3 and write encrypted results back to your S3 bucket. Both,server-side encryption and
  client-side encryption are supported.

  RedShift Spectrum for the complex queries   
- Amazon Athena for the ad hoc SQL querying 
- Performing in-place queries on a data lake allows you to run sophisticated analytics queries directly on the data in S3 without having to load it into a
  data warehouse

- You would typically use Athena for ad hoc data discovery and SQL querying, and then use Redshift Spectrum for more complex queries and scenarios where 
  a large number of data lake users want to run concurrent BI and reporting workloads.


******************************************************************************************************************

Amazon AppFlow: it is a fully managed integration service that helps you securely transfer data between SaaS applications(Salesforce,Slack,
                linkedin,gitla,github,google search engine,facebook ads  ) and aws services (AWS S3 or redshift).  
   - using aws appflow we can transfer the data between the app services( linkedin,gitla,github,google search engine,facebook ads) to aws services.

*******************************************************************************************************************************

AWS Cost Explorer : using this we can visualize,understand and manage our aws costs and over time.
     - explorer have many filter options(instance type,region,accoiunt etc). we can also change visualization to stack,graph,bar. 
       we can watch daily or monthly costs.

**************************************************************************************************************

AWS Transfer Family : it securely scales your recurring business-to-business file transfers to AWS Storage services using SFTP, FTPS, FTP protocols
 
ex : if we have existing applications that makes use of SFTP, FTPS, FTP protocols.now if want to connect and transfer file to aws s3 or efs.but ftp or sftp
     do not support protocols related to s3 or efs. this becomes an issue. that's why there is a middlemen through transfer family  which allows the data 
     from existing applications to upload or download the data through and from s3 or efs through sftp,ftp,ftps protocol via the aws transfer family.
    

SERVERLESS LATEST :

SIMPLE EMAIL SERVICE(SES):
- Amazon SES is an email platform that provides an easy, cost effective way for you to send and receive email using your own email address and domains
- whenever we register for a website or a org you will get a generic emails like noreply@example.com.there can be  a requirment to send 1000's of emails
  individyuals who are regestering etc.and in order for that they can make use of Amazon ses service.
- if we don't have ses service then we have to take care of our own mail server.
- Ses also gives a great set of dashboards related statistics of the emails,rejects,the overall bounces etc.

HOW EMAIL SENDING WORKS IN AMAZON SES : 
- Email sender makes a request to SES to send email to recepients.
- if the request is valid,SES accepts the email.
- SES sends the messages over the internet to the recepients server.
Bounce notifications (emails not exist ) & complaints(feedback)are sent back to SES which then forwards it to the sender.
- EMAIL FORMAT IN AWS SES : 
FORMATTED : construct simple test message using the form provided.
RAW : for complex use cases like using html por attatcments.





******************************************************************************************************************************************************** 

DOCKER(CONTAINER SERVICES )

********************************************************************************************************************************************************
* typical installation of a software- workflow : i have a windows OS. i want to install an application.

1) the first step is to download a installer. installer would be (.exe file) 
 executable file(.exe)is a computer file that contains an encoded sequence of instructions that the system can execute directly when clicks the file icon

2) after installing installer will go and double click on installer.
3) in the middle of installation we will get a error saying that "some dependencies are missing or files are misssing)
4) we start to troubleshoot the issue 
5) after installing the specific dependencies we re run the installer
6) we will get another error ,which might not the same as previous error. we will go to 4th step again.
 this workflow is the end user workflow.

- there is anoter workflow related to development.
 1.a developer who works for developing a software;he has developed a specific software; now depending upon the way in which he has created the software 
   it might or might not work on the OS out of the box. ex ; if has created a software in vs code it might work on Windows.but it will not work in linux
    and mac.we can use java to work on all.but we don't want to use. so it is a pain for software. so what he can do? docker container.

 2. he create a software and put it in a docker container.now he can deploy docker container in windows,linux,mac. he's 100% sure that he has tested
    docker container in his local env will work perfectly in Windows, mac , linux. this really becomes much more easier for developer to push his software

    to wide variety of OS's and also all the dependencies which are required by the software we can put docker container itself. now end user doesn't have
    to do anything other than running the docker container.

DOCKER : Docker is an open platform, once we build a docker container we can run it anywhere say it windows,mac linux whether on laptop,datacenter,cloud.
       - It follows build once run anywhere approach. means once the developer created software in a container he can run it anywhere(windows,linux,mac) in
         a perfectly running condition depending upon the  test that he has performed while he has created the container.

DEMO : we will be taking NGINX. nginx is a popular webserver in the market. we will consider nginx as a software .
       nginx support amazon linux,debian,ubuntu,redhat. but it doesn't support windows. if we try to install in windows it doesn't have windows .exe file.
 - now if we put nginx in docker container we can install in windows.

* 2 steps that involve for using docker : this specific docker conatiner image (nginx software) it would be in a specific website.dockerhub is one such web
  site.now we have to pull or download this image in our OS where we want to start the container from it.for that we have a command of"docker pull nginx".
- once the image is downloaded we can go and start the container from it.(before downloading docker image we should install docker daomen in our OS.)
- docker image is downloaded now we will start contaner from it.

  Command : docker run -p 8080:80 -dt nginx 
 -p : port mapping 8080 to the 80 of container
  Unlike some other types of servers, webservers do not negotiate encryption. If it's encrypted (https), it must be sent to 443/8443.
  If it's plain text (http) it must be sent to 80/8080

command : docker ps  : show the list of docker containers running. we see nginx container running.
now if we search 127.0.0.1:8080 : we see nginx html page.

localhost is generally the address 127.0. 0.1;  but the :8080 part means to connect to port 8080 instead of the default port 80.
Localhost is the default name of the computer you are working on. The term is a pseudo name for 127.0.0.1, the IP address of the local computer. 
This IP address allows the machine to connect to and communicate with itself.



ELASTIC CONTAINER REGISTRY ( ECR ) : Stroing container images ;

- use case : google playstore 
  we have android phones.playstore comes with installed in phones. in playstore developers can go ahead and they can add their apps,games,books etc.
  whenever we want to install an app like whatsapp i can dwnld it from playstore.we can say google play store acts as a central repository for apps.
- and for ios applications apple has its own store.

- the central repository concept helps alot for users and developers. developers in the sense that whatever new changes or updates that they want to push 
  they can add it in the central appstore.the benefit for the users is they don't have to run around multiple places, there is a single location from whi
  ch the data can be fetch or the android apps can be fetched.

- similiar way when we discuss about docker containers, there are docker containers based on various images like nginx,mongodb,alpine,redis,ubuntu etc.
  since there are so many containers that are available , there is a req where all of them can be stored in a central place similiar to the google playstor
  
- the central place for containers referred to as container registry.
- container registry is a single place for your team to manage docker images.
- whenever we want to launch a docker container,the associated image is pulled from registry. means whenever we want a docker container we will need image
  let's say launc a docker container based on ubuntu.so we have to have a ubuntu image firsr.this ubuntu image can be fetched from container registry from
  which the container can be launched.

- dockerhub is the popular container registry.here we find most of the public images. generally when we pull a specific docker image from docker(hub.docker
  .com) is the website which image will be pulled.
  ex ; ubuntu image (docker pull ubuntu ) this will pull ubuntu image fro docker hub to our server.and from this image container can be launched.

******************************************************************************************************************************************************** 

ELASTIC CONTAINER REGISTRY ( ECR ) 

ECR : AMAZON ECR  is a fully managed container registry for storing docker images. similar to docker hub where docker images stored centrally.
 ex : we have 2 docker images present in ecr.ec2 instances can directly pull the images from ecr.
demo : go to ecr console and click on repositories. i have one repository. inside that i hav one docker image. i can go and pyull the docker image from
  registry using it enndpoint or domain name.

Q. docker hub already there with various docker images why do we need ecr ? although the docker hub is present but many org's aprt from using public imgs
   they also have various customized images avilable for their appl's.and for cutomized set of images incase if we would like to use docker hub there are
   multiple plans that are available based on price. 
 so instead of that org's can also make use of ECR . ECR supports private registtry and public registry as well.


CONTAINER ORCHESTRATION : container orchestration is all about managing the life cycles of containers, especially in large,dynamic containers.

EX : i have 3 virtaul machines.i have webserver container and app server container in both vm1 and vm2.vm3 is empty. if we have small environment like this
     we can go and start the container manually. but there are a lot of aspects other than running the container. one among them is monitoring. so let's 
     imagine webserver conatiner in vm1 went down. if our prod environment to work perfectly  we need to have a min of 2 webserver containers. i install a

     container and it went down,so who will monitor this web server container. and that monitoring script should have event driven mechanism in such a way
     that if webserver container goes down restart the container for some reason if container is not restarting  then that script should automatically star
     a new webserver container in vm3. this can be achived with container orchestration.

 - container orchestration used to perform a lot of tasks.some of them :
 * provisioning and deployment of containers. ex ; if we have 100 vm's.we want to deploy containers across all vm's.doing it manually is a difficult task.
   Using docker container orchestration we can do it.

 * scaling up or removing containers to spread application load evenly.
 * movement of containers from one host to another if there is a shortage of resources. ex : if we have 2 vms. vm1 has 2 containers but it has a short of 
   ram. container orchestration tool will move the container from vm1 to vm which is much empty which has better resource available.

 * load balancing of service discovery between containers.
 * health monitoring of containers and hosts.

EX : req min of 2 web server containers all the time. i have 3 vms. if we keep both web1 and web2 in one container is risky.because vm goes down both web's
     goes down. keep both web1 and web2 in one container is good for load balancing but not good for fault tolerance.
    container orchestration  tool should be intelligent in such a way that if we mention 2 web server containers all the time " it should keep each in a 
    seperate vm's.

ECS : ECS is a orchestration tool for AWS.it has a feature of task placement. what does task placement does is it use constraints like AZ's or instance typ
      to split the containers across the vm's. if we have 2 vms,2 AZ's.one web in one VM in same AZ.for load balancing it is good.but for fault tolerance
      it is not good.we need an intelligence in such a way that each instance should be running across AZ's.if a single AZ goes down that does not affect 
      our application.this is the capability of ECS.

* there are many container orchestration solutions available.some of the popular ones : 
    
   1.Docker Swarm
   2.kubernetes
   3.Apache Mesos
   4.Elastic container service(AWS ECS) 

There are also other various container orchestration platforms available like EKS. Basically kubernetes K8s is difficult.Handling K8s as a team of 2 or 3
   devops is a bit challenging if environment is big. So there are platforms like EKS which is managed Kubernetes.So AWS takes care of managing the K8s 
   and we take care the rest	

Docker swarn Example ; hello world is running on a swarn01 node. if accedentally my swarn01(container) goes down,docker swarn orchestration tool will 
monitor whetherthe app or swarn01 is running or stopped.if it is stopped it will automatically start a new docker swan02(container) node. eventhoug if we
 write command : systemctl docker stop(stopping contaiuner manually from deamon).swarn02 will stop.docker swarn will monitor and launch a new docker
 node swarn03 container

Container Orchestration(CO) in AWS ; if our org is decided to move to docker the we have to choose right CO tool.
   
 Primarily we have 2 CO tools in AWS : ECS  ; EKS 
 Globally when we discuss about container orchestration K8s is the defacto orchestration tool that is extensively used across the platform.
 However for aws users as well,  we have good alternative as ECS we can also use.

 ECS : this can run dcker containers and manage them well.
 EKS : this can run containers also.

EKS : 
     
     1) Eks since it is useing K8S.K8S is open source.so eks also open source
     2) EKS is more complex
     3) we will get more community support in EKS

*K8S is not just specific to AWS. we will see managed K8S service in various other platforms like AZure,GCP etc. 

ECS :
   
     1) ECS is not open source.
     2) ECS is less complex.
     3) we will get less community support in ECS

Choosing right orchestrator : if we plan to work exclusively on AWS, we should choose ECS as it offers more in depth integration than Amazon EKS.
 
 * organaizations with limited expertise and insufficient resources to invest in k8s can go with ECS. because learning k8s is too diffcult.
 * if we plan to deploy containers across multiple platforms we can choose EKS. means if our org is using gcp,aws,azure we can use EKS.because k8s is 
   available in all these platforms.


ECS : container management : ECS is a highly available and fast container management service. consider it as cont orchs.
  EX : asume we have ECS.since this is a aws service.it has integrated with other aws services like ECR. ECS can fetch the container images from ECR.from
       ecs it can go and deploy containers in ec2 instances.
  * we can use ecs to run,stop, and manage containers on a cluster.

Overview of ECS : path - ECS(using IAM) - connected to EC2 instance( which has ECS agent integrated with docker. and the containers inside ec2 are running
                         with the help of docker.
Three primary components of ECS 1) Task definition 2) tasks  3) Services

1) Task definition : it contains info related to container image from which image we want to launch application from.(apache,nginx,or any custom appl
                     it also containe port info,port mapping,on which port it is running. also contain storage info.
  task def is a text file that describes one or more containers that form our applicat.	
  it contains info like OS, containers to use, ports to open,storage.

2) TASK : let's say i created a task definition based on nginx image. now this task defn by itself will not do anything.ifs we want to run appl
           in an ec2.we go and create a task.if we have 10 ec2's in a cluster we create multiple tasks to load balance traffic.

 def : after we create a task dfn for our appl withing ECS we can specify the num of tasks to run on our cluster.

3) Service : service to run and maintain our desired no of tasks 	simultaniously in ecs  cluster.
   if any of our tasks fails or stopped for any resason, the amazon ecs scheduler launches another instance based on our task defn.

ECS DEMO : create an ecs cluster.select amazon os. select min and max ec2 instances. create task dfn. in task dfn specify the container images that we 
           want to use(ngninx image).

KUBERNETES : popular container orchestration tool. K8S is an open source container orchestration engine developed by google. But now it is maintained by
             cloud native computing foundaion

K8S architecture : we have a kubernetes master .and we have worker nodes.Worker nodes are the physical servers or VM's,where the container could be running
                   k8s master is the one that will be managing the container.k8s master will recieve the commands from the user. users can send commands
                   to k8s master throiugh API,GUI,CLI. k8s mastr will process the things mentioned in the commands.

  ex : if i want to launch an app .now k8s master will check which node has more resources.if node1 has 100%cpu used it can't launch app in node1.
       k8s master will launch the container in free worker node. 
    Also if any worker node stopped working k8s master will detect the stopped worker node and it will shift the container to another working node.

 demo : we have said to k8s master to launch a container from nginx image.  now k8s master look into worker nodes and it will launch that specific
        pod(container) in one of the worker nodes. if we do kubectls get pods.it will show running container.now we will stop that container using docker
        ps manually.now k8s master will detect the pod is not working it will go and launch the pod in another worker node or same worker node.

ELASTIC KUBERNETES SERVICE(MANAGED KUBERNETES IN AWS ) : 
 - Building and maintaining entire k8s clustr takes a lots of time and resources
 - managing k8s manually means updation latest k8s packages,security,HA,load balacing.normal org's can't manage all these things.

 -now cloud providers are managing all the k8s managemnt(ha,security etc).
 - we just have to manage worker nodes which will have applications inside.
 - ( worker nodes are launched as ec2 instances).

* EKS provides tight integration with various other AWS services like ECR,IAM,ELB to provide end to end features for application deployments.

AWS FARGATE : before aws fargate,if we want to run any container orchastrator(docker swarn,ecs,k8s),we have to crewate a cluster of ec2 instances and
              we have to deploy containers on tye ec2's.we have to manage security(patch management,vulnarabilities,sg's) of ec2, we have to maintain 
              the incoming traffic to ec2. all these things we have to manage.is there any easier way ? yes.aws fargate.

 - in the serverless approach we don't have to worry abpout provisioning andmanaging of ec2.
 - aws fargate is a serverless,pay as you go compute engine that's let's you focus on building applications without managing servers.
 - container orcgastrator can directly deploy containers and fargate will take care of everything remainib


***********************************************************************************************************************************************************

Q :

- Some Amazon ECS containers are running on a cluster using the EC2 launch type. The current configuration uses the container instance’s IAM roles for 
  assigning permissions to the containerized applications.A Solutions Architect needs to implement more granular permissions so that some applications can
  be assigned more restrictive permissions. How can this be achieved : This can be achieved using IAM roles for tasks, and splitting the containers 
  according to the permissions required to different task definition profiles 

- The ECS container agent is included in the Amazon ECS optimized AMI and can also be installed on any EC2 instance that supports the ECS specification
  (only supported on EC2 instances). Therefore, you don’t need to verify that the agent is installed.

You need to verify that the installed agent is running and that the IAM instance profile has the necessary permissions applied.
Troubleshooting steps for containers include:
– Verify that the Docker daemon is running on the container instance.
– Verify that the Docker Container daemon is running on the container instance.
– Verify that the container agent is running on the container instance.
– Verify that the IAM instance profile has the necessary permissions.

-  A Python application is currently running on Amazon ECS containers using the Fargate launch type.An ALB has been created with a Target Group that routes
   incoming connections to the ECS-based application. The application will be used by consumers who will authenticate using federated OIDC compliant 
   Identity Providers such as Google and Facebook. The users must be securely authenticated on the front-end before they access the secured portions of the
   application :It can be done on the ALB by creating an authentication action on a listener rule that configures an Aws Cognito user pool with the 
   social IdP

- Create an IAM policy with permissions to DynamoDB and assign It to a task using the taskRoleArn parameter

- To specify permissions for a specific task on Amazon ECS you should use IAM Roles for Tasks. The permissions policy can be applied to tasks when creating
  the task definition, or by using an IAM task role override using the AWS CLI or SDKs. The taskRoleArn parameter is used to specify the policy.

- Amazon ECS uses the AWS Application Auto Scaling service to scales tasks. This is configured through Amazon ECS using Amazon ECS Service Auto Scaling.

- Use CloudWatch Container Insights to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices. 
  Container Insights is available for Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Kubernetes Service (Amazon EKS), and Kubernetes 
  platforms on Amazon EC2.

- With Container Insights for EKS you can see the top contributors by memory or CPU, or the most recently active resources. This is available when you
  select any of the following dashboards in the drop-down box near the top of the page:
- ECS Services
- ECS Tasks
- EKS Namespaces
- EKS Services
- EKS Pods

- Secondly the compute layer needs to not only be containerized, and implemented with the least possible operational overhead. The option that best fits
  these requirements is Amazon ECS on AWS Fargate, as AWS Fargate is a Serverless, containerized deployment option

- An application on Amazon Elastic Container Service (ECS) performs data processing in two parts. The second part takes much longer to complete. How can
  an Architect decouple the data processing from the backend application component : process each part using a seperate ecs task. Create an sqs queue to
  decouple tasks.

- IAM roles for ECS tasks enabled you to secure your infrastructure by assigning an IAM role directly to the ECS task rather than to the EC2 container 
  instance. This means you can have one task that uses a specific IAM role for access to S3 and one task that uses an IAM role to access DynamoDB.
  IAM roles can be specified at the container and task level on EC2 launch type and the task level on Fargate launch type.

-With IAM roles for EC2 instances you assign all of the IAM policies required by tasks in the cluster to the EC2 instances that host the cluster. This does
 not allow the secure separation requested.


- You can only apply one IAM role to a Task Definition so you must create a separate Task Definition. A Task Definition is required to run Docker 
  containers in Amazon ECS and you can specify the IAM role (Task Role) that the task should use for permissions.

-  With the EC2 launch type you can apply IAM roles at the container and task level, whereas with Fargate you can only apply at the task level. 

EKS : 

- Amazon EKS is a managed service that can be used to run Kubernetes on AWS. Kubernetes is an open-source system for automating the deployment, scaling, 
  and management of containerized applications.Applications running on Amazon EKS are fully compatible with applications running on any standard Kubernetes
  environment, whether running in on-premises data centers or public clouds. This means that you can easily migrate any standard Kubernetes application to
  Amazon EKS without any code modification.

- This solution ensures that the same open-source software is used for automating the deployment, scaling, and management of containerized applications 
  both on-premises and in the AWS Cloud.

fargate :


- Amazon RDS is a managed service and you do not need to manage the instances. This is an ideal backend for the application and you can run a MySQL 
  database on RDS without any refactoring. For the application components these can run on Docker containers with AWS Fargate. Fargate is a serverless 
  service for running containers on AWS.

- company is deploying an analytics application on AWS Fargate. The application requires connected storage that offers concurrent access to files and 
  high performance : Which storage option should the solutions architect recommend :create an efs file system and attatch a iam role that allowes fargate
  to communicate with efs. 

- If you do not want to manage EC2 instances you must use the AWS Fargate launch type which is a serverless infrastructure managed by AWS.Fargate only 
  supports container images hosted on Elastic Container Registry (ECR) or Docker Hub









******************************************************************************************************************************************************
DATABASES : (DB)

*****************************************************************************************************************************************************

Traditional System : Earlier, many of the works were done on a traditional paper-based approach.
example:
Sending Messages (postal cards)
Tickets Booking
Government Application Form

Newer approach : Instead of storing data in a paper based , the data is now stored in a computer systems. The term it is called as Database

*A database is an organized collection of data, generally stored and accessed electronically from a computer system.

various types of databases:

1.Flat File : A flat-file database is a database that stores data in a plain text file.  Example: Excel
  * On Top of the data we will be able to run diff type of functions depending upon use cases that we want to achieve

2.Relational Database : Data is organized into tables which consists of rows and coloumns that represents the specific entity type.
  * generally it makes use of SQL(Structured query language) for managing databases. EX : MYSQL is the popular Relational DAtabase.
  * Here we run sql quiries to get desired result.

3. NoSQL Databases : NoSQL databases stores and manages data that allows very high speeds and great operational flexibility. 
  * Popular No SQL DB's Are MongoDB , and in aws DynamoDB.  
  * Here instead of storing data in the form of tables which consists of rows and coloumns we store in other forms like KEY-VALUE pair in NOSQL DB's.

Installing & Managing Databases : Depending on the organizations, they can either decide to manage a database on their own servers or use a managed 
database offerings like RDS.

  * which ever approach we choose each one comes with its own adv and disadv's.
1.Manage DB's on our own server : 
  * we have to install and provision database. once database is upon running on our prod env.then we also have to take care of host security like monthly
 patching of system, hardening of systeand others. after that we have to take care of configuring replicas, High-Availability, Upgrading, Monitoring and
 otherswith all these adv we have :

2.Managed Database Offering  : here Provisioning Database via simple GUI steps in simple and fast phase. Host Security (Patching, Hardening and others)
 taken care by the managed provider.we do't have to worry about all these things. Configure Replicas, High-Availability within a single click.

Introduction to RDS(Managed Database Service) : before going to managed rds 

Challenges with EC2 DB
Maintaining database in EC2 instance leads to multiple challenges, this includes:  Provisioning DB (it can be mysql or any other technology) ,Host Security
 (Vurnability assessment,Patching, Hardening and others).  Configure Replicas, High-Availability, Upgrading(if a new version of database has come we have
 to upgrade it) and others

- Most organizations simply offload these tasks to 3rd party vendors or hire database administrators.
in order to overcome all the above challenges aws has released RDS.

RDS(Managed Database Service) : 	Amazon RDS is a managed service that makes it easy to set up, operate, and scale a relational database in the cloud. 
so here installing and managing DB manuallin a instance or server we can launch DB through RDS and let AWS manage it for us.

- AWS manages the underlying hardware, OS, security and software patching, automated failure detection and recovery for you.
 In click of few buttons we can : 
                                  Provision / Resize hardware on-demand.
                                  configure Multi-AZ deployments.
                    	            Create read replicas.
*in RDS wehenever we create a DB we get an endpoint identifier(.rds.amazon.com).this is the urlthrough which we can connect to the DB MYSQL or othr clients
- since rds is managed we can see monitoring and we can take care maintainance and the backups .along with that we can take care of read replicas,multi AZ 
  with a few clicks.

Creating RDS Database : go to rds console and click create databases.whenever we create DB there are multiple engines(PostgresSQl,MSQl,oracle,amazon 
aurora,MariaDb) options available. we also have template options(Prod,DEv/Test, Free tier) for prod Multi Az etc options available. for free tier there is
 no options like that. DB instance identifier means name of DB.


-Connecting RDS DB  from EC2 : 2 imp points that we need to do : we have to know the RDS endpoint. and we have to verify the security group associate with
 RDS . here we created RDS based onMYSQL engine.so the DB port here is 3306. so incase if any appl or an ec2 wants to connect to our rds DB , then the port
 3306 should be allowed in the SG associate with DB.after doing thatnow in ec2, connect to ec2 using aws cli and switch to root user.one of the imp utility
 that we need to connect to DB is MYSQl.when we type MYSQL it says command not found.so we have to install MYSQL . now command " yum -y install mariadb " 
this contains binaries of MYSQL. to connect to DB now
 
** Type[ "mysql -h hemant.us-east-1.rds.amazonaws.com -u admin -p "]   hemant.us-east-1.rds.amazonaws.com : rds endpoint  -u = user  -p = password it
   will ask for password
 - if we type " show databases " it currently shows the list of Databases we can also create Databases 
 - " create database demo " for demo database creation.
 - atlast if i press "ctrl + d " we will come out of Mysql


 RDS Read Replicas : 
simple use case : In bank, for different kind of work purpose, there are different people you might have to approach.
  For example :
   Cash Collector
   Cheque Counter ( for cheque collection we hve one person)
   Enquiry Counter( for any enquiry one person is allocated)
   - so overall the work is distributed among multiple people in bank and this allows overall oprtns to work efficiently.

* So now DB : if we are making use of a single DB for performing all the kind of activity(it can be read or write operation) in such cases the over all
 load on the DB will increase and this  will slow down the operations.(even users and servers also connecting to DB) .to overcome this

- There is a concept of Read replica that has been introduced in DB. READ REPLICA allows customers to offload read requests or analytics traffic from the
 primary instance. means  we have one single DB referred to as Primary DB. now in read replica scnario there is one more DB that has been set up and b/w
 both the DB's there is a replication that is configured. this means that any data that is written on Primary Db will also be copied to the read replica 
that has been running.

- Here all the write related operations goes to primary DB. all the read related operations goes to read replica.since there is a replication b/w ongoing
 the DB's instances .read opn when run on read replica it will not really have a impact.because the data ultimately remains to be the same.

RDS Read Replica : RDS Read Replica feature allows customers to implement “Database Read Replica” functionality for RDS databases. this has same
 functionality of read replicas discussed above.

Demo : in rds DB if we go down we can see in replicas section 2 DB : primary DB in (Us-west region) replica in Mumbai region. Endpoint of read replica 
is different from Primary DB.

- so based on our or our application requirment we can connect to DB's accordingly. if we want to write operations then we can connect to Primary DB using
 it's Endpoint. if we want to perform read operations for our applications we can connect to read replica using it's endpoint.
* we can't perform write operations on read replica after connecting to read replica.

* You can create one or more replicas to our primary DB Instance and serve high-volume application read traffic. EX : sometimes in org very huge amount 
  of read traffic and less amount of write
  traffic in such cases we need more no of read replicas that can serve the read only traffic.
* With Amazon RDS, you can create a read replica in a different AWS Region from the primary DB instance. for EX : we have a primary DB in US-East-1 region
 and our read replica in Ap-south-1region.

DEMO :first we create a primary DB using MY Sql engine and using free tier.after it is created we can go and create read replica by clicking actions on
primary DB. read replouca has littlediff conf compared to primary DB. Read replica can have a didd DB instance class.we can select any region(destination)
 for read replica. we can also give DB identifier (name of read replica ) 



Amazon RDS Multi AZ Deployments : 
*******************************

use case :if we have one DB that is running in AZ-1 . now due to some reason if that AZ goes down or may be the H/W on which DB was running goes down,then
 our entire appl will go down. becausthe heart of the appl where the data is stored is DB. this is the thing that we don't want for our prod ENV.
- it is very imp that whenever we use RDS for prod ENV we should avoid the deployments based on a single AZ only. and the better architecture is to use a
 MULTI-AZ deployments.

- here we have primary DB in one AZ and we have a secondary DB(Standby) that is running in a diff AZ in a same region. and there is a replication that 
 happens b/w both of these DB's. now due to some reason the primary DB goes down in such cases the RDS will re-direct all the req's to the standby
 instances. This is a automated proces.as a user or appl we don't have to worry .

- In which scenario this automated failover takes place from primary to standby instance. 
Failover conditions : 
If a planned or unplanned outage of your primary DB instance results from an infrastructure defect, Amazon RDS automatically switches to a standby 
replication another Availability Zone if you have turned on Multi-AZ. the conditions to switch are below : 
* Loss of availability in primary Availability Zone
* Loss of network connectivity to primary
* Compute unit failure on primary
* Storage failure on primary
- Failover times are typically 60–120 seconds
- the endpoint url of both primary and standby instances remains to be the same.

RDS PROXY  :
*********
 
first we will learn basics of DB proxy: Database Proxy is a intermediary between a user and database.where the proxy acts as a middle man b/w user and DB.
one benefit from this approach :since all of the queries from client goes through proxy , there are lot of controls and optimizations that can be applied
 at proxy level.

 EX : if i want to block a req from specific adress we can do this at poxy level itself. if such request will come it will be blocked at proxy level only
 and it never goes to DB.

Use-Case  -  FailOver Scenarios : we have a DB .and all the connections from clients are going through the proxy. now due to some reason primary DB fails.
 in such cases if there is a standby DB that is available , now proxy can go and redirect the req to standby Db instance.now client doesn'treally need to
 worry about ip address of standby DB and primary Db.because if primary DBfails the url and ip address of db will change. But since client is connecting
 to proxy , proxy takes care of redirecting the request. so the existing req's will go to the standby with this the resiliency of appl will improve. 
 otherwise if the client is directly connecting to DB if DB fails the entire APPl will go down.

Basics of Database Connections : let's suppose a client sends a write req to the Db where he wants to write the data to the DB. so behind the scenes even
 a small data is written to DB , thereare lot of steps that are taken care behind the scene. steps :
1. Application makes use of Database driver to open connection to DB.
2. Network Socket is opened in OS to connect application to DB.
3. Authentication takes place. also have data written to DB once the auth completes
4. Operation Complete and Connection can be closed from the application side.
5. Network Socket is closed.
- from here we can understand that even client wants to read or write small amount of data behind the scenes there are lot of steps taking place.this 
inetrn cost in terms of cpu and memory.there are millons of users who are using or visiting DB.maintaining the DB connection is something that will 
increase the load significantly. to overcome this there is a conecpt"Connection poling "  
* Database connection pooling is a way to reduce the cost of opening and closing connections by maintaining a “pool” of open connections.

- here we have a proxy.between a proxy and DB there is an open connection that is already established. now when the client send the req to proxysince the
 connections are already present the proxy willuse the existing connection to go ahead and perform the operation that the client has requested . similiar
 way we have a feature called RDS proxy.

* RDS PROXY : AWS RDS proxy is a fully-managed database proxy for Amazon RDS. it's features :
Connection Pooling : Improves application performance by reducing the number of open database connections.
Availability  : Makes applications more resilient to database failures byautomatically connecting to a standby DB instance while preserving application
 connections.
Authentication : Can also enforce AWS Identity and Access Management (IAM) authentication for databases,

DEmo : in this i have a RDS DB-1. and i created a RDS proxy. and i given the target as RDS DB-1(means proxy is connected to DB-1). also i have endpoint 
of rds proxy. from an ec2 i will trytoconnect to proxy .once we connect to proxy we can see all the list of DB's in DB-1. here connection pooling,
availability all taken care by proxy	


* RDS Custom : before going to it we will understand about limitations of RDS ; 
- By default, RDS does not offer any access to the underlying OS where database engine is running. for ex we see the the aws rds through console. but 
behind the scenes DB engine like mysqlor etc they are hosted on a VM or ec2's. AWS doesn't allow customers to connect to that ec2's to configure or
 optimize the underlying OS 

- a legacy old application is used many times we require customization at a OS layer where some kind of custom patches and 3rd party appl needs to be 
installed. Since RDS doesn't offer the connectivity. 	SO THIS CAN BE A LIMITING FACTOR. 

* to overcome this aws has released AWS custom. with rds custom aws allows us to connect to underlying OS and customize our DB env and OS.
* it doesn't support all the DB engines. it only supports ORACLE and MICROSOFT SQL SERVER
Benefits of RDS Custom : 
- Install third-party applications on the on the OS where the DB engine is running.
- Modify file systems to support legacy applications.
- Apply custom database patches or modify OS packages on your RDS Custom DB instances.

RDS DB instance storage : 
- RDS offers multiple options in terms of storage suitable for different set of applications.
1. Magnetic storage : consider this as slowest one among the three. aws doesn't really recommend unless and until if it is req for specific usecase.
Some of the limitations of Magnetic Storage include:
- Doesn't support storage autoscaling.
- Limited to a maximum size of 3 TiB.
- Limited to a maximum of 1,000 IOPS.

2. general purpose SSD : we will see this regularly been used in org's. this type of storage is cost effectuve tier.
- it is acceptable for most database workloads that aren't latency sensitive.
- it is best suited for development and testing environments, small scale env's
3. provisioned IOPS
- whenever we deploy a application in rds specifically in production env we should know each one to use in specific categ.

** REFER PPT FOR COMPARING STORAGES

AMAZON AURORA (closed source DB ): it is a great piece pf DB offering under relational DB
Databases are generally divided into two types:
- Open Source Databases
- Commercial or Enterprise Databases
* Commercial Offering does come with various aspects it can be performance, security that are not found in open source DB offerings. So this is where
  aurora steps in.
** Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of 
   traditional ENTERPRISE DATABASES with thesimplicity and cost-effectiveness of OPEN SOURCE DATABASES.

* amazon aurora provides Performance,Availability of Enterprise Databases with Simplicity and Cost Effectiveness of Open Source Databases
- It tries to be the best of both (open source and enterprise)

* Amazon Aurora is up to five times faster than standard MySQL databases and three times faster than standard PostgreSQL databases. it's not like 5 times
 and 3 times faster. but there are certaworkloads where aurora proves to be much more fatser than traditional mysql and postgressql
-It provides the security, availability, and reliability of commercial databases at 1/10th the cost

 generally in rds whenevr we try to make a db using Mysql ,after selecting mysql if we scroll down to storage section we can see option
 to choose iops,or gp2, or gp3.

 if we do the same thing in rds using amazon aurora engine, after slecting it if we scroll down we can't see any storage section. here we don't select 
storage. whenever we require the storage it will automatically grow without hindering the overall performance of Db.

RDS - Multi-AZ & Read Replica Architecture : In a typical setup, primary, standby and read replicas are three different instances in multiple AZ's.
      The underlying storage is EBS volume.

- imagine we have 3 AZ's. in AZ1 we have primary RDS instance running. this rds is not only multi AZ(standby in AZ2) it also has read replica(in AZ3)

- when we discuss about multi-AZ this works based on synchronous replication ;and when we discuss about rds read replica this works based on Asynchronous 
replication. and all of them in the underlying storage is EBS storage. EX : we have rds with mysql. it's underlying storage is EBS volume	

Aurora Architecture(refer ppt) :

Two Primary Components:  DB Instances + Storage Cluster Volume 
- DB instance is running in one AZ1.and storage cluster volume present across multi az's(i.e AZ1,2,3). and data is replicated b/w all of these 3 az's
-Since Aurora and Storage Layer are independent, we can scale the storage easily.not like traditional rds with mysql which makes use of ebs store volume.
- here any one az goes down.we don;t have to worry.because data is still present in 2 AZ's

 Scalability Aspect in Aurora : With this architecture, where a storage cluster volume is used instead of a EBS volume you can add a DB instance quickly
 because Aurora doesn't make a new copy of the table data. Instead, the DB instance connects to the shared volume that already contains all your data. 
means our aurora is connected to az1. but clustered storages volume is span across all the AZ's.so if we want to create read replica, we just create it
 in any other AZ and we just connect it to clustered storage vol that is already ptesent in that AZ.

Aurora Endpoints (refer ppt also) 
You can connect to Aurora Cluster through endpoints.Endpoints is Aurora Specific URL consisting of host and port.There are three primary types ofendpoints
 available:imagine we have an application that is using aurora. so appl connects to endpoints based on it's need.

- Cluster Level Endpoints :  Connects to current primary DB instance in the cluster.Used for performing write operations.
  if application wants to write something into the DB then we have to connect it to cluster level endpoint 

- Reader Level Endpoints(endpoint connected to read replicas) :  Built-In endpoints for Read Replicas.For Multiple Read Replicas, this endpoint will
  balance load among all read replicas.
 
- Instance Endpoints : it Allows application to directly connect to the rds instance. but best practices is we should use cluster or read endpoints.

- Custom Endpoints : Ability to create custom endpoints for our own requirements. 

AURORA SERVERLESS ( AUTOSCALING DATABASE) : 
- before going to serverless let's understand a typical setup : in a typical DB setup one of the primary config during Db set up Db instance size.
  ex : t2.small : let's say a developer needs a Db instance for dev env. after if our workload changes we can modify the db instance class size.

- in some env's workloads can be unpredictable.. there can be periods of heavy workloads that might last for only a few hour or minutes. and also long
 periods of low activity or even no activijust like in prod env. we can't predict the workload or traffic in prod env.

- in these cases it can be difficult to configure the correct capacity at the correct times.
  provision for peak : expensive  ; provision less than peak : end user(busines) impact ; continuesly monitor and adjust capacity manually ;
  require experts & risk outages.

now comes the aurora serverless : 
- aurora serverless scales up and down based on the capacity our workload consumes. when Db is idle, it will automatically be shut down,and when workload
 resumes it will automatically spin it backs up. it is just like ASG.we set the min and max capacity. like when the workload is low it will run with
 minimal instance size (2gb).when workload is too high it will run with max(488gb)
 
Diagram path: 
Application - connected to fleet of LB's.- Connected to active aurora instance.- connected to a shared storage - here monitoring service will check scale
 up and down - depending upon that - also we have a Warm pool of aurora instances in diff sizes(depending upon the load one of these instance size will 
 replace the active aurora instance

- here connection and amount of load on active aurora instance it will replace with much more smallr aurora instance size from a pool of aurorainstances.
  if amount of workload increases the smaller instance aurotra replaces with bigger instance size.

Overview of Global Database: Aurora Global Database allows a single Amazon Aurora database to span multiple AWS regions.
** in one sentence it is nothing but a cross region read replica.

- It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disasterrecovery
 from region-wide outages.means we have aurora master in one reg. and in diff regions we have read read replicas that are available. so if we haveglobal
 appl we can perform read directly from read replicas. incase ifmaster goes down, then we can easily promote read replica to the new master.

Replication Approach (refer ppt no.56) : 
- Data is replicated based on asynchronous replication between the storage layer of the two regions.means we have aurora in primary region. and aurora in
 secondary region. between them at a storage level there is a asynchronous replication that is happening. since the aurora storage is seperated from the 

 instance itself. at the primary region we can see that the writes that are happening fromm the primary instance to the storage layer.however in the 
 secondary region we only have reads that are happening from the aurora instance to the storage layer.

Important Pointers 
* Global Database does not support automated failover to the secondary region. This step is manual. 
* Not all instance types are supported. You can't use db.t2 or db.t3 instance classes.
* Certain features like Backtrack are not supported.
* Stopping and starting the DB clusters within the global database is not supported.
 
 RDS Storage Auto-Scaling(Auto-Scaling Storage) :
* RDS Storage Auto Scaling automatically scales storage capacity of rds instance in response to growing database workloads, with zero downtime
- while creating normal rds instance we can mention inautoscaling section " maxim storage threshold req". depending upon the iunstance type we choosethe
  max threshold would differ

Important Pointers to Remember for rds storage autoscaling( refer ppt page.no 60,61)

RDS Event Notification : RDS Event Notification provides notification when a specific type of RDS event occurs. now an event can be multiple categories 
like Availability, Configuration Change, Failure, Deletion, Low Storage and others related to rds.

EX : we have an rds. assume that rds has low storage. event would be generated. this event would be associated with sns topic. now depending upon the 
     subscription(email,sms) associated with sns topic the appropriate notification will be delivered to administraton team.

AWS ElastiCache (let's cache): 
Ex : there is a new vegetable shop.it became popular. everyday 300-400 people buy vegetables from shop. each person asks about atleast 2 vegetables. 
     this is quite a pain. so in order to reducepain veg shop owner made a dashboard that has a list of all common veg's prices.

- because of this people no longer need to ask the owner about prices. this will decrease burden on owner
- visitors can quickly go through the prices - better efficiency 

Challenges with Database Workloads : 
- There can be certain common queries within the database that hundreds of users might request.
- This would increase the load on the database and can lead to performance degradation
So to overcome this caching soln was introduced.
- With caching solutions, you can cache the response associated with frequent queries that are been made to the DB.
path : 
 a client has made a quiery to request data from DB. now Db replied with answer of query. apart from sending the data to client, Db also sent its answer 
 for the query to the caching engine.
- next time when client make the same query yet again, that query can go to the caching engine and respones can come back from caching engine itself.
- caching engines are generally in- memory and they are extremly fast. it not only increases the performance, it decreases the load on DB.

Two of the most popular caching solutions used for databases are:
1. Memcached
2. Redis
- redis is most popular. it is introduced to overcome some of the disadv associated with memcached.

To use them, you will have to install, configure, optimize and secure the EC2 instances where these engines would be running. org found this way is
 problamatic. because org has to manage allthese things. and that is where ELASTIC CACHE comes up. managed aws service

AWS ElastiCache : 
- ElastiCache is a fully managed AWS service that makes it easier to deploy, operate and scale an in-memory data-store or cache in the cloud.
- It is like a managed service and within a few clicks, we can have an in-memory layer in our infra.
- Depending upon the config ElastiCache can also detect and replace failed nodes thus reducing the overhead.
DEmo : when we search elasticcache we can see memcached and redis( select any engine)
-.AWS elastic memcached does not support high availability or multi-AZ
- DAtabase that supports high availability and replication for the caching layer is AWS elasticcache redis.

- an in-memory database that supports data replication: elastic cache for redis.(in ElastiCache Memcached there is no data replication or high availability
- A company is deploying an Amazon ElastiCache for Redis cluster.To enhance security a pswrd should be required to access the database:redis auth command
- It is important that the application is highly responsive and retrieval times are optimized. You’re looking for a persistent data store that can 
  provide the required performance : ElastiCache with the Redis engine


NoSQL Databases  : 
Before going to it : let's understand  Importance of Schema Free Structure
- In a traditional schema database like MySQL, before you start to add data in it, you must first define the structure of those records.
- ex : user data info ( refer ppt page.no : 73)
 here we have to define the structure first then only we can add some data into it. if tmrw we want to add some more data as aws no, we have to create a
 new coloumn aws no then only we can add.

Schema Free Database : 

- On Schema Free databases like MongoDB (NoSQL), you can simply add records without any previous structure.
- We can easily group records that do not have a same structure
ex : " id" = 1,
     " name " = " harsh"
     " age " = "32"
here if we want to add some more data we just have to add one more line only.

- Basics of NoSQL Database :(refer ppt.no 75)
NoSQL databases ("not only SQL") are non-tabular databases and store data differently than relational tables.
- NoSQL have gained huge popularity because they are simpler to use, flexible and can achieve performance that are very difficult with traditional
  relational databases.

There are lot of advantages of NoSQL database over standard relational databases :
- Schema(Structure) Free 
- Horizontal Scaling
- Easy Replication
- Can manage huge amount of data.

DynamoDB (Storing data NoSQL Way) : 
- an org has understood the benefits that a nosql data base provides and they have decided to use nosql db. there are multiple nosql db's like mongodb 
which is quite popular. but they want to  manage it from scratch. they want to take care HA, Security, overall config, the backups and so on.

- So instead of doing all of these things from scratch org's require a managed srvice where they have to take care is adding and removing data by appl. 
all the remaining things lik HA, config,security should manage by provider. this is where AWS and Dynamodb comes into picture

* DynamoDB is a fully managed NoSQL database service by AWS.
* Being managed service, it simplifies lots of operations like hardware provisioning, setup and configurations, patching, replication, clustering etc for
 the users. users just have to createa dynamodb table and rest is taken care by provider.

Core Components - DynamoDB : 
- In DynamoDB, tables, items, and attributes are the core components that you work with.
- Table is a collection of items, and each item is a collection of attributes
(refer ppt.no 80 ) : in that table item1 = 1 ; item2 = 2; attributes = user id, name , age , interests.

PRIMARY KEY :Each item in the table has a unique identifier, or primary key, that distinguishes the item from all of the others in the table
Other than the primary key, the table is schemaless, which means that neither the attributes nor their data types need to be defined beforehand.


DynamoDb Consistency Model : Since Dynamodb is a managed service. In DynamoDB, all of your data is stored on SSDs and is automatically replicated across 
multiple Availability Zones in an Amazon Region, which intern provide built-in high availability and data durability.

- suppose we have one dynamodb and is span across 3 AZ's. if we add any data to dynamodb, the data will be replicated across 3 AZ's. incase if one AZ goes
  down the dynamodb can still retrieve data from other AZ's

- When your application writes data to a DynamoDB table and receives an HTTP 200 response (OK), the write has occurred and is durable. 
- The data is eventually consistent across all storage locations(like all AZ's), usually within one second or less.

Eventual Consistency Reads : 
- When you read data from a DynamoDB table, the response might not reflect the results of a recently completed write operation.
- The response might include some stale data. 
- If you repeat your read request after a short time, the response should return the latest data.

Strong Consistency Reads : 
When you request a strongly consistent read, DynamoDB returns a response with the most up-to-date data, reflecting the updates from all prior write 
operations that were successful. 

1. A strongly consistent read might not be available if there is a network delay or outage. In this case, DynamoDB may return a server error (HTTP 500).

2. Strongly consistent reads may have higher latency than eventually consistent reads.

3. Strongly consistent reads use more throughput capacity than eventually consistent reads.

IMP Note :
 
DynamoDB uses eventually consistent reads, unless you specify otherwise.
Read operations (such as GetItem, Query, and Scan) provide a Consistent Read parameter.  If you set this parameter to true, DynamoDB uses strongly 
consistent reads during the operation.

Example Command: aws dynamodb get-item --table-name MusicCollection --key file://key.json --consistent-read (here we mentioned --consistent-read ;
 this will use strong consistent reads)

Read/Write Capacity Units( managing throughput)
- before we discuss about it understand throughput : Throughput in dynamodb is the maximum amount of capacity that an application can consume from
  a table or index

 Ex : we have dybamodb and we have data . now throughput pipe is b/w user and dynamodb( path : user-throughput pipe -dynamodb-data). now how much amount
 of data can we retrieve from through put pipe will depend upon the capacity that we set as read/write capacity units. suppose pipe is small and we try
 to retrieve lot data the we will go through issues which is not expectin reverse if we put big pipe and we retriev onlysmall amnt of data then we get so
 much cost 

- so in dynamodb we can specify throughput capacity in terms of read capacity units (RCUs) and write capacity units.
* for read request unit and write request unitr(REFER PPT.NO 91,92)

Capacity Modes in DynamoDB(Adjust Throughput automatically) : REFER PPT.NO 93-98

Provisioned Mode: 
- If you choose provisioned mode, you specify the number of reads and writes per second that you require for your application. 
- You can use auto scaling to adjust your table’s provisioned capacity automatically in response to traffic changes.

Recommended Traffic Patterns for Provisioned Mode :
Provisioned mode is a good option if any of the following are true:
- You have predictable application traffic.
- You run applications whose traffic is consistent or ramps gradually.(suppose i am getting 5 read req's. after 1 sec suddenly if i get 500secs.here 
  provisioned  will not be able to scale veryfast when i have spike of high.however for high spike on demand is good
- You can forecast capacity requirements to control costs.

On-Demand Mode : 
- Amazon DynamoDB on-demand is  capable of serving thousands of requests per second without capacity planning. 
- DynamoDB on-demand offers pay-per-request pricing for read and write requests so that you pay only for what you use.


DynamoDB Streams (Stream Records Real) : 
- DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for up to 24 hrs
 ex : cleint(user) is performing multple oprns related to adduser or deluser oprn. dynamodbv stores all this info in a specific log file in which the user 
      can identify the even the exact record  name and other details will be logged as part of dynamodb streams. means : 
- Applications can access this log and view the data items as they appeared before and after they were modified, in near-real time.
- this will be benfitting interms of auditing ,security,also helps while reconstructing table.

Use case : i have a dynamodb table. i have added a new item to the dynamodb table.since the dynamodb streams is enabled.the data related to new item will
   be added as part of dynamodb stream.as soon as new record added in dynamodb stream this would trigger a lambda function that has created already and
   that lambda is integrated with sns topic.and that sns topic will go and send an email or sms stating that there is a new item thathave been added in the
   dynamodb table.
- also lambda can retrieve the data from dynamodb streams and it can add it to cloud watch logs.

Use-Cases
1. Allows setting up a relationship across multiple tables in which, based on the value of an  item from one table, you update the item in a second table
   we can achieve this using lambda function.
2. Triggering an event based on a particular item change (ex arch : new item -dynamodb stream - lambda - sns-) 

3. Audit or Archive Data

4. Replicating Data Across Multiple Tables

- Amazon DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in 
   response to actual traffic patterns. This is the most efficient and cost-effective solution to optimizing for cost.
- DynamoDB DAX is an in-memory cache that increases the performance of DynamoDB

DynamoDB best practices include:
– Keep item sizes small.
– If you are storing serial data in DynamoDB that will require actions based on data/time use separate tables for days, weeks, months.
– Store more frequently and less frequently accessed data in separate tables.
– If possible compress larger attribute values.
– Store objects larger than 400KB in S3 and use pointers (S3 Object ID) in DynamoDB
- DynamoDB can be used for storing session state data


- Create an IAM policy with permissions to DynamoDB and assign It to a task using the taskRoleArn parameter

- The application will rapidly ingest large amounts of dynamic data and requires very low latency. The database must be scalable without incurring 
  downtime : Dynamodb.
- Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. Push button scaling 
  means that you can scale the DB at any time without incurring downtime. DynamoDB provides low read and write latency.

- Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement 
– from milliseconds to microseconds – even at millions of requests per second. You can enable DAX for a DynamoDB database with a few clicks.


DynamoDB - Global Table : (let's replicate) 
- Global tables feature provides us with a fully managed, multi-Region, and multi-active database that delivers fast, local, read and write performance for
  massively scaled, global applications.
ex : lot of org's has global users.users are coming from diff continents altogether.if we have dynamodb table in one specific region and we have great amnt
     of users coming from diff regions , for them diff kind of latency that would be involved.
   - so more of a global users we can make use of global tables. here we have multple dynamodb tables in a replication state (replica in asia,europe,us)
     and users can be directed one among them this can lower the latency.

- A global table is a collection of one or more replica tables, all owned by a single AWS account.
- A replica table is a single DynamoDB table that functions as a part of a global table. Each replica stores the same set of data items.
- When an application writes data to a replica table in one Region, DynamoDB propagates the write to the other replica tables in the other AWS Regions
  automatically.

imp points : 
- In a global table, a newly-written item is usually propagated to all replica tables within seconds.
- With a global table, each replica table stores the same set of data items. DynamoDB does not support partial replication of only some of the items.
- Conflicts can arise if applications update the same item in different regions at about the same time. To ensure eventual consistency, DynamoDB global
  tables use a “last writer wins”

- A large multi-national client has requested a design for a multi-region, multi-master database. The client has requested that the database be designed
  for fast, massively scaled applications for a global user base. The database should be a fully managed service including the replication
  A. DynamoDB with Global Tables and Multi-Region Replication 

- Amazon DynamoDB global tables provide a fully managed solution for deploying a multi-region, multi-master database. This is the only solution presented 
  that provides an active-active configuration where reads and writes can take place in multiple regions with full bi-directional synchronization.

- Amazon Aurora Global Database provides read access to a database in multiple regions – it does not provide active-active configuration with bi-directi
  onal synchronization (though you can failover to your read-only DBs and promote them to writable)

Database Migration Service : AWS Database Migration Service helps you migrate databases to AWS quickly and securely.
- many org's want to migrate their db's present in on premises to as cloud. dns makes this things much more simpler.
- during migration phase The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database.
- AWS Database Migration Service supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations between different database
 platforms, such as Oracle or Microsoft SQL Server to Amazon Aurora.

EX : i have aurora database this acts as source.noew i want to migrate this aurora to mysql on premises or amazon rds for mysql or amazon aurora in diff 
     region.we put a db migration service in b/w.this dms can talk to aurora, it can talk to mysql on premise based in the config we set up dms will go and
     transfer the data prsnt in aurora to any other db which is acting as a target. here source is aurora (where we want to migrate data from) , mysql is
     target db( where we want data to shift to) .

AMAZON NEPTUNE(Let's Monitor everything) : 
There are multiple different types of database technologies available and each has its own set of benefits.
Some of the popular ones include:
1. Relational Database [MySQL]
2. NoSQL Databases [key/value] [DynamoDB]
3. Graph Databases [Neptune]

Graph Database : 
-  A graph database stores nodes and relationships instead of tables.
- Whenever connections or relationships between entities are at the core of the data that you're trying to model, a graph database is your natural choice.
- You can easily find out who the "friends of friends" of a particular person are—for example, the friends of Howard's friends.

usually in social media apps graph database is useful
AMAZON NEPTUNE : Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work
                 with highly connected datasets.
- In many cases the Neptune workbench can create a visual diagram of your query results.


AWS SECRETS MANAGER : 
- in many org's secrets are hard coded as part of application or server.
  ex : i have an ec2 instance which want to communicate to the database.and there is some application running on ec2.for ec2 to communicate with DB cred's 
  are needed like dbusername,DBpswrd.all of these cred's generelly see hardcoded as part of ec2 config or appl.in this case if server is compromised, the 
  credentials also leaked.incase if these credentials hardcoided as part of the application anyone that has access to the cental repo where the appl code 
  is stored they also can access cred's.

- if we want to rotate the secret credential, all the application server need to be updated.if we miss one the prod can go down.
To overcome all these challenges we have a Secret management that has been intoduced
- Secret managemnt is a practice that allows developers to securely store sensitive data,such as passwords,keys,tokens in a secure env with strict access
  controls.
 here instead of hardcoding the credentails as part of ec2,the cred's are stored in a dedicated tool in this case it is hashicorp vault.And when ever appl
 wants to communicate with DB,then appl can connect to tools.
Many popular tools related to Secrets management : Hashicorp vault,AWS SECRETS MANAGER.

-  Here the cred's are stored in Secrets manager And when ever applwants to communicate with DB,then appl can connect to Secrets manager.
- aws secrets manager is as features rich as hashicorp vault.it does the basics aspects of secrets management.
Demo : go to secrets manager console.we can see that there are two secrets that are created 
- secrets manager has integration with rds.




- AWS Secrets Manager enables customers to rotate, manage and retrieve database credentials, API keys and other secrets throughout their lifecycle.
generally Lots of developers store secrets as plain text or DevOps team adds them as an environmental variable. This creates security risks.

- Compliance like PCI DSS requires secrets must be rotated and audit on who does what with secrets.
lot of org's they need a service which keeps the secrets. Hashicorp vault is one of them, which many org's are using.but here we have to manage these 
services

* but secrets manager is a managed service so we don't have to worry about going down or responding slowly 
key features : 

****  Built-In integration for rotating MySQL, PostgreSQL and Aurora on RDS.
- Use versisoning so applications do not break when secrets are rotated.
- Fine grained access control to control who has access to secrets with help of IAM and Resource based policies.

demo ; when we open secrets managr console : we can see 3 secret types : 1.credentials for rds database 2.cred's for other db 
 3. other type of secrets(ssh key or any api key)


*********************************************************************************************************************************************************

MACHINE LEARNING SERVICES

***********************************************************************************************************************************************************


AMAZON COMPREHEND : ML to analyze text

There are 100 customer representatives working in a call center.

All the conversation is recorded into text (speech to text converter)( Transcribe will convert speech to text, written seperately)

Management wants to know the overall sentiment of conversation (positive/negative).

- to identify positive/negative we need ml that can go through all the text and can find key words to identify.

* Amazon Comprehend is a natural-language processing (NLP) service that uses machine learning to uncover valuable insights and connections in text.
- if we have a text file comprehend will analyze positive sentiment,personal dat(PII) , language in text.

Amazon Comprehend Medical is a HIPAA-eligible NLP service that uses machine learning to understand and extract health data from medical text, such as
 prescriptions, procedures, or diagnoses

Amazon Translate:   is a neural machine translation service that delivers fast, high-quality, affordable, and customizable language translation.
 Alternative of goggle translator in aws.
 EX : if we write "ela unnavu in telugu". translate will give " HOW are you " in result.
- You can translate messages in real-time based received on services like Twitch. 

Amazon Textract: is a machine learning (ML) service that automatically extracts text, handwriting, and data from scanned documents.
 alternative to the google lens 

Amazon Lex : Amazon Lex is a service for building conversational interfaces into any application using voice and text.
 Ex : for bookings we chat with bots will reply automatically using lex.

Conversational Bots : A conversational bot is a computer program which conducts conversation in natural language via speech or text.

They are meant to understand the intent of the user and respond according to the business rules and logic that you have associated it with.

Nowadays chatbots are built with various new technologies like machine learning and Artificial Intelligence. 

LEX : types : Automatic speech recognition ; Natural processing unit(NIU)

LEX can also be integrated with AWS Lambda to achieve a specific use case.

 EX : user(price of CPU) to lex - lex to lambda give input. now lambda will give output to lex. now lex will give that o/p response to user.


Amazon Transcribe is an automatic speech recognition service that uses machine learning models to convert audio to text 

Amazon Transcribe Call Analytics allows organizations to gain insight into customer-agent interactions. Call Analytics provides you with:

1. Call characteristics, including talk time, non-talk time, speaker loudness, interruptions, and talk speed

2. Speaker sentiment for each caller at various points in a call

3. Call summarization, which detects issues, action items, and outcomes 


Amazon Kendra :   is a highly accurate and intelligent search service that enables your users to search unstructured and structured data using natural 
language processing and advanced search algorithms.
EX : Where does DevOps  Team sit? Amazon kendra : building a 3rd floor.


AWS Rekognition:  is a deep learning based virtual analysis service.
It allows us to easily integrate powerful visual analysis into our application

EX : when we upload a picture to recognition it will detect the objects in the picture with accuraty % type.
 EX : if we upload a celebriuty picture.it will detect the person andgives the name of him.
EX : we can compare faces also.
EX : image that contains a text.it will give that text as o/p 

Amazon Polly  :  is a service that turns text into lifelike speech,

******************************************************************************************************************************************************

CONTENT DELIVERY NETWORKS : 

********************************************************************************************************************************************************

CDN: let's consider a scenario where evrything( website,database,etc) hosted in a single server. it's fine for a smaller margin.but when the 
     traffic and popularity goes high there are lot of chllenges.
* with increased in number of visitoras peformance can goes down. a mid level company that has 100's of servers still faces content caching prolems.
  if website has 1 image and 1000 users are visiting then the same image will need to be send to 1000 users.
	
* attackers love internet ; if we launch a server and kept open for port 22 we can see many attacks on that. a typical website and web application face 
  various types of attacks from Deniel of services(DOS) and web application attacks;  a website that serving the internet traffic  not only users will be
  connecting to it even attackers can connect to the server that serving the traffic.

Based on the above challenge : typical solution : 
 * increase the num of servers or size of servers for better performance.
 * configure ddos(distributed deniel of services) protection,web application firewall at server level
Doing these things at server level is a difficult task and can't scale very well. because increasing servers when we have millions of users latency and 
  performance will be problem.

Better archt: better architeture would be setting a middle layer which has all functionalities related to protecting against attacks , caching of common
         requesting objects for better performance. here i have an user he can connect only to middle layer and middle layer is connected to backend.he 
         can't directly connect to backend instances. so any attacker he wants to attack on the instances using ddos attacks the attacks will go to middle
         layer. multiple soln's will be used at the middle layer. popular one is reverse proxy,lot of webservers like nginx they work well at revrse proxy
         level. cdn's also extensively used at middle layer level in org's.

 [ a reverse proxy is the application that sits in front of back-end applications and forwards client requests to those applications.]

CDN : a cdn acts as a proxy which recieves the requests and then forwards it to the backend systems.  ; here user can only conect with cdn,not with the
      backend servers.
  Various CDN's also comes with features like DDOS ,WAF ,caching and others.

Popular CDn : 1) cloudflare cdn ; it provides unmetered ddos protections
              2) Akamai ; popular cdn toll provides ddos,dos,cache,waf features
              3) Amazon cloudfront : cloudfront integrates seamlesly with aws shield for layer 3/4 ddos mitigation, waf for layer 7 attacks

We discussed 1 image and 1000 user requests. but with cdn, cdn makes the frequently asked image storage in cdn.frequently requested things stored at cdn.
  next time when 100 users are requesting the image , the image is provided by the cdn only.the req doesn't goes to server.thus it reduces latency

EX : there is a static website i used to use. i realize that the static website went down 3 days ago.however users were able to browse that website perfect
     ly well. the reaseon is because of cdn. the entire conent related to website was cached at the cdn level.

Demo : we have a user(laptop). and aws cloudfront (cdn) acting as a proxy or middle layer. we have origin(backend server) it can be s3 or ec2. but in 
this case we have s3 bucket where files are stored. in demo we created a WAF and we associated it with cdn to stop access to admin.txt file in s3.

 in this we requested a shiv.jpg image.we requested 1st time in chrome.when we inspect it we can see there " X-cache : miss from cdn" means it is 
generated from s3 (ORIGIN) if we request the same image multiple times.when we do inspection we can see" X-cache: Hit from CDN" means now it is generated
 from cdn.cdn stores the frequently accessed things. it reduces the overall traffic that goes to server or s3.it also reduces the latency.

 We can have geo restriction at Cloudfront level. we can add any country in block list in cdn level.if any request come from that country cdn will check 
 the list of blocked countries.if it is generated from that country it will block access.

EDGE LOCATION : 
 
- many org uses cdn for content caching and ddos attack. Edge Location is part of Conent caching.

EX : i have a server in mumbai.now 2 users one in us and one in aus. the amount of time that us user can connect to server is more than the time it takes 
     for aus user. There is a concept called hops. the packet that user will send it will not directly reach server. it will travel through something calle
     as hops. Hops are basically routers. when US user tries to connect to server the packet of user first goes to one hop(Router) in one country from that
     country to it will go to another hop and finally reach server. There is a website to route the packets that are going through.

     The amount of time AUS user(200ms) takes less time to reach server than the US user(1100ms).


Edge location : when we create cloudfront ,cloud front will create edge location. we have a mp3 file in mumbai which we want all the users to download.

     Cloudfront will copy all the mp3 file to all edge location. when theuser make a request the doesn't request to the server. their request goes tonearby
     edge location. the amount of time in latency will be decreased a lot. more than 50+ edge location. while creating cdn we can choose options for edge
     locations.like use a particular edge location or use all edge locations. 

DEMO : create a sample website. create cloudfront distribution. connect cloudfront with website endpoint(associate cloudfront with website endpoint).
   means we have a image. and we create a cdn. now cdn will associate with website endpoint(the server where image got stored).
 we can also block any users from a specific country by just adding that country in our list of restrict geographical location.


Q : 

  - Amazon CloudFront is a content delivery network (CDN) that improves website performance by caching content at edge locations around the world. It can 
    serve both dynamic and static content. This is the best solution for improving the performance of the website.

  - AWSCloudFront caches content closer to users at Edge locations around the world. This is the lowest latency option for uploading content.API Gateway
    and AWS Lambda are present in all options. DynamoDB can be used for storing session state data. This is a 100% serverless application.

  - DynamoDB can be used for storing session state data.Amazon RDS is not a serverless service so this option can be ruled out. 

  - An Amazon EC2 instance running a video on demand web application has been experiencing high CPU utilization. A Solutions Architect needs to take steps
    to reduce the impact on the EC2 instance and improve performance for consumers :Create a CloudFront distribution and configure a custom origin pointing
    at the EC2 instance .

  - This is a good use case for CloudFront which is a content delivery netwr(CDN) that caches content to improve performance for users who are consuming 
    the content. This will take the load off of the EC2 instances as CloudFront has a cached copy of the video files.

  - An origin is the origin of the files that the CDN will distribute. Origins can be either an S3 bucket, an EC2 instance, and Elastic Load Balancer, or 
    Route 53 – can also be external (non-AWS)

  - This is a good use case for CloudFront. CloudFront is a content delivery network (CDN) that caches content closer to users. You can cache the static 
    content on CloudFront using the EC2 instances as origins for the content. This will improve performance (as content is closer to the users) and reduce
    the need for the ASG to scale (as you don’t need the processing power of the EC2 instances to serve the static content)

* - to remove CloudFront caches before expiration =Invalidate the files.

  - If you need to remove a file from CloudFront caches before it expires, you can do one of the following:
    Invalidate the file from caches. The next time a viewer requests the file, CloudFront returns to the origin to fetch the latest version of the file.
    Use file versioning to serve a different version of the file that has a different name

* - With Amazon CloudFront you can set the price class to determine where in the world the content will be cached. One of the price classes is “U.S,Canada 
    and Europe” and this is where the company’s users are located. Choosing this price class will result in lower costs and better performance for the 
    company’s users


*  A company runs a dynamic website that is hosted on an on-premises server in the United States. The company is expanding to Europe and is investigating
   how they can optimize the performance of the website for European users. The website’s backed must remain in the United States. The company requires a
   solution that can be implemented within a few days : Use Amazon CloudFront with a custom origin pointing to the on-premises servers

 - A custom origin can point to an on-premises server and CloudFront is able to cache content for dynamic websites. CloudFront can provide performance 
   optimizations for custom origins even if they are running on on-premises servers.These include persistent TCP connections to the origin,SSL enhancements
   such as Session tickets and OCSP stapling.

   When a user requests your content, CloudFront typically serves the requested content regardless of where the user is located. If you need to prevent 
    users in specific countries from accessing your content, you can use the CloudFront geo restriction feature to do one of the following:

 - Allow your users to access your content only if they're in one of the countries on a whitelist of approved countries.
   Prevent your users from accessing your content if they're in one of the countries on a blacklist of banned countries

   An Amazon S3 bucket in the us-east-1 Region hosts the static website content of a company. The content is made available through an Amazon CloudFront
   origin pointing to that bucket. A second copy of the bucket is created in the ap-southeast-1 Region using cross-region replication. The chief solutions
   architect wants a solution that provides greater availability for the website : Add an origin for ap-southeast-1 to CloudFron

 - also Using us-east-1 bucket as the primary bucket and ap-southeast-1 bucket as the secondary bucket, create a CloudFront origin group

 - You can set up CloudFront with origin failover for scenarios that require high availability. To get started, you create an origin group with two 
   origins: a primary and a secondary. If the primary origin is unavailable or returns specific HTTP response status codes that indicate a failure, 
   CloudFront automatically switches to the secondary origin.


**********************************************************************************************************************************************************



ORIGIN ACCESS IDENTITY(OAI) (CDN security) : whenever we have contents that are based on s3 bucket having OAI is very imp for production

Use case : we have an s3 bucket associated with cloudfront distr. now users can connect to cdn to connect to s3 bucket. now after a week if administrator
           want to add geo restriction filter in cdn. in that they decided to block country A and country B. these countries access will be blocked at 
           cloudfront level. now if there is a user who always used to access the bucket from country a will be blocked. now when he blocked in cdn level

           he manages to figure the s3 bucket url from which the contents are loaded from cloudfront location. now he directly make a request to s3
           bypassing the cloudfront distribution. now successfully he can fetch the conetnts of s3 bucket. 

  Whatever geo restriction filters or WAF rules are applied at cdn level. if a user manages to diretly access s3 bucket by bypassing cdn and all the rules.
   he can access the contents in s3 bucket. To overcome this we make use of Cloudfront OAI.
 
Cloudfrontdistribution(CFn) OAi ensures that only users coming through cfn distribution are able to access the contents of s3 buckets. here any user who 
    bypassing the cdn and trying to access s3 bucket directly will not be possible. any user who is trying to fetch through cdn can only get access to s3.
- here s3 no need to be public.it can be private.


Q :

  - Create a VPC Security Group for the ELB and use AWS Lambda to automatically update the CloudFront internal service IP addresses when they change
    As these are updated from time to time, you can use AWS Lambda to automatically update the addresses. This is done using a trigger that is triggered 
    when AWS issues an SNS topic update when the addresses are changed.
  - You can use an OAI to restrict access to content in Amazon S3 but not on EC2 or ELB.

  - To restrict access to content that you serve from Amazon S3 buckets, follow these steps:
  – Create a special CloudFront user called an origin access identity (OAI) and associate it with your distribution.
  – Configure your S3 bucket permissions so that CloudFront can use the OAI to access the files in your bucket and serve them to your users.Make sure that
  users can’t use a direct URL to the S3 bucket to access a file there

S3 TRANSFER ACCELERATION(STA) (least latencies) : STA allows users to accelerate data uploads from all over the world to central S3 bucket.

use case : a org have global users.if any user from us to upload file to s3 bucket in Mumbai.then it goes via the internet.multiple routings that happen
          via the internet.when we have terrabytes of data that needs to be Uploaded it can be little slow here.

  But with the help pf STA instead going through multiple hops(routes) through internet,the user can send data towards the cloudfront edge location.
  cfn has a very optimized network between the edge location and s3 bucket. once the user uploads the data to edge location, the edge can intewrn takes
  care of data towards the s3 bucket. this is optimized network with various optimized protocols.

- in s3 when we enable STA we get an accelerated endpoint url or link.the user has to send data through the STA Path link which through the endpoint.
  if user has s3 endpoint if he will send data towards the s3 endpoint then the data will not go through the sta endpoint.

STA not only helps in data upload it also helps when we go and download the data from s3.
to access a file in s3 user can use s3 endpoit or STA endpoint (cloudfront edge link)
 
when we access any file in s3 using STAendpoint first time it will in inspection like" x-cache = Miss from cloudfront. server =s3" .when we do access 1st
     time it will come from s3. but after some time it will be accessed from edge location only.

Q : 

- Amazon S3 Transfer Acceleration is used for speeding up uploads of data to Amazon S3 by using the CloudFront network. It is  used for downloading data






LAMBDA@EDGE (RUNNING SERVERLESS AT THE EDGE).

- lambda@edge lets you run lambda fiunctions to customise content that cloudfront delivers.

Path : 1) we have a user  - 2) cloudfront cache (CDN) - 3)origin(s3 or any website) .  ; now lambdaedge allows us to run lambda function at 4 major
  
  points : 1) first major point is request hits before cloudfront cache from user  (1st LF)
           2) after the request misses the cloudfront cache and before it hits the origin (2nd lambda function)
           3) as soon as we get data back from the origin ( 3rd LF)
           4) after the data traverses the cloudfront cache before it reaches user.
  the above 4 pointers are defined by a name 1) viewer request 2) Origin req 3) origin response 4) viewer resp

we are using lambda functions to change cloud front requests responses at the above points 

1) 1st lambda function(viewer req)  will be executed on every request before cloudfront cache is checked.
   various things that we can do at this stage : 
  * Modify URLs ,cookies query strings.

  * perform Authorization and authentication checks :when  a user request cdn cache first the viewer request event (LF) will run.and it will check whether
    the user has appropriate paswrd to open the website.if request has correct rights it will go to okay and will access the origin.if the request doesn't
    have apprpriate info it will return " HTTP 403 Forbidden"( Sent by Viewer request event)

   - also if a user requesting a specific file from s3 but it is stored in cloud front cache also that's why we kept lambda function before reaching cloud
     front cache. because here he can access that file without correct rights if we don't use that viewer req lambda.

2) 2nd LF(Origin request) : executed on a cache miss, before a request is forwarded to the origin
  various stages like : Dynamically select origin based on request headers. ; assume the request first went to viewer req then it went to cloudfront cache
  there is a cache miss at cloudfront( remember if there is a "CACHE HIT" then cfn will deliver the data directly to the user; origin req might not exe)

  whenever a cache miss occurs the data traverses through the origin request. in origin req we can control a lot of parameters here : Dynamically select
  origin based on request headers. ; means if we have mutiple origins(s3, ec2) .from this lambda function we can tell whether the req should go s3 or ec2 
  based on the request headers. we can also send data from origin requests directly back to cfn cache.

3) 3rd LF(Origin response) : executed on a cache miss, after a response is recieved from the origin. various things that we can do at this stage
    stage is : Modify the response headers  ;   intercept and replace various 4XX and 5XX errors from the origin.

 - origin resp come after the origin. the data which origin sends back it goes to origin response. let's say i have origin it can be ec2 or s3 ; and some
   how my application is down and it is not working at all and it gave 5XX error back. Now at the origin resp we can have a logic that whenever we get a
   5XX resopnse from origin either it can replace that response with a page saying that a website in maintainence or etc. or it can send a 301 redirect to 
   a backend to the backup origin for the req to happen.
  
4) viewer resp : executed on all the responses recieved either from the origin or the cache. 
   Stage : modiefies the response headers before caching the response.   ; from the origin resp if we have certain headers it might get cache at the cfn
           cache. but if we put at viewer resp level we will not have cahing at all. it will go directly 

Q : 

 - An application generates unique files that are returned to customers after they submit requests to the application. The application uses an Amazon 
   CloudFront distribution for sending the files to customers. The company wishes to reduce data transfer costs without modifying the application.
   How can this be accomplished : Use lambda@Edge to compress the files as they are sent to users.

 - Lambda@Edge is a feature of Amazon CloudFront that lets you run code closer to users of your application,which improves performance and reduces latency.
   Lambda@Edge runs code in response to events generated by the Amazon CloudFront.

 - You simply upload your code to AWS Lambda, and it takes care of everything required to run and scale your code with high availability at an AWS location
   closest to your end user.In this case Lambda@Edge can compress the files before they are sent to users which will reduce data egress costs.


**********************************************************************************************************************************************************


FIELD LEVEL ENCRYPTION (CRYPTOGRAPHY) : 

Challenge : sensitive data when travelling via multi tier architectures leads to security challenges.

- we have a cfn distr.and there are multiple servers involved in this org. and server 3 has a responsiblity to store the data in database. let's assume tha
  a user has send a sensitive data. now sensoitive data needs to be stored in the database. however when we have https website; what we will do is we will
  terminate https at higher level(CFN level or ELB level). the data might be encrypted as part of[[TLS(Transport Layer Security encrypts data sent over 

  theInternet to ensure that eavesdroppers and hackers are unable to see what you transmit which is particularly useful for private and sensitive info 
  such as passwords, credit card numbers, and personal correspondence. encryption)]]. But once it comes in within our network it will be in a decrypted

  format. we can see one object travelling from server 1 to server 2 may be it is in a internal network it is in a decrypted format .but at server 3 ; 
  server 3 might have a logic to encrypt the object with a kms and store it in the database. however there is a risk here.

- let's say server 1 is compromised , then the object(personaldat(PII)) can also be compromised. so any one of the servers is compromised the PII will be
  compromised. ; THIS TYPE OF ARCHITECTURE IS NOT GOOD WHEN WE ARE DEALING WITH A SENSETIVE DATA.

Possible SOLn: Encrypting data end to end so that no intermediate service has access to it. ; Like when data traversed from CFN or ELB, when it is traverse
               from origin server to server2 or from server 2 to DB. it will be always in encrypted format.

 - Only a specific service which has genuine business need should be able to fetch and decrypt data.
 When genuine service request DB for data it will be able to fetch the decrypted object back. Now overall attacks reduced to min. here even an attacker can
 hack any of the server  he can't able to see data in plain.

 This can only be achieved with cloudfront field level encryption 

cloudfront field level encryption encrypts the sensitive PII data before the request is forwarded to the origin. croudfront before forwarding the data to
 the origin server it will encrypt it.

PATH : users(sending phone number) - Aws CFn(public  key contains) - encrypted phone number - AWS API gateway - AWS Lambda - Dynomodb table(encrypted phnno
       - AWS Lambda function- AWS ec2 systems manager parameter store (private key encrypted) - Decrypted phone number- administrator.

  AWSCDN : it will encrypt the phone number with the public key, which we store in the cfn distribution. now encrypted phone number goes to the origin it 
    can be EC2 or API gateway. origin will go and store it in the dynamodb table( or any other Db table it can be rds etc). now we have a encrypted phnno 
  in dynamodb table.

 Now we have a admin( in the above case it is a genuine server we call it is a admin here). now admin wants to look data in dynamodb in decrypted format.
 admin will make a call via lmbda to the dynamodb table. dynamodb will send data in encrypted format.the private key associate with the public key which 
 was stored in a cloudfront.this private key is stored in the parameter store.This private key is used to decrypt the specific phone number.decrypt data
 will go back to admin.

Steps:
1) application sends post req with PII data; we can specify which is PII data among total data.
2) field level encryption intercepts post req, encrypts the data with public key and forwards to origin.
3) origin takes the data and processes it normally stores to dynamodb table  ; here even api gaeway or lambda function is comprtomised there will no prblm.
4) lambda function stores the data in dynamodb.we can also have ec2 instead of lambda.

Rev :
5) admin makes use of anothher lambda functiuon to retrieve data from dynamodb table
6) admin uses key material(pvt key)stored in parameter store to decrypt sensitive data
7) decrypted data is returned to the data center.


Q : 

**********************************************************************************************************************************************************

MONITORING : 

**********************************************************************************************************************************************************


Cloudwatch : CloudWatch is a monitoring service for your AWS Cloud Resources & Applications.

- CloudWatch is a monitoring service which can monitor our server and applications. cloudwatch can do much more than traditional monitoring based on metrix
- it can even collect and monitor our log files which can be our system or appl log files.

- We can use CloudWatch to collect and monitor log files, set alarms and also automatically react to changes in the AWS Resources.
- in left side i have various datapoints(in un-understandable way)related to cpu utilization of our ec2 instances .all these basically comes
   from cloudwatch.  once we record the cpu utilization we can even set the alarms based on that. let's say when a cpu utilization is greater than 70%.then
   we shoukd get a alarm on our mail. all these functionalities can be achieved with the help of cloudwatch.

Demo : i have a ec2 which is running. if i go to monitoring in ec2 , i can see lot of cludwatch metrics available. these cloudwatch metrics are " cpu util,
       , networking, status metrics" all these are coming from cloudwatch. if we click on any of the metric we can see the time of % of cpu utilization
       we can also see the metrics fro evry 5 min or 24 hrs or monthhs etc. 
- cloudwatch can do more than just monitoring. various aspects like alarm, it can also monitor log files. i can store system or appl logs in cloudwatch.
- if we click on any of the log files we can see all the logs relate to it stored within the cloudwatch. it is something similiar to KIBANA( ELK STACK) or 
  splunk(but not really like Splunk). it just provide some basic features of splunk(just pushing our logs to cloudwatch logs).
- cloudwatch provides metrics for other services as well(EBS,ELB,S3,Dynamodb,RDS etc).
 

Q:

A Solutions Architect needs to monitor application logs and receive a notification whenever a specific number of occurrences of certain HTTP status code
   errors occur : CloudWatch Logs	

- You can use CloudWatch Logs to monitor applications and systems using log data. For example, CloudWatch Logs can track the number of errors that occur
  in your application logs and send you a notification whenever the rate of errors exceeds a threshold you specify. This is the best tool for this 
  requirement.

- CloudWatch Metrics are the fundamental concept in CloudWatch. A metric represents a time-ordered set of data points that are published to CloudWatch.
  You cannot use a metric alone, it is used when setting up monitoring for any service in CloudWatch.

- Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Though you can 
  generate custom application-level events and publish them to CloudWatch Events this is not the best tool for monitoring application logs.

- Install an Amazon CloudWatch agent on the instances. Run an appropriate script on a set schedule. Monitor SwapUtilization metrics in CloudWatch.

- You can use the CloudWatch agent to collect both system metrics and log files from Amazon EC2 instances and on-premises servers. The agent supports both
  Windows Server and Linux, and enables you to select the metrics to be collected, including sub-resource metrics such as per-CPU core

- There is now a unified agent and previously there were monitoring scripts. Both of these tools can capture SwapUtilization metrics and send them to
  CloudWatch. This is the best way to get memory utilization metrics from Amazon EC2 instnaces.

- The CloudWatch Alarm Evaluation Period is the number of the most recent data points to evaluate when determining alarm state. This would help as you can 
  increase the number of datapoints required to trigger an alarm.

- There is no standard metric in CloudWatch for collecting EC2 memory usage. However, you can use the CloudWatch agent to collect both system metrics and
  log files from Amazon EC2 instances and on-premises servers. The metrics can be pushed to a CloudWatch custom metric.

Real-time notifications based using Amazon CloudWatch = CloudWatch Events

- Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple 
  rules that you can quickly set up, you can match events and route them to one or more target functions or streams. CloudWatch Events becomes aware of 
  operational changes as they occur. CloudWatch Events responds to these operational changes and takes corrective action as necessary, by sending messages 
  to respond to the environment, activating functions, making changes, and capturing state information.

- GuardDuty can send notifications based on Amazon CloudWatch Events when any changes in the findings takes place. These changes include newly generated
  findings or subsequent occurrences of existing findings.

- Every GuardDuty finding is assigned a finding ID. GuardDuty creates a CloudWatch event for every finding with a unique finding ID. All subsequent 
  occurrences of an existing finding are always assigned a finding ID that is identical to the ID of the original finding.

- In order to receive notifications about GuardDuty findings based on CloudWatch Events, you must create a CloudWatch Events rule and a target for 
  GuardDuty. This rule enables CloudWatch to send events for all findings that GuardDuty generates to the target that is specified in the rule

- to monitor VPN connection if it is up or down use Use CloudWatch TunnelState Metric.

**********************************************************************************************************************************************************


CLOUDTRAIL(LET'S MONITOR EVERYTHING) : Installing video survivalance systems(CCTV CAMERA)allows us to monitor activities round the clock and provides lots
 of benefits some of the benefits : Deterring investiigation  ;  Helps in investigation  ; regular monitoring activities   ; insurance benefits 

when we discuss importance of recodring generally we leave upto a physical level. and various services and resources that we make use of online therecordin
concept is not extensively used. for ex : let's say i have aws account with 100's of users it is very imp to record all of the activities of all of the 
 users what user is doing ; whether user is secretly trying to create a new resource without informing you or something like that. even at a aws level it 
 is very important for org to record the activities that are happening within the infrastructure as well as that of the servers. 

a mandotaory thing when we go through audit. 
auditor quest : when we have a list of users. show me what anne did on 3rd of nov b/w 10 am and 12pm. we have to ensure that we give the logs to auditors 
 related to what a specific user has done. we also have to ensure that we have all of the logs. depending upon the compliance auditor might ask to store
 logs of all the users. for 4 or 10 or 20 years. 
for physical location recording the things we have cctv camera.but when we discuss about online resource like aws or a server the tools of recordng change.
 
For AWS env one of the primary service for recoring is CLOUDTRAIL.which will go and record the events for us. let's say i have a linux server running on 
 top of AWS ,now cloud trail will not really monitor what's going inside that linux instance; cloudtrail is specific for aws as a whole.but the resources
 that we are running under aws like linux server for that we might have a defferent tool. for linux inst we make use of a tool "auditd"  to record the 
 linux events. if we are windows server the tools may change.

in amazon console :
cloudtrail(track user activity and api usage). this service is enabled by default. if we look in event history it will basically show all the events that
are happening in our aws environment. this is default config.we don't have to enable anything explicitly. if we take example in any of events : 

"schedule key deletion " event ex : a user called as root he has initiated a deletion of specific resource 	of type kms.here resource type is KMS.if i clic
on that event it will show details associate with it.event time at what time event has occured.
- at what time james has logged in ; when anne modified SG. along with the event time it shows the user name who performed the activity. it also shows the
source I.P adress from which the activity has perormed.

- It also show details in greate way using JSON format.
in event history we have options to filter using "resource type" or "time" "read only" etc.

* aws cloudtrail is now enabled by default for all users and will provide visibility of account activity without need for us to config(create) a trail in 
the service to get sorted. but default cloudtrail will come with few disadv. 

- in default cloudtrail events are not stored forever. they will be deleted after N days. Default Event history shows us last 90 days of management events.
  Not all of the event types are logged.

  Little customization.(what exactly we want to record,and where we want to store  or where the logs should be sent to). to overcome all this disadv we can
  make use of new cloudtrail trail.which allows us to have lot of customization related to the recording patterns, we can also say that all of the cloudtra
  logs should go to s3 bucket(where we can store logs for 5 years, 10 years), we can aslo send logs to cloudwatch logs.we can enable Encryption as well.

 CLOUDTRAIL TRAIL creation: when we create trail we can give option to where to store logs. we can also have using KMS option for encryption. we also have 
 option to cloudwatch logs( to send cloudtrail logs to cloudwatch). also option for event trypes : management events( capture management operations
 performed on our aws resources, we can also select read or write events    ex : creating new ec2  ;  creating new iam user ; user sign in's.)  ; 

data events( explenation just below)  ; insight events( identify unusual activity, errors or user behaviour in our account)
- now in all of the regions same trail will apply. 

DATA EVENTS :where we want to see the activities that are performed inside s3 or.. like who is quering for a specific object in s3 buck.data events 
   - they provide the info about the resource operations performed on or in a resource and are often high vol activities.
 ex : events capture when data events is enabled : GET object ; put object ; delete object
 charged seperately.

 insight events( identify unusual activity, errors or user behaviour in our account)
  - insight event helps customers identify unusual operating activity in their aws accounts such as spikes in aws resource provisiong,burst ofAWSIAM actios

 - it is designed to automatically analyze managemnt events to establish a baseline for normal behaviour, and then raise issues by generating insights
   events when it detects unusual patterns.

 exp : if we take example of last 30 days certain resource provisioning of 3 resources per day.one day we see 300 resource per day which is not expected
        so now insights events will show unusual spike in specific event

  we can also specify whereas events types whether we want event based on API CALLRATE(a measurement of write only api calls that occur per minute againts 
   a baseline API call volyume )  ; API ERROR RATE ( when we have lot of errors that are happening at api level ; the error is shown if api call is unsuces


Q :

- CloudTrail is used for monitoring API activity on your account, not for monitoring application logs

- Data in the application changes frequently. All levels of stored data must be audited under a new regulation which the company adheres to: cloudtrail 
 to log management events.

- AWS CloudTrail monitors and records account activity across your AWS infrastructure, giving you control over storage, analysis, and remediation actions.

- CloudTrail is used for auditing not performance monitoring.

- Data events: These events provide insight into the resource operations performed on or within a resource. These are also known as data plane operations
  Management events: Management events provide insight into management operations that are performed on resources in your AWS account. These are also known
  as control plane operations. Management events can also include non-API events that occur in your account

**********************************************************************************************************************************************************


AWS Trusted Advisor(TA)(Recomendations are always Good) : AWS Trusted Advisor analyzes your AWS environment and provides best practice recommendations in 
    five major categories : 1) Cost optimization  ; 2) performance  ; 3) security  ; 4) fault tolerance   5) service limits 
 
-it will analyze the entire AWS accounts based on the rules that are part of each one of the above categories. Depending upon the overall rule matching we
 will get the appropriate recommedation.

- in trusted adviser dashboard we can see certain actions that TA recommending.some of them " MFA authentication on root acnt ; if i open that it basically
   describe about mfa". " SG's specific ports unrestricted  ; ex : in one ec2 port 22 is opened from 0.0.0.0/0. it shouln't allow" " s3 bucket permissions;
   which has permissions to allow from all which shouldn't have given ". SG comes uner the category of security. 

 - not all of the checks is available for free account.we have to upgrade to it.

- in TA console : on left side we have all 5 categories : if we click on security : we can see only few of the rules which can be accessable. and many are
  not accessable (not enabled in free account) ; same for cost optimization and remaining also.
 TA Plans ; DEVELOPER ; BUSINESS  ; ENTERPRISE ; 

Developer plan TA : access to 7 core trusted advisor checks ; 
-if we want all of the rules or TA checks we need to have Business or enterprise plan.

1) Cost optimization : recommendation that allows us to save some money

2) performance : recommendation that can improve speed and responsivness of application. ex : high utilza ec2's ; ebs provisioned iops(ssd) , vol config. ;
                 over utilized amazon ebs magnetic volumes. 
3) security: above.

4) fault tolerance : recommendation that helps in increase the resiliency of our aws soln.ex : ec2 AZ balance : checks the distribution of amazon ec2 insta
                     across AZ in a region.

5) service limits : checks the usage of account  and whether there are limits that are been approaches 
  Ex :VPC IG's ; if we try to create huge number of inernet gateways; if no of ig's is exxceeding the limit amount we will not be able to do that.there is
      a option to increase limit.


AWS WELL ARCHITECT FRAMEWORK( DESIGN MATTERS) : 
EX : if we need to buy a mobile phone .if i need in my budget with best features ; i have to check all mobiles and checklist the features of evry phone for
     best features; 
whenever we are designing architectures in aws we need to ensure that we follow the best practices. in order to ensure that aws has constructed a well
 architected framework. this framework is designed on 5 pillars :
● Operational Excellence
● Security
● Reliability
● Performance Efficiency
● Cost Optimization

if we makesure that whatevr architecture we design on aws follows all 5 pillars(with each of pointers) then we will make sure that our architeture is not 
 only scalable it is secure,cost optiumize and it also will be good interms of performance . it's just like a framework we just have to look into what does
 the frameweork says and apply the same within your architecture  design.

- if we are an enterprise customer which has 5000 servers we are big aws enterprise customers ; here aws support team they come to our org and theylook int
  entire architecture and they will give us pdf document ; they will review it according to the 5 pillars ; in pdf they mention which areas we are missing
  in well architected framework.

Operational Excellenc : Focuses on running and monitoring systems so provide business value 
Design principles:
 Perform operations as code(earlier all the servers and resources were launched manually. accidentally if we dlete any server this is dangerous.what if DB 
         server got deleted; that's why it is very imp to perform any operations as code.instead of doing anything manually we can make use of IAC tools 
         like Cloudformation, terraform. in this we have a power of pull request and approval process. similiar to what we typically have in our applicatio
         code managemnt. here if i want to delete or edit any server i will write code for it; i will send that code for pull request to my manager once he
         approves it and i will go and make changes on server.
 Annotated Documentation
 Make frequent, small, reversible changes(similiar to agile methodologies) 
 Define Operations procedures frequently.
 Anticipate failure.
 Learn from operational failure ( it's like a post martum9(where we learn after failure)or rca ; once the application got failed and we have recovered ; 
         then we will learn and we willdo rca; and we define what are the things that ensure this doesn't happen again. best practices is also to do
         pre mortum( where before the failure occurs we test or design various failure scenarios and learn before hand) . let's say i have appl in prod
         uction server we try and see what are the various failure scenarios that might happen  and we make sure that that scenario will not happen through
         discussion with teams 
 
Security : Focuses on protecting information & systems.  ; Even big org that security is least priority unless and until they get compromised.once they get
         compromised suddenly security becomes very high priority 

Design principles

Implement a strong identity foundation. : in a startup they found that people who already left company 6 months back they are still accessing systems. like
         still using aws account for various activities. that's why we need a strong identity foundation.	
Enable traceability
Apply security at all layers. : not only we should have security on network acl. we can even have HITS and IPS at host level. even if someone accidentally
         launches in public subnet and puts the SG 0.0.0.0/0. we still have HITS and IPS within our system which can still protet against various attacks.
Automate security best practices.
Protect data in transit and at rest.  : it happens with encryption. TLS may be in transit and ES for encryption at rest 
Keep people away from data.  : avoid giving random access to developers is not required. form a central log management so the logs from prod goes to centra
         system.
Prepare for security events.


Reliability : Focuses on the ability to prevent, and quickly recover from failures to meet business and customer demand.
   EX :one org used to take critical database production server backup on a weekly basis. suddenly the prod went down. here the datacenter went down dueto
       which the production server went down. now backups were stored in a centrlaiged s3. the backups are still present. the mngmnt decided to launch that
       server in another datacenter and when the DB team tried to restore the backup it never worked . realiged that the backup procedure itself was incorr
       all the backup present was corrupted and they couldn't restore. then they realiged TESTING RECOVERY PROCEDURE is very important.
design principles : 
Test Recovery Procedure.
Automatically recover from failure(it can be done with the help of ASG)
Scale horizontally to increase aggregate system availability
Stop guessing capacity ( it can be done with the help of LAMBDA) 
Manage change in automation

Performance Efficiency : Focuses on using IT and computing resources efficiently. 
  EX : on guy in one org from application team. for small testing or dev env's also he used to launch m3.xlarge file. if he look cpu utilization it was 
       hardly  1 or 2 % . so it is really not at all 	PERFORMANCE OPTIMIZED. if we dont ensure performance efficiency we will lose cost.
  * LAmbda , API gateway , ELB are use cases of performance efficiency.
Design Principles : 
Democratize advanced technologies
Go global in minutes
Use serverless architectures( serverless not only reduce cost it also we don't have to worry much handling capacity because serverless can really scale 
                              based on the request we might have)
Experiment more often
Mechanical sympathy


Cost Optimization : Focuses on avoiding un-needed costs. ( make sure that whatever resource that are running in AWS  they are utilized for the fulliest.we
          don't want a server which has only 1 or 2% cpu utiliz. Performance and efficiency is the imp factors for cost optimization) 
Design principles:

Adopt a consumption model
Measure overall efficiency
Stop spending money on data center operations(datcenter is really expensive. if we have DC try to move it to AWS).
Analyze and attribute expenditure
Anticipate failure.
Use managed services to reduce the cost of ownership ( in org they used to have MONGodb. they have mongodb cluster of five servers. those 5 servers are big
     too much cost for it. Later they move completely to dynamodb which is maaged service and we can decrease cost) here if we don't use the managed servic
     either we have to use managed cluster and managed high availability and otehrs) 


Personal Health Dashboard :  it gives the notifications as well as overview if there are any errors or any alamalies which are going on in aws services
************************* 

* with health of dashboard we will get the info related to various dashboard services we also have details whether the services are opersting normallyn not
- normally aws provides service health dashboard with so much info which we can't understad. that's aws suggests to create a PERSONAL HEALTH DASHBOARD.

* AWS Personal Health Dashboard displays issues that are impacting your resources or potentially impacting services that you use for your AWS.
- it will give info that open issues reklated to our env within aws 
- in consol we can see bill icon right side of the upper part.if we click on that it will show alerts"open issues ; scheduled changes ; other notifications
* if we are running a server suddenly if we get networking error that we are facing ; we are not sure that this is due to the change last time or it is due
  to some aws components. to check first we open serv dashboard whether our open issues has aws side that are occuring due to share com[ponent might go dow

- dashboard contain info from last 7 dsays only.
- event log contains data from last 90 days. if we clickj on any issue(event) it will give all info (datre, region, affected resources)
ex : i have 3 teams : sre,sysops,security team.whenever there are security  services has any issue then i want security team alerted for, for compute rela
     i want sysops team to have alert for. all of these can be done thorugh cloudwatch.we can set ant ype of rules to get notifications to mail from perso
     health dashboard.

DATA TRANSFER CHARGES : (How you will be billed) 
*********************
-AWS data transfer costs are the costs associated with transferring data either within AWSbetween various AWS services like EC2 and S3 or
 AWS and the public internet.

Charges categories :
  Internet
  Region to Region
  Inter Availability Zone
  VPC Peering


Internet :
If the data is going outside of the AWS towards the public internet, it will be charged at 0.09$/GB up to the first 10 TB.
* Internet = From AWS Public IP to Non-AWS Public IP
- lot of aws services are global. if we are sending data to those aws services which has global ip that is not considered as public internet.

-The above definition excludes traffic between two AWS Regions or traffic between public IP ofthe same AWS region.

-There is no Data Transfer charge between Amazon EC2 and other Amazon Web Services within the same region (i.e. between EC2 US West and S3 in US West)

Region Level : 

- Within an Availability Zone: The data transfer costs in the same region and within the same availability zone is zero, with one condition. You must be
  using a private IP address EX : i have 2 ec2 instances. i am sharing lot of data b/w that 2 ec2 instances. if i am doing it with private ip addresses i 
  will not get charged. if i am doing it using public ip addresses i will get charged.

Across Availability Zone in the same region: The data transferred between AWS services which are located in the same region but in different AZ  is 
                                             considered as regional data transfer and is charged at $ 0.01/GB (outgoing datatransfer).
 
Peering Connect - 0.0.1$ per GB

- the data transfer between regions is costs ate 0.02$/GB. for diff AZ in same region it is 0.01$/GB.


AMAZON OPENSEARCH: it allows us to ingest the data,search the data,and perform visualization on top of the data.
*****************

EX : i have datasouces like (log files, metrics,docs and list) we feed in all of this to amazon opensearch and once the data is ingested we can go and perf
     searching,we can create dashboards,and other things.
* amazon open search is forked version of elastic search.
- some legally issues happend with elasticsearch and amazon. if we want legal support then we go for elastic search. if we normal want to create a dashboar
  we can go for open search. it allows use to create any kind of dashboard(graph,chart,stack).



**********************************************************************************************************************************************************

ANALYTICS :      ppt req

*********************************************************************************************************************************************************


AMAZON KINESIS (Streaming data) : it is aws appropriate service for streaming data
**************
Streaming Data : Streaming data is contineous flow of data generated by various sources EX : we have sensors across city which contineiusly measure somethi
          and send data or stream data  to central source.the data is referred to as streaming data. 2.EX : if we observe stock market contineous flow of 
          data that is going
Challenges with working streaming data : when we dealing with streaming data we require an adeuite system that can handle such big load.ideally streaming 
     data requiures 2 primary layers 1.Storage Lyer  ;  2. Processing Layer.
- in storage layer few features that are really recommended : record orderning, Strong consistency , Replayebale reads.
- for processing layer : Consuming data from storage layer, and running computation on that data and many other tasks.

 various tools or paid tools that can work with streaming data.challenges : we have to take care of lot of things including hosting of applications if
 applicatuions running slow we have to deal with it, perfromance,high availability,scalability etc. to overcome this we can make use of Kinesis.

-Amazon Kinesis: it makes easy to collect,process and analyze real time streaming data.so we can get timely insights and react quickly to new information
 kinesis provides everything from security, perfo,HA takes care.
3 ENities : similiar to SQS three entities involved. Producer(any entity that producing information ex : sensor.they will send data to any kind of stream
     store),Stream store(Amazon Kinsesis. from kinesis we can connect multiple consumers that can run mulltiple analytics on top of the data and produce
     some relevent results,) consumer.

1.AMAZON KINESIS DATASTREAMS : This place where producers can send data into. we can see graphs also. Producers need a way to send data to kinesis.for that
  we have multiple ways in :Amazon Kinesis Agent. we also have consumer to perform some analytics on data. Consumers : aws Data analytics which allows us 
  to process the data in real time( perform analytics on stored data).

AWS Kinesis is a set of services that makes it easy to work with a set of streaming data on AWS.

1.AMAZON KINESIS DATASTREAMS: it allows us to capture, store , process data streams. producers(sensors) can send data into it. from this consumers(EC2,kine
    Sis data analytics, AWS lambda) will fetch the data from datasreams and they can go and run some computational tasks on that data.

" Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time datastreaming service. KDS can continuously capture gigabytes of data 
  per second from thousandsof sources such as website clickstreams, database event streams, financial transactions, socialmedia feeds, IT logs, and 
  location-tracking events. The data collected is available inmilliseconds to enable real-time analytics use cases such as real-time dashboards,
   real-timeanomaly detection, dynamic pricing, and more."

2.AMAZON KINESIS FIREHOUSE : it delivers data from point A to point B. we have various set of inputs(datastreams or etc) connected to FIre house. from fire
   house we can go and send the data various endpoints like S3,Amazon redshift, Amazon elastic service,splunk(kibana alt) . Ex; if i want to move data from
   data streams to splunk, then firehouse is better option

- Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes,data stores, and analytics tools.

3.AMAZON KINESIS DATA ANALYTICS : it has ability to analyze data streams in real life. USing sql, java we can perfor analytics. We can get data from data
  streams or fireshouse to analytics

4. Amazo Kinesis Video Stream: Amazon Kinesis Video Streams make it easy to securely stream video from connected devices to AWS.
  EX : i have camera in one place.now i want send video feed of camera to aws.now we can connect camera to kinesis video streams here camera will send live
       feed to kinesis videos treams. now videos stream can connect to other AI services which can perform some analytics.

Q :

- Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. Firehose Destinations include: Amazon S3, Amazon
 Redshift, Amazon Elasticsearch Service, and Splunk ; RDS is a transactional database and is not a supported Kinesis Firehose destination

- Amazon Kinesis Data Streams collect and process data in real time. A Kinesis data stream is a set of shards. Each shard has a sequence of data records.
  Each data record has a sequence number that is assigned by Kinesis Data Streams. A shard is a uniquely identified sequence of data records in a stream.

- A Kinesis consumer application is reading at a slower rate than expected. It has been identified that multiple consumer applications have total reads
  exceeding the per-shard limits : Increase the number of shards in the Kinesis data stream.

- One shard provides a capacity of 1MB/sec data input and 2MB/sec data output. One shard can support up to 1000 PUT records per second. The total capacity
  of the stream is the sum of the capacities of its shards.

- In a case where multiple consumer applications have total reads exceeding the per-shard limits, you need to increase the number of shards in the Kinesis
  data stream


- A partition key is used to group data by shard within a stream. Kinesis Data Streams segregates the data records belonging to a stream into multiple 
  shards. It uses the partition key that is associated with each data record to determine which shard a given data record belongs to

- Currently, thousands of machines send data every 5 minutes, and this is expected to grow to hundreds of thousands of machines in the near future. The 
  data is logged with the intent to be analyzed in the future as needed: Create an Amazon Kinesis Firehose delivery stream to store the data in Amazon S3






*********************************************************************************************************************************************************

Business Intelligence (ETL and Visualization) : Business Intelligence comprises of data, technology, analytics, and human intelligence to 
       provide insights tha result in more successful business outcomes.

Business Intelligence is an act of transforming raw data into useful information for the purposeof business analytics

EX : i have a video course that has users across the globe to watch it and subscribe. as a creator of course for successful business outcome what i have 
     do. i need to gain multiple insights into the user activity. insights : 

 I want to see number of pageviews on my course.
 Find out sessions based on country of origin.
 Find out sessions based on the device the user is visiting from.
 From where the traffic is coming from [direct, google, bing, email, quora etc ]
 Session Duration of the user
 Age and gender of the users who are visiting
if we follw all these insights we can gain business outcomes.

when we discuss about business intelligence ; ONE OF THE PRIMARY STEP THAT IS INVOLVED Is ETL( EXTRACT TRANSFORM LOAD).

AWS ETL TOOL IS AWS GLUE

ETL : ETL is a data integration process that combines data from multiple data sources into a single,consistent data store that is loaded into a data 
      warehouse or other target system. 

 EX : take udemy platform. udemy contains many courses and they might be using multiple sirvices like (cloudfront,RDS,EC2) and so many others. each of 
      these servuce has its own set of logging data.and that logging data might be able to answer our insights(written above). we combine all logging data
      into a s3 bucket. since all of that data has diff format.we need to go ahead and perform the ETL operation. and after ETL opern that data can be stor
      ed in diff s3 bucket.

Sample case of etl :Data normalization : mister bob will be written as mr.bob.  8939-89837-83983= written as 893989484**.  ip-19.0.298.2 will be written as
      19.0.298.2.   28thaugust1987 is written as 28/08/1987 . All of these steps part of ETL process. After tranforming all data , now transformed data wil
      be stored in another s3 bucket.
- once we have transformed data in s3 bucket we can make use of various tools to analyze transformed data. for analysis of data we have QUICKSIGHT

* AMZON QUICKSIGHT is a scalable,serverless, machine learing-powered business intelligence (BI) servic built for the cloud. we have visualization dashboard
  in quicksight.we can see dasboard  which has users logged in from(mobile,laptop,tablet)

*** Business intelligence and visualization then Quicksight is answer.

- IAM permissions-related Access Denied errors and Unauthorized errors need to be analyzed and troubleshooted by a company. AWS CloudTrail has been
   enabled at the company : Search CloudTrail logs with Amazon QuickSight. Create a dashboard to identify the errors

- CloudTrail logs are stored natively within an S3 bucket , which can then be easily integrated with Amazon QuickSight. Amazon QuickSight is a data 
  visualization tool which will show any IAM permissions-related Access Denied errors and Unauthorized errors.

Q :

- Configure S3 event notifications to trigger a Lambda function when data is uploaded and use the Lambda function to trigger the ETL job
- Use AWS Glue to extract, transform and load the data into the target data store

- As the network is saturated, the solutions architect will have to use a physical solution, i.e. a member of the snow family to achieve this requirement
  quickly. As the data transformation job needs to be completed in the cloud, using AWS Glue will suit this requirement also. AWS Glue is a managed data
  transformation service.

- RedShift is a columnar data warehouse DB that is ideal for running long complex queries. RedShift can also improve performance for repeat queries by
 caching the result and returning the cached result when queries are re-run. Dashboard, visualization, and business intelligence (BI) tools that execute
 repeat queries see a significant boost in performance due to result cachin


*********************************************************************************************************************************************************


AWS LAKE FORMATION : 

-Basics of data lake : data lake is centralized repo that allows us to store all our structured and unstructured data at any scale.
- In data lake we can store data as it is without having to first structure the data( dat get pumped in data lake from various sources like on premises or
  ,real time data movement). and on that data we can perform or run diff types of analytics like SQL quiries, ML, Big data analytics.

- in aws we have data lake is LAKE FORMATION. from multiple sources(RDS,s3,no sql DB) we can ingest the data to lake formation..now data lake storage is s3
  s3 is hosting all of the storage. now multiple teams can be connected to s3 thye can run set of quires on s3 based on athena , EMR,Redshift.



Q :

- There is some data which exists within an Amazon RDS MySQL database, and they need a solution which can easily retrieve data from the database.
  Which service can be used to build a centralized data repository to be used for Machine Learning purposes : AWS Lake Formation

- AWS Lake Formation is a service that makes it easy to set up a secure data lake in days. A data lake is a centralized, curated, and secured repository
  that stores all your data, both in its original form and prepared for analysis. With AWS Lake Formation, you can import data from MySQL, PostgreSQL, SQL
  Server, MariaDB, and Oracle databases running in Amazon Relational Database Service (RDS) or hosted in Amazon Elastic Compute Cloud (EC2). Both bulk 
  and incremental data loading are supported.

- Performing in-place queries on a data lake allows you to run sophisticated analytics queries directly on the data in S3 without having to load it into a
  data warehouse

- You would typically use Athena for ad hoc data discovery and SQL querying, and then use Redshift Spectrum for more complex queries and scenarios where 
  a large number of data lake users want to run concurrent BI and reporting workloads.

- Petabyte-scale data lake analytics: You can run queries against petabytes of data in Amazon S3 without having to load or transform any data with the
  Redshift Spectrum feature. You can use S3 as a highly available, secure, and cost-effective data lake to store unlimited data in open data formats.


- High Performance + Big Historical Data +  business intelligence(BI) tools = data warehouse= Amazon Redshift

- Amazon Redshift is the most popular and fastest cloud data warehouse. Redshift is integrated with your data lake, offers up to 3x faster performance than
  any other data warehouse, and costs up to 75% less than any other cloud data warehouse.



AWS SWF : The Amazon Simple Workflow Service (Amazon SWF) makes it easy to build applications that coordinate work across distributed components.

SQS vs SWF
● AWS SWF offers a task-oriented approach.
● AWS SQS offers a message-oriented approach.
● AWS SWF ensures that the task is assigned only once and is never duplicated.
● With AWS SQS, we need to handle duplicate messages

Q : 

- A Solutions Architect is creating the business process workflows associated with an order fulfilment system. What AWS service can assist with 
   coordinating tasks across distributed application components : AWS SWF.

- Amazon Simple Workflow Service (SWF) is a web service that makes it easy to coordinate work across distributed application components. SWF enables 
  applications for a range of use cases, including media processing, web application back-ends, business process workflows, and analytics pipelines, to be 
  designed as a coordination of tasks

**********************************************************************************************************************************************************

REDSHIFT :
- redshift  is a fully managed petabyte-scale data warehouse used for storing large amounts of data for business intelligence applications.

- RedShift can be a single node or a multi-node cluster.
- Depending on the instance type, the storage is determined.
- RedShift clusters are currently supported in a single AZ-based architecture.

- Data in redshift can be backed up with redshift snapshots.

- Both automatic and manual snapshot options are available.

- Redshift nodes are continuously backed up to S3 and in event of failure, data is restored and new nodes are launched.

- RedShift can restore the data from snapshot by launching a new cluster by importing data from snapshot.

- RedShift snapshots can be copied from one region to another [ can be both manual or automated


Q :

- Convert the data warehouse schema and code from the Oracle database running on RDS using the AWS Schema Conversion Tool (AWS SCT) then migrate data from
  the Oracle database to Amazon Redshift using the AWS Database Migration Service (AWS DMS)

- Amazon Redshift is an enterprise-level, petabyte scale, fully managed data warehousing service. It uses columnar storage to improve the performance of
  complex queries.

- Amazon Redshift is a fast, fully managed data warehouse that makes it simple and cost-effective to analyze all your data using standard SQL and existing
  Business Intelligence (BI) tools

- Amazon S3 Select is designed to help analyze and process data within an object in Amazon S3 buckets, faster and cheaper. It works by providing the 
  ability to retrieve a subset of data from an object in Amazon S3 using simple SQL expressions

- Amazon Redshift Spectrum allows you to directly run SQL queries against exabytes of unstructured data in Amazon S3.No loading or transformation required.

- RedShift is a columnar data warehouse DB that is ideal for running long complex queries. RedShift can also improve performance for repeat queries by
  caching the result and returning the cached result when queries are re-run. Dashboard, visualization, and business intelligence (BI) tools that execute
  repeat queries see a significant boost in performance due to result cachin

- Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. Firehose Destinations include: Amazon S3, Amazon
  Redshift, Amazon Elasticsearch Service, and Splunk.

- Amazon Redshift is a fast, fully managed data warehouse that makes it simple and cost-effective to analyze all your data using standard SQL and existing
  Business Intelligence (BI) tools.

- Amazon RedShift is a data warehouse service used for online analytics processing (OLAP) workloads.

- datasets with millions of rows that must be summarized by column. Existing business intelligence tools will be used to build daily reports:AWS redshift

- Amazon Redshift is a fast, fully managed, petabyte-scale data warehouse service that makes it simple and cost-effective to efficiently analyze all your
  data using your existing business intelligence tools. It is optimized for datasets ranging from a few hundred gigabytes to a petabyte or more. 
  Amazon RedShift uses columnar storage.

- Performing in-place queries on a data lake allows you to run sophisticated analytics queries directly on the data in S3 without having to load it into a
  data warehouse

- You would typically use Athena for ad hoc data discovery and SQL querying, and then use Redshift Spectrum for more complex queries and scenarios where 
  a large number of data lake users want to run concurrent BI and reporting workloads.

- Amazon RedShift Spectrum is a feature of Amazon Redshift that enables you to run queries against exabytes of unstructured data in Amazon S3, with no 
  loading or ETL required.

- data analytics+ petabyte-scale datasets+ standard SQL + BI tools + open data formats = Amazon Redshift with Amazon Redshift Spectrum.

- Amazon Redshift  can be suitable for the following cases :
  Petabyte-scale data warehousing: Amazon Redshift is simple and quickly scales as your needs change.With a few clicks in the console or a simple API call,
  you can easily change the number or type of nodes in your data warehouse, and scale up or down as your needs change. With managed storage, capacity is 
  added automatically to support workloads up to 8PB of compressed data.

- Petabyte-scale data lake analytics: You can run queries against petabytes of data in Amazon S3 without having to load or transform any data with the
  Redshift Spectrum feature. You can use S3 as a highly available, secure, and cost-effective data lake to store unlimited data in open data formats.
 
- Amazon Redshift Spectrum executes queries across thousands of parallelized nodes to deliver fast results, regardless of the complexity of the query or 
  the amount of data.

- Limitless concurrency: Amazon Redshift provides consistently fast performance, even with thousands of concurrent queries, whether they query data in your
  Amazon Redshift data warehouse, or directly in your Amazon S3 data lake. Amazon Redshift Concurrency Scaling supports virtually unlimited concurrent 
  users and concurrent queries with consistent service levels by adding transient capacity in seconds as concurrency increases.




**********************************************************************************************************************************************************

BUILDING SCALABLE APPLICATIONS

**********************************************************************************************************************************************************

INTODUCTION TO  AMAZON SQS : (MESSAGE QUEING SERVICE)
- simple use case beforegoing to sqs : there is an application that restores the images. an org is designing an appl that enhance and restore the images 
that users submit through the online portal.

as part of appl overall architecture involves two components:
Image Gatherer - Takes the Images from the user via Upload button..as soon as user uploads the image, image goes to the image gatherer. and image gatherer
 wil send that image to enhancer
 -               image enhancer is responsible for enhansing and doing some conversion on image.

Imager Enhancer   -  Receives the Image from Image Gatherer.
* in this type arch what happens if image enhancer goes down. image gatherer is working fine. the user is able to upload the photo. but when gather send
 photo to enhancer the server is down

ANOTHER CHALLENGE : 
- Due to the popularity of the application and the huge traffic spike, Medium Corp has decided to add more image enhancer servers.
  So now we have 4 image enhancers. even if one enhancer goes down. the gather will be able to send the image to the other servers.
- so for image gatherer to discover the latest servers that are running  a Db or configuration file(4 servrs with each associated IP) with relevant infor
 need to be done

  the challenge : if one server goes down. gather has no way of knowing that the server is down. there needs to be another logic that will check the health
 check of all of the servers. if healthcheck is fail then the config file entry associated with that server also needs to be removed 


- TO OVERCOME THIS THERE IS A BETTER ARCHITECTURE queue :  
* here we design a queue b/w gatherer and enhancer . image gatherer just have to send the image to queue. enhancer just have to collect the image or msge
 from queue. in gatherer no need to have cofig file, no need to do health checks.
** One of the main functions of a message queue service is to take a message from a Publisher(gatherer) and forward that to a consumer(enhancer)
- since the gatherer is sending the images to queue. HERE QUEUE NEEDS TO BE HIGHLY AVAILABLE .if queue goes down nothing could be done.


** SO THIS CHALLENGE OF HIGHLY AVAILABLE AND RELIABLE QUEUE IS TAKEN CARE BY AMAZON SQS SERVICE.
** Amazon SQS is a fast reliable, scalable, and fully managed message queuing service.
-  Amazon SQS makes it simple and quite cost-effective to decouple the components of a specific application.
   Here we already have a gather and enhancer. now we create a highly available and reliable queue with the help of SQS SERVICE and config our image 
gatherer to	send a message to the queue, and configure the enhancer to take image from queue

DEMO : I have one sqs queue in console and currently there is no messages that are available. also have 2 EC2's for image gather and another for enhancer.
 here gatherer will send some messages
       to sqs queue . and enhancer will fetch the images from queue. after logging in gatherer ec2 if we type " ./send-message.sh " this will send message
 to sqs queque. it sent 5 messages at atime. now go to sqs and see messages availble are "5" . login to enhancer ec2  and run script ./recieve-message.sh"
 it will recive 1 msge. if we run it again it will recieve 2 more mess now if we go to sqs console we can see messages available are 2. and messages in 
 flight are 3.

TIGHTLY COUPLED SYSTEMS :  Components of system architecture directly communicate with each other and have a hard dependency on each other.
 ex : we have producer and consumer( like gatherer,enhancer) . here producrer is directly sending a mesg to consuer. incase the consumer is down, the 
producer will also starts to give error messages to users

LOOSELY COUPLED SYSTEMS : Components of system architecture that can process the information without being directly connected.
 ex : we have a producer, queue, consumer. now if consumer is down, then the producer doesn't need to worry essentially . all producer can do is it can tak
 image from user and it can send theimage to queue. it can give success message back to user. once the set of consumers are back online again they will 
fetch the image from queue and they will do processing.
- now mostly in prod env we will see loosely coupled system

DEMO : when we open sqs console for new queue creation. we can see standard and FIFO queue. after selecting standard for the time being we give name of 
queue. and in configuration retention period ( means once the producer sends the message to the sqs , for how many days message should be present in the

 sqs we can define it b/w 1 minute and 14 days).max mesage size(1-256kb) now go and create a queue. now we can send and recieve images. instead of doing
 it using ec2. we can do it in console only. click send message and type message and send. now to recieve a message from queue we just have to poll for 
messages in console only. we can see one message and if we click on it we can see the body of msge. after we recieve the message if we goto sqs console we

 can still see the available message is 1 eventhough we opened it. means after consumer has taken msge from queue the msge is still present in queue unless
 and untilthe message is explicitly deleted by consumer or the expiry is set. if we try to recieve msge again we can see the same message again. along 
 with that we can see recieve count is 2 .means 2 times msge is recieved by a concumer.

SQS Visibility Timeout :
Challenge: When a consumer receives a message from the queue, the message still remains in the queue and is not deleted. reason is let's say a producer 
send a message "m1" to the queue. now oneof the consmer has polling for the msge and he recieved m1.now msge is recieved and let's assume theat msge is 
deleted from the queue. now one of the consumers while processing m1,ifit goes down. here m1 which is part of consumer also lost and m1 in queue also lost.
 means entire msge is lost. this way is not good for prod env

- this is the reason why sqs queue will not delet msge even after the msge is recieved by consumer
Soln : what will sqs do when ever a msge is recieved by consumer.

Visibility timeout : 
- When a consumer receives a message from the queue, the message still remains in the queue and is not deleted. However, it remains in a hidden state for a
  specific interval and can be deleted bythe consumer once it has completed processing.

 now i have msge m1 present in queue. and consumer1 has recieved m1. as soon as the msge recieved, the msge present in the queue becomes hidden not yet 
deleted. now consumer expected that it will go ahead and complete the processing msge m1. after the processing is completed the consumer can go ahead and
 delete the msge from sqs queue. however consuer1 while processing mesge m1it will suddenly goes down, then after certain amount of time the hidden message
 will be removed and the msge will be back to the queue. now another consumer that is runing it can go ahead andfetch the msge from the queue and it will 
 process it. after process complete it will go and delete the msge from queue.


* we can see visibility timeout in queue config  in console. visibility timeout can be b/w 0 sec to 12 hours.
if we give visiblity timeout is 30 secs. after 30sec the msge is not deleted by consumer. then the msge automatically shown up after 30 sec in sqs.

SQS DEAD LETTER QUEUE: 
- Amazon sqs supports dead letter queues, which other queues(source queues) can target for messages that can't be processed(consumed successfully)

Ex : let's assume that i have an sqs queue. along with that i have a lambda function which acts as a consumer. this lambda function basically fetches msges
 from the sqs queue and processes themafter the process is completed it will delete the msge from original sqs queue. EX : however the first 4 messages has
 been successfully processed and 5th msge is it was able to fetch butit was not able to process it due to some reason. so now the 5th msge is contineous to

 be part of the queue. because of lambda function is not able to successfully process this specificmsge so that it can later delete it. since the 
processing is not completed. this 5th msge is contineous to stay in sqs queue. whatever other processes that are fetching these msges fromqueue this msge 
will always lead to an error. TO OVERCOME THIS WE HAVE A FEATURE OF DEAD LETTER QUEUE.

the concept is that if there is a msge that consumer not able to process it move the msge to another queue called as dead letter queue. so the 5th msge is
 now moved to another queue called asdead letter queue. and then the developer can manually go and inspect the msge from dead letetr queue to understand
 that why things are not working properly

Demo : i have two queues.1.msge queue(configure in such a way that max amount of recieves is 2. after the msge is recieved more than 2 times it assumes 
 that there is an issue with the msge) consumer is fetching the msge but it is not able to process. hence the msge will be moved to dead letetr queue.
       2. Dead letter queue( whic will store msges that are processed more than 2 times in normal queue)
setup : in 1.msge queue we have an option of dead letter queue( if we enable it we can see option to choose any other existing queue as dead letter queue)

Imp points :
- when a msge moves to a dead letter queue the time stamp remains unchanged.
Ex :
- message has been in the source queue for one day and moved to dead letter queue.
- message retention period in dead letter queue is 4 days.
- message will be delete from dead letter queue after 3 days. ( because msge alread present in source queue for one day)
* best practice is to have higher retention period for dead letter queues than source queue.



AMAZON SQS QUEUE TYPES : 1. Standard Queue : Ocassionally messages are delivered in an order which is defferent from which they were sent.
                         (ex : 4,3,3,5,2,1 : messages are not in order,also have duplicate
                         2. FIFO Queue : the order in which messages are sent and recieved is strictly preserverd . ( ex : 5,4,3,2,1 in order) 
- primary diff b/w both queue types is message ordering. 

Standard Queue:
 Throughtput : it supports a nearly unlimited no of API calls per second, per action related to send message, recieve message, delete message 
 Delivery : a message is delivered atleast once, but occassionally more than one copy of message is delivered( Duplicayes is allowed)

FIFO queue : 
 Throughtput : it supports upto nearly 300 ip calls per second, per api method(  related to send message, recieve message, delete message )
 Delivery : a message is delivered once and remains available until a consumer processess and deletes it. Duplicates aren't introduced or allowed into the 
            queue
- while creating fifo queue the name of queue must ends with(.fifo) like m=first.fifo


AUTO-SCALING BASED ON AMAZON SQS :
- in many scenarios the number of ec2 instances that are required directly proportional to the no of msges in sqs queue

ex : let's say there are 10 messages in the sqs queue and since there are 2 ec2 instances that are running each ec2 instance will take care and process 5
 messages. afer processing they uploadthe artifacts(artifact means object) to the s3 bucket. let's say tmrw there is a big scale going on. in the sqs 

instead of 10 msges there are 10000 msges. since there are lot of msges inthe sqs queue. we have 2 options. 1. keep 2 ec2 instances and the processing of
 10000 messsages will take lot of time. and 2. increase the no of ec2's that are running in the prod env.from 2 ec2's to ec2's will proccess msges quickly.
 however price will increase. after the sale 10000 rduced to 10 messages. now we have to terminate the extra ec2's that are configured.


- now in prod env we want to impl arch in such a way where ec2 instances are launched and terminated based on the no of messages in sqs queue.

scaling based on amazon sqs : 

Path: sqs queue - connected to an application( which can read no of messages innsqs queue using (SQS ATTRIBUTE : it will give approx no of messages in 
queue) ) - amazon cloudwatch is connected to application( it will recieve custom metrics from appl) - autoscaling event( it is connected to cloudwatch.
 based on the capacity or metrics provided by appl to cloudwatch it willlaunch or terminate ec2's in asg) 


MESSAGE QUEUES IN DB TRANSACTIONS : 
use case : assume that a asingle Db is hosted on RDS . due to sales the no of write txns has reached 20X normal load.
           since we have one db. so many of the reqs are failing regularly due to high load.
           these sale promotions are scheduled every alternate month.
vertical scaling is the soln : we can increase the size of DB (vertical scaling) and also increase the no of provisioned IOPS so that it can handle 20X 
capacity load. with this soln 2 challeng

                               1.Down time( shutdown, resize and start db again). 2. increased cost
- SO THE BETTER APPROACH IS MESSAGE QUEUE IS USED. Here instead of messages directly inserted into DB , they are now inserted as part of SQS queue. now 
ec2's will pull the messages from the sqs queue and they will insert in the DB depending upon the load. let's say the db is already going through the high
 load,then the msges can stay in the sqs for a minute or 2

* This type arch seen in many org. but in this arch there is a little time delay b/w client msge and db
  ex : if we update any profile name in any app. to do this 2 to 3 min not a prblm.

AMAZON MQ :  ( Amazon MQ means managed message broker service, just mq no full form)

current stage : 
- SQS is a simple queuing service
- sqs has limited set of fxnalities
- there are various message broker services like apche active MQ that provide many set of features that are extremly used in on-premise org's.
 Now org face challenges when they try to migrate to cloud. in cloud we have sqs.now sqs and active mq if there is any migration that need to take place
 then the appl code will also need to be re written.but org want to avoid this.
So 
* if we are using messaging with existing applications and want to move our messaging to the cloud quickly and easily the amazon mq is suitable.
 Path : amazon mq(mnage your message brokers on AWS) [ now it will go and create an active MQ or rabbit MQ message broker ; behind the scenes it will go 
and create a set of ec2 instances it can be single ec2 or multi. this ec2s have active mq configured. and we will be able to connect to appropriate message
 brokers] 

Imp points :
* if we are building brand new applications on cloud then aws recommends you consider Amazon SQS and SNS instead of MQ.
* Amazon sqs and sns are lightweight, fully managed message queue and topic services and can scale infinitely(MQ can't scale infinitely) and provide simple
  easy to use APIs.
* an active/standby amazon mq broker is comprised of 2 brokers in 2 diff AZ's configured in a redundant pair. data is written on shared efs volume.



SNS (SIMPLE NOTIFICATION SERVICE) : SNS is a fully managed messaging and mobile notification service for delivering messages to the subscribed endpoints.
Path : Publisher - SNS topic - Subscribers

* whenever a publisher sends a message to the sns topic, sns topic will intern forward the message to all the subscribers who are connected with that sns
 topic.here if we don't have sns topic, the web appl(publisher) would need to have a logic to send message to each of the service endpoints. ( SNS supports
 endpoints like Email,sms,https,notificatiohttp, lambda etc). so instead of adding all the logics into web application, all we have to do is follow the 
 traffic to sns and let sns handle all the backend things which are require.

Demo : in sns console we can see topics. click on create topics. give the name and create topic. after that subscribe endpoints. sms will autonatically
    get subscribed. and for email we need confirmation from that mail. after subscriptions we can directly publish a message from console only.

use cases : AWS CLOUDWATCH integrates well with sns.we can integrate the cloudwatch alarms that we might create with the sns topic.
- Whenever a disk usage of a server exceeds 95%, send an EMAIL and SMS notification to the NOC team.
- Whenever a server load in production is more than 90%, send an email and SMS notification
- sns also used to send push notifications. in exam whenever the question push notifications then SNS is the answer mostly.

SNS FANOUT :
fanout means : fanout is a pettern in which message is delivered to multiple destinations.
use case :ordering a product : let's say customer has ordered a product on a website. now this order and it's associated details are sent to 2 set of 
consumers.one is logistic queue, this will be connected with the appl that deals with logistics. and 2nd is notification queue.and we can also connect
 other queues related to other use cases.

SNS fanout : in this a publisher which publishes the msge to the sns topic. and this sns topic sends the msge to the multiple set of endpoints. here the 
endpoints are sqs queues. from each sqs queue i have a set of ec2 instances , this ec2 instances can fetch the msge from queue and they can process.

def : the fan out scenario is when a message published to an sns topic and it is replicated and pushed to multiple endpoints, such as kinesis data fire
 house, delivery streams,sqs, lambda fxns.

another use case :
  we can also use fanout to replicate data sent to our prod env with our test env.

SNS MESSAGE FILTERING : 
- sns default settings : whenever we send a msge to the sns topic , the msge is forwarded to all of the subscriber endpoints. suppose i have 2 sqs queues
  as endpoints. if i send hello world to sns, sns will forward to both sqs's.

- However sometimes we want to send msge to particular selective endpoint based on the type of msge that is recieved in sns. like 2 sqs's prod and dev. 
if prod related msge comes to sns then sns should forward it to prod only. same with the dev env.

in order to acheive SNS filtering we need to add a filter policy.
- filter policy is a JSON object containing properties that define which messages the subscriber recieves . in policy alice - prod, bob - dev filter policy
  attached
 EX : if alice send message to sns it will go to dev queue, same to bob - dev env.

S3 Event Notiﬁcation ( s3 is more than just storage) : 
- Amazon S3 notification feature allows us to receive notifications when certain events happen in your bucket.

  ex : let's say some x operation that heppened in our bucket.(x might be uploading object to bucket) . so based on this put oprn the s3 bucket with the
 help of event notification can send even to various endpoints like sqs and sns,lambda. here the event basically contains someone has uploaded the file 
 to the bucket like put oprn with time stamp.

Demo : create an s3 bucket. open s3 bucket properties. we can find s3 event notifications section. inside that we can see types of events(put,post,copy, 
delete related events, restore related)at last we can see destination(lambda,sqs,sns) we select sns as destination. create sns topic. in sns if it want 
to recieve notifications from s3, sns has to allow from s3. here we need add policy to s3 event notification. after adding policy in sns it will allow s3
 bucket to publish notifications to sns.

use case : we use s3 event notif as part of security related project. we have a s3 bucket. anytime a user add a bucket to the s3 bucket we don't know if 
the data is infected with malware,virusso the data needs to be scanned. here lambda function will take care the scanning. if there is any virus, lambda
 will notify us via mail.

AWS Systems Manager(ssm) ( interesting set of services) : it is a group of services which allows customers to have a better visibility and control of 
 the infra.

- we have a ssm and it has a subset of services( run command, parameter store, sessions manager, patch manager) along with many others.


Systems Manager :
Use case :
 the basic idea behind the " systems manager " is that there will be an ssm agent that will be installed in ec2 instances, and the customer can provide a
 specific tasks to the installed agent from the systems manager console.
path : ec2(with ssm agent installed) - to this from the ssm console in aws we instruct the ssm agent to perform task A. now ssm agent once it recieves the
       instruction it goeas ahead and implements the task a in ec2. and whatever the results are retrieved by ssm agent and sent back to the SSM.

 here runcommand,sessions manager, patch manager all of them depend on the ssm agent.how ever services like paramter store doesn't depend on ssm agent.
 Demo : ssm agent installed in ec2. open ssm console in aws. select sessions manager service. click on start session. now it is connected to ec2.

SSM-RUN COMMAND(Running commands remotely) :

 Run command as the name suggests it allows us to run commands in the instances where ssm agent is installed.
Path : i have a run command. i am supplying "iptables-F" command(this flush the iptables) . ssm agent will take the command and it will run it as part of
       ec2 instances.agent will fetch the result and resukt will be sent backt to central service(may be run command) 
- in org many times if want to install any anti virus agent. but no of ec2's are more than 400.in such cases run command used to help a lot.

* run command provides much more granular features because of it's " command document" feature.
  various command docs available that can perform certain ready made actions :
 ~AWS-runansiblePlaybook 
 ~AWS-ConfigureDocker ( configure docker and it will install docker in our system)
 ~AWS-InstallMissingWindowsUpdates
 ~AWS-RunShellScript
DEMO : 
 in ssm console first we need to check whether ssm agent is online or not ,we can check it by going to fleet manager console.
 go to run command console and start running a command.while trying to create a run command we can see doc feature. here we selected RunShellScript cmnd do
cument which will run shill script commands. after that we rite" touch /tmp/runcommand.txt  : it will create a runcommand file in tmpfolder, 
  echo " this is my 1st command" > /tmp/runcommand.txt  : this will write the text inside that text file  " . in targets section we can select them by usin
  tags( useful for 100's of ec2s) , or choose instances manually. we can even set the o/p's of run commands as s3 bucket etc.


SSM Sessions Manager : 

- sessions manager allows customers to connect to the instances through an interactive one click browser based shell or through aws cli.
use case : i have set of ec2 instances running.instead of using ssh client like putty, mobaxtream we can conncet to ec2's from webserver itself only with
         ssm manger.
demo : go to fleet manager in ssm console.check our agent is in online state.go to sessions manager console and start the session by clicking the ec2 ssm
       like target ec2 instances. if we click on start session we can run script commands on that sesion. 

Diff B/W EC2 connect and Sessions manager : 

EC2 connect : if any user wants to connect to ec2 no IAM ROLE IS REQUIRED. ec2 connect required Security goup with port 22 allowed. Public IP must be requi
              for the Ec2 connect.

Sessions Manager : if any user wants to connect to EC2 through sessions manager he need IAm Role with appropriate policy. SesionsManager doesn't ned any SG
              even sg port 22 is not allowed it doesn't matter for sessions manager. for sessions manager even if the ec2 is in public subnet and it doesn'
              have public IP we will still be able to connect to it.

BENEFITS OF SESSIONS MANAGER : 
- centralaized acess control using IAM policies
- No inbound ports needed to be open
- logging and auditing session activity
- one click access to instances from the console and cli
- no need of VPN to connect to instances
 logging and auditing session activity : in sessions manager sessions history we can see which user connected to which instance. here we can also enable 
 cloudwatch logging with this we can even see which commands used by user to the instances.

SSM - PARAMETER STORE(no hardcoding the secrets) : parameter store is to offload the secrets from the appl to a centralaized storage.
def : aws ssm parameter store provides centralized store to manage the configuration data, whether it is plain text data like database strings or even
      secrets such as passwords.

use case : left side we have an Appl A code. here developer is hardcoding the username and password.( lot of developers do like this and they commit the
      code to github or any version control system repositories that the org used.so any user who has access to that repo will be able to find username and
      password here. this is not a best approach). so with the parameter store a better approch is the credentials would be stored in a centralized way whi
      ch would be parameter store. and from our replication all we have to give a reference like " paswrd : ${ssm.getparam} " it is a reference to ssm para
      meter store.when ever application executes it can fetch the pswrd from parameter store. here even code gets published by mistake the attacker will 
      never get the pswrd here

Demo: go to ssm console and open parameter store console.click on create a parameter. give the name and select the type as string,stringlist(multiple val
      ues with commas). select string and give value as "mypaswrd. create one more and give name, here select type :securestring. here it goes and encrypt
      the data with the help of KMS.here we also have to mention KMS key source. give value as "mysecurepswrd".


    now we have 2 parameters.one with string,another with securestring.if we click on string parameter it will show the value(pswrd). in securestring para
    it will hidden the value(pswrd). it will show if we click on show. here we need to have appropr permisiions.

we can also do this through CLI.  ex : "aws ssm get-parameter string" it will give that paramter details along with the value(pswrd). if we do the same wit
securestring it will show value in encrypted manner not plain text, if we give "--with description" it will show the pswrd if we have appropriate permissio

SSM-PATCH MANAGER(interesting set of services) : patch manager automates the process of patching managed instances with both security related and other 
 types of updates.

use case : i have an ec2 and a patch manager. patch manager will go and scan for missing patches. after scanning the ec2(server) for missing patches it wil
 go ahead and it can create a list of patches or updates which are missing on the server.updates can be bug fixes or security related. once the patch manag
 er figure out the missing fatches it will go and apply the patches. if the server is missing patches then it is vulnerable to security and other type of
 attacks.after applying patches server becomes green(all are updated, before it is red(patches required)


Patch baseline : it determines the list of missing patches that need to be installed in the instance.a patch baseline defines which patches are approved 
 for installation on our instances. we can specify approved  one by one. suppose we have set of patches that server is missing. so at patch baseline me can
 mention which patches need to be installed and which should be denied or rejected  into the server.

Maintanance Window : it provides a mechanism for schedulng a particular activity on the specific target.
  ex : perform patching activity at 2 am in the morning, this can be setup or configured by maintanance window.

AWS TAGS ( META DATA TO AWS RESOURCES) :
- a tag is a label that we assign to an aws resource.
- each tag consists of a key and an optional value, or both which we define.
Use case : 
There are 3 ec2 instances are running. how will we identify them : after assigning any label or tag we can easily identify their purpose and use.
ex : email-server ,  payment-app, compliance system
- a resource can have multiple tags assigned to it.
- AWS has 100's of services. not all of them support tags.
- max no of tags per resource : 50
- tag keys and values are case sensitive. if we give tag as (name(key) = email-server(value). it doesn't show on the ec2. beacuse we given name instead Nam
- tags can be integrated in billing and that allows customers to understand their aws bill in a granular way.

- tags can be used with IAM to control the access to resources. allow alice to only delet resources which has tag of env as development
- it is imp to have consistent and effective tagging strategy.
  ex : alice create ec2 with the tag of key of env and value of prod
       mathew create ec2 with the tag of key of Env and value of Prod. this 2 are wrong. both of them have to use small or big. not both. it will be diff.

AWS RESOURCE GROUPS (organizing resources centrally) : 

- aws mgmt console is organized based on services(Ec2, ebs,elb etc)
 use case : now i want to see resources of the payments team.payments team might hiva ec2,rds,elb, ebs.how to we show the all resources that are part of
  single team under a single console. this can easily achieved with the help of RESOURCE GROUPS.

- within the resource group we can quickly see what is the exact resources of any particular team.we can also pefform multiple task on them.
- resource groups can also be used as part of automation. let's say i have a resource group of ec2-automation. we have 4 ec2's in that. on top of that grou
  we can perform automation tasks like : restart ec2 instances  ;  Attach IAM to ec2's   ; Create AMI of ec2 instances  ; perform patching activities 

Demo(how resource groups can be created ) : first create a netwrok interface, associate it with any subnet and with any SG, and add tags to it. for resourc
  Groups we need tags as mandatory and click on create interface. after that create a EIP . create 2 instances with the tag of dev.
  Now click on resource group . create a new resource group(2 options : tag based or cfn stack based ). click on tage based and select tags(key name and
  value dev). after that click on preview groups we can see 2 resources are running and one EIP. after that if we add dev tag to any other SG.now go to 
  resource group refresh it, you can see newly addes security group in that.


AMAZON EVENTBRIDGE(CONNECTING SERVICES) : 
- if we have used cloudwatch events , eventbridge is similiar to that and it is more enhanced version to cloudwatch events.
- it basically use the same set of api's , in the featyre cloudwatch events might be replaced with eventbridge
- Event bridge delivers a stream of real time data from event sources to targets.


- the overall architecture involves 3 primary components. 1. Event source on left. 2. target on right. 3. event bridge in middle (where rules are created)
 so whenever a event is created(it can be ec2 that is getting terminated or s3 bucket that is getting created or any user that has created support ticket).
 whenever a event happens we want a specific target to be triggered. let's say a support ticket got created in custom appl then it should trigger the sns
 topic. the sns topic will go and send an email or sms.this can be achieved with the help of Event Bridge. 


- targets can be lambda, sns, api(to saas apps or cutom apps).
Use case : whenever ec2 stopped Admin should be notified. here ec2 to sns.ec2 is event source and sns is target. in the middle we have evenbridge. while
 creating event bridge we specify the specific rules or events that should be notified to targets.

2nd use case : stopping dev ec2 instances. we used to implement this based on the cloudwatch event service. here we used to stop all the dev instances at
 8pm on friday and start at 9am on monday. here we have event bridge . event source can be a schedule like 8pm on friday eventbridge gets triggered.here
 it will go and run a lambda fxn called as stop ec2 . now lambda will stop all the ec2's. similiarly we have a schedule on 9am on monday . in this schedule
 eventbridge gets triggered and it will go ahead start ec2 based lambda fxn on mondays.

Demo : source is s3. target is cloudwatch logs. s3 has put object operation or delete objct operation. automatically data should sent to cloudwatch log grp
     here we need to cheeck whether target is available.we have one s3 bucket available.now we have to enable cloudtrail.so for our eventbridge to know 
     that specific oprn at a s3 level,we need to have cloudtrail. let's create a cloudtrail in one region where we are.inside that for s3 we have to enable

     data events,inside that we have data events of s3.now open eventbridge and create rule.in define pattern we have two types(event pattern based on patt
     erns and schedules to invoke targets based on schedule. select event pattern inside that select aws service as s3. inside that select objct oprns as 
     put object and delet object.on the right hand side we can see event pattern code. below down we can see target section we can select targets to invook
     based on the actions we given in source.selct cloudwatch log group as target. and click on create rule. now if we go to cloudwatch console inside tgat
     if we go to log groups we cane see s3- events are created.
- eventbrideg not only support aws services. it also supports custom apps, saas apps etc.



***********************************************************************************************************************************************************


DNS ( DOMAIN NAME SYSTEM ) : 


***********************************************************************************************************************************************************


Introduction to DNS( The backbone of internet) : 
- reason for bacbone of internet : lot of attacks happens on dns to slow down the overall internet speed(ex: biggest DDOS attack  slows internet).
 
use case : in phone when we have a number 2 imp aspects that happen.first we have to store a name.2nd is we have to store a associate number(alice : 99298)
  so whenever we want to call someone we go to contacts list.and we click on specific contact.it will automatically start calling.here we don't have to rem
  ember evryones number.generally whenever we get a sim we get the number associated with it.we can't remember no's. so in phonebook each name is associate
  with its no. it is easy.

- DNS also works in the same way.if we talk about internet or servers everything is based upon numeric systems.every server has specific ip addresses.
  like google has on ip,yahoo has one ip address. similiar to how a phn num works. every person has a different phn no.we can't have a same no for2 diff pp

- similiarly every website or network in the internet has a unique ip addresses. since humans are not good at remembering ips. so there came a need for hav
  ing a mapping b/w the name and the ip addresses. the software that does this specific mapping b/w name to address is called DNS.

- DNS servers are responsible for converting or translating domain name(name of person) to ip address(phn.no). we supply the domain name to DNS server and
  dns server respond with the corresponding ip address associated with the domain name.

- if i search zealvora.com entire blog will loaded.in the backend lot of things that happen.unless and unitil we don't get the ip address associate with
  dns.com we can't really do anyting.
 RUN nslookup zealvora.com : it will give corresponding ip address associated with zealvora.com

PING zealvora.com : it resolve the ip address associate with the domain name.once it resolve the ip address it is actually sending the reqst on the 
specific ip address. (  resolution resolve is the process of translating IP addresses to domain names)

TRAFFIC PACKET CAPTURE OF DNS PACKETS : study about the internal workings of the query is been made,and how the response is been recieved in terms of packe
  t analysis.
when we did nslookup zealvora.com it gave us ip " 139.62.8.95" associated with the zealvora.com. what is the dns server which gave us this specific ip addr
  and the dns server is "139.62.8.5" this ip addres. if we do nslookup zealvora.com 139.62.8.5 (doing query on dns server itself); or if we do nslookup zea
  lvora.com 8.8.8.8 (ip address of google) it will still give the ip address of zealvora.com(139.62.8.95) 

DNS RECORDS : DNS records are basically mapping files stored  in the DNS server
Example:
kplabs.in              → 128.30.45.50
ipa.kplabs.in          →   128.45.32.54
spacewalk.kplabs.in    →    139.20.42.52
- Three dns records stored in dns server.

* we have a dns client and dns server. dns client will send a query which is ip address of kplabs.in. how will dns server knows ip addresses.ans : dns serv
  er has a databse which has mapping for each domain name to its ip addresses.now dns server will go and check its db and give the result back to the clien
	
DNS RECORD TYPES : there are various types of dns records,each one serves a specific purpose.
types : 
● A
● AAAA
● CNAME
● ALIAS
● MX
● NS 
Q. DNS record must all zones have by default? SOA.
Q. Which protocol is primarily used by DNS to serve requests? User Datagram protocol.

- while creating a specific dns record we have to select specific type that we have to use. if we don't select the correct type

DNS records A & AAAA :

A RECORD :  A which basically stands for "address" is one of the most basic types of DNS record and it points a domain to ip address 
EX :  zealvora.com - 192.37.89.90
      hemanth.com  - 168.98.8.35   * here a domain is attached or associated or matched to ipv4 address.

 AAAA RECORD :   AAAA records similiar to A records with difference of input being a IPV6 address.

EX : ipready.org  - 1623:9888:9383:9839::100    * Here domain name is associated with IPV6 address.
- try : nslookup ipv6ready.org  : we will get ipv6 address associated with domain name.


INTRODUCTION TO ROUTE53 ( THE DNS SERVER IN AWS ) :
- In order to get the DNS resolution, it is important to have a DNS Server running.
  whenever we do ns lookup or whenever we open any website like google.com in web browser ultimately behind the scenes the request is always made to a DNS 
  server and that dns server is responsible for resolution EX : if we do google.com in web browser, the browser intern makes use of a dns query to a speci
  fic dns server. and that server returns back with the result

Q. WHAT are the softwares used in DNS server?
There are various software like Bind that provides features associated with DNS Resolutions
ex : i have a ec2 . i want to create a own dns server. we will install bind , we configure appropriate settings. and our dns server will be ready.
    what if ec2 or bind goes down. essentially all the resolutions that has been made wil will fail. if any client will makes any query to dns server,
    he won't get any response.

Managed vs Unmanaged :

Un-Managed Approach :

- Organization can configure and maintain their own DNS servers.
- Good for learning, not recommended on the longer term.
cons : we have to manage,configure, HA we have to manage.

Managed Approach:

- Let the service provider manage the DNS Servers for you.
- Sleep peacefully.

Managed DNS Service : Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service.
- instead of ec2 instance running bind we make use of route53.

CREATING HOSTED ZONES IN ROUTE53 ( CREATING ROUTE53 RECORDS ) :

Hosted Zone(HZ): A hosted zone is a container for records , records contain info about how we want to route traffic for a specific domain, such as 
example.com.
- we have a route53. it has a hosted zone. it contains a table which have domain names and its mapping values.
hosted zone for example.com : which contains multiple records or entries ( example.com, demo.example.com , kplabs.example.com each has diff types like 
cname,a, soa)

demo : inside route53, click on create a hosted zone. ; [ give name if we want to register any domain name. we are not doing this ]. in HZ configuration
 give domain name like kplabs.internal
-  when we click on create hosted zone, there are 2 types of hosted zone.
- Public HZ : specifies how we want to route traffic on the internet.
ex : whenever we type kplabs.in the traffic that comes to the dns server is routed.routing happens over the internet.
  real ex :  mobile phone used for public communication. we comm over internet,cell phone towers.

- Private HZ :  specifies how we want to route traffic in an Amazon VPC 
ex : we specified a domain name called kplabs.internal this is actually a private domain name. we haven't really purchased yet.for this use case we can 
select private HZ. here all the queries to this domain name can be happen from the resources that are part of VPC.no one from the internet will be able to 
send the query.
  real ex : walkie talkie : it primarily used for private communication b/w individuals or a group of team.s

- if i have already have a domain name kplabs.in and there will be users who will be accessing over the inter
net.in this case we have to select public HZ. 
  for Public HZ domain name needs to be purchased.

demo contd : select private HZ. choose which vpc to associate with HZ.after that click create HZ. now we can see diff records with diff types created by 
default. create one more record.give record name as demo(kplabs.internal suffix for demo.because it is main domain).specify the record type as A. specify 
the value(ipv4 address). this basically converts to ip address when a request made. create a ec2 in the same vpc.login to ec2 using ssh. and run "nslookup 
demo.kplabs.internal" we will get the ip address assocyiated with and.we can see dns server ip address with #53

 

CNAME RECORD (NAME BASED RECORD ) : 
- CNAME records are used for pointing a domain name to another hostname.
- In the below diagram, example.internal has CNAME to kplabs.in
- When we resolve example.internal, the response will be 192.168.0.5
ex : if we request anything to example.internal the req forwards to kplabs.in using cname type. path : example.internal - cname associateion - kplabs.in.
use case : we have load balncer. when we create LB it doesn't generate ip address of it. it only provide domain name. if we want to send request to lb,
 we can use it using cname type.
- many aws services which makes dns names only like cloudfront.

 Alias:
-An Alias record can be used for resolving apex or naked domain names (e.g. example.com). You can create an A record that is an Alias that uses the 
customer’s website zone apex domain name and map it to the ELB DNS name

Q : 
- above A is for address. here A is for Alias)

Create an A record that is an Alias, and select the ELB DNS as a target  

- An Alias record can be used for resolving apex or naked domain names (e.g. example.com). You can create an A record that is an Alias that uses the 
  customer’s website zone apex domain name and map it to the ELB DNS name
- A standard A record maps the DNS domain name to the IP address of a resource. You cannot obtain the IP of the ELB so you must use an Alias record which
  maps the DNS domain name of the customer’s website to the ELB DNS name (rather than its IP).

- The failover routing policy is used for active/passive configurations. Alias records can be used to map the domain apex (example.com) to the Elastic 
  Load Balancers.

- Failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy. The
  primary and secondary records can route traffic to anything from an Amazon S3 bucket that is configured as a website to a complex tree of records.

- A company hosts statistical data in an Amazon S3 bucket that users around the world download from their website using a URL that resolves to a domain 
   name. The company needs to provide low latency access to users and plans to use Amazon Route 53 for hosting DNS records : create a web distribution on
   amazon cloudfront pointing to an Amazon S3 origin.Create an alias record in route53 HZ that points to cloudfront distributio, resolving to the
   application's url domain name




MX RECORD (MAIL SERVER USE CASE) : 
- A mail exchanger record (MX record) specifies the mail server responsible for accepting email messages on behalf of a domain name.
- A single domain is used for multiple purposes, including loading websites, sending emails, and others.
ex : if we send a mail yo instructors@kplabs.in it will go to dns server record type of MX. mx will forward or send that mail to it's associated mail 
like aps.google.com.

- The characteristic payload information of an MX record is a Priority and the domain name of a mail server.
- The priority field identifies which mailserver should be preferred
ex : i have two records of MX type in dns server.

 
TXT RECORD ( TEXT INFO IN DNS) : 
- The DNS ‘text’ (TXT) record lets a domain administrator enter text into the Domain Name System (DNS) record.
ex : i have a subdomain of txt.kplabs.in and the record type is TXT. to txt we can add our own text under value section. here any time user visits this
 specific domain he will get that specific value. when we do nslookup txt.kplabs.internal ; we get " no answer or can't find the answer ".
Use case : Prove Ownership of the Domain

- I have a domain (kplabs.in) and I want to prove ownership of this domain to a 3rd Part.
- 3rd Part asks me to store the following contents on verify.kplabs.in: “3rd Party Verification 123”    
here i have to verify to a third party that this domain (kplabs.in) belomngs to me. 3rd party allows me to store some values in subdomain of my domain.
 it will give value" 3rd party verif 123"

 

ADVANCED ROUTE53 CONFIGURATIONS : 
Generally, a managed DNS server supports basic functionality like :
- Domain Registration
- GUI for putting DNS records
- Mapping & Resolving various DNS Records.
- WHO IS Management

 Route53 does a lot more

- Support of Public and Private Hosted Zones.
- Routing policies - Weighted, Latency, Geolocation, Round Robin
- Health Checks & Monitoring
- Route53 Endpoints
- DNS Firewall 


Route53 Health Checks : Amazon Route 53 health checks monitor the health and performance of your web applications, web servers, and other resources
- we have a route 53 and we have endpoint( ec2 or server) . with the help of health check route 53 will send a request to server and it expect the response
  from it. depending upon the respnse and configuration time that we have configured , route 53 consider this endpoint to be healthy or unhealthy. if it is
  unhealthy then route53 can also send alarms to us if req.

demo : 3 route53 health checks that are configured for 3 diff websites. 1st website is healthy which website is working fine. 2nd one is unhealthy.if we 
  open the 2nd website it is loading, but if we inspect there we can seee 403 forbidden eror that's why it is unhealthy. 3rd website is also unhealthy if 
  we open it doesn't load.

Route 53 has health checkers in locations around the world.
- When you create a health check in route53 that monitors an endpoint, health checkers start to send requests to the endpoint that you specify to determine
 whether the endpoint is healthy.
- the health checkers we can consider this to be a small servers that are distributed across the world. route53 uses the servers to send the request to the
  endpoint and look into whether the response is coming or not.depending upon the response the target is considered as healthy or unhealthy.

demo : go to route53 console and click on create health check give the name and select the endpoint that we want to monitor(endpoint or cloudwatch alarm).
 select endpoint and create a ec2 server.and we will be giving the ip address of server. monitor an endpoint : we can specify the endpoint by ip address 
 or domain name , we can also select http or https. also create  an elastic ip address. we can select the request interval, failure threshhold,we can 
 select heslth checkers for required regions. 

login to ec2 instance and install nginx website.manage sg for all. copy ip address of ec2 and paste it in
 endpoint section(in route53 create health check portal of ec2 endpoint) and press create health check.currently health check of ec2 is unknowsn.
- here health checkers will start requesting to endpoint. login to ec2 and run " cd /var/log "  after that run " cd nginx". inside that to monitor the acce
  ss logs run " tail -f access.log" . here we can see multiple requests are coming from the health checkers.among them we can see the response code is 200
  means destination client was able to successfully fetch 

ROUTING POLICIES IN ROUTE53 : 
Routing Policies determine how Amazon Route53 responds to the queries. There are various supported routing policies available in Route53.
Each policy supports a specific use case.

- Simple : In simple routing, there is a plain one-to-one mapping between domain and host.
      ex : blog.kplabs.internal  A   128.199.241.125

- Weighted : it helps us to route the traffic to multiple resources in a proportion that we specify from our end. ex : we have route53 and 2 ec
  2 instances . we specified 50-50 routing. means 50% traffic goes to one ec2, 50% goes to another ec2.

- Latency : If your application is hosted in multiple AWS regions, we can improve the performance for the users by serving their requests from the AWS 
  region that provides the lowest latency. ex : we have multiple servers across regions.it can happen that 1st ec2 has higher latency , 2nd ec2 which is 
  different part of world has low latency . latency based routing will divert the fraffic to server which has low latency.

- Failover : Failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is 
  unhealthy. for ex route 53 is routing policy to ec2. if ec2 goes down then with the help of failover routing route53 can goe ahead and send the traffic 
  to diff resource.

Q : A company has created a disaster recovery solution for an application that runs behind an Application Load Balancer (ALB). The DR solution consists of
   a second copy of the application running behind a second ALB in another Region. The Solutions Architect requires a method of automatically updating the
   DNS record to point to the ALB in the second Region : Amazon Route 53 health checks monitor the health and performance of your web applications, web
   servers, and other resources.

  - Health checks can be used with other configurations such as a failover routing policy.In this case a failover routing policy will direct traffic to the
   ALB of the primary Region unless health checks fail at which time it will direct traffic to the secondary record for the DR ALB.

Q : The failover routing policy is used for active/passive configurations. Alias records can be used to map the domain apex (example.com) to the Elastic 
    Load Balancers.

  - Failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy. The
  primary and secondary records can route traffic to anything from an Amazon S3 bucket that is configured as a website to a complex tree of records.



- Geolocation : this routing allows us to choose different resources for different users based on different countries/continents
  ex : we have 2 users in ind and usa. based on the geolocation routing rules route53 can automatically route the traffic to diff resources. like ind user
  traffic will divert to sing server and usa user traffic divert to oregon region.


Q : A company hosts an application on Amazon EC2 instances behind Application Load Balancers in several AWS Regions. Distribution rights for the content
   require that users in different geographies must be served content from specific regions :  Amazon Route 53 records with a geolocation routing policy

  - To protect the distribution rights of the content and ensure that users are directed to the appropriate AWS Region based on the location of the user,
  the geolocation routing policy can be used with Amazon Route 53.

  - Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS
  queries originate from.

  - When you use geolocation routing, you can localize your content and present some or all of your website in the language of your users. You can also use
  geolocation routing to restrict distribution of content to only the locations in which you have distribution rights.



- Multi-value answer routing : it allows us to return multiple values ( such as ip addresse) in response to a dns query.
  it also allows us to check the health of the resources so that route53 responds with details of only healthy resources.route53 can respond upto dns query
  with upto 8 healthy records.

demo : if i do nslookup demo.kpl.internal, it will show 4 records with ip addresses(10.77.88.(1,2,3,4) 4 ip addresses).
where we apply routing policies : once we have HZ in route53 while creating a record under routing policy we can see multiple types. 

 same in route53 : if we open HZ of kplabs.internal we can see 4 recordsof (demo.kplabs.internal) are same but each of 4 has diff ip addresses . here anyti
 dns server requests for the domain of demo.kplabs.internal multiple servers ip addresses would be sent in the dns response.

* this type of helps when we have lot of webservers if we want  to diustrbute the traffic we can make use of multivalue routing , this is not a replacemnt
  to ELB. 
* we have 4 webservers in each of record.if one of the wbeserver is not working any time request comes from the domain of mydemo.kplabs.internal , then the
  route53 should only serve ip addresses associated with the webserves whose health check has been passed, and not serve the ip address which has failed.

GEO PROXIMITY ROUTING POLICY ( route traffic based on physical distance) : this routing lets amazon route53 route traffic based on the physical distance 
  b/w resources and users.

 ex : we have a resource created in regions 1,2,3,4,5. they are not aws regions but physical regions.like any user coming from region 4 will be directed 
  towards resource in region 4.any user coming from region 3 will be directed towards region 3.

1. Active-passive: Route 53 actively returns a primary resource. In case of failure, Route 53 returns the backup resource. Configured using a failover 
   policy.
2. Active-active: Route 53 actively returns more than one resource. In case of failure, Route 53 fails back to the healthy resource. Configured using any
   routing policy besides failover.
3. Combination: Multiple routing policies (such as latency-based, weighted, etc.) are combined into a tree to configure more complex DNS failover.


***********************************************************************************************************************************************************


UNDERSTANDING HTTPS CONNECTION (SECURE COMMUNICATION) : 

- Https is Extension of HTTP.
- In HTTPS,the communication is encrypted using transport layer security. TLS is newer version of SSL.
- The protocol is also referred to as HTTP over TLS or HTTP over SSL.
* many times when we open website in browser sites like banking,or sites which contain sensitive info there we will see with green colour lock and https.
  the lock means communication is secured.

Scenarios where HTTPS is useful :

Scenario 1 : MITM(Man in the middle) Attacks : a user is sending their username and password in plaintext to a webserver for authentication over a network.
- i have a webserver(any appl) , and user. user open the website in browser and the website ask you for login and pswrd.we put username and pswrd.


- if there is an attacker who is sitting b/w them doing a MITM attack and storing all the credentials he finds over the network to a file. means 
  attacker is sniffing the network then he will be able to fetch all the crednetials which are passing this network within plaintext. he can store within 
  his databse. the longer he sniff the network the more usernames and pswrds he will be able to fetch.
- the reason he able to see the credentials is they are in plain text.THERE ARE VARIOUS PLAIN TEXT PROTOCOLS LIKE FTP,HTTP. if we use plain text protocol
  to send sensitive data , if there is mitm attack then credentials would be leaked.

Scenario 2 : MITM and Integrity Attacks : Attacker changing the payment details while packets are in transit.
- a user sending the info that send 100 rs to zeal.here attacker can modify the details.first he recieve that info and he modifies it to send 100 to attack
  er. now destination server will recievs the details and it sends the 100 to attack and it will be successful.

** to avoid the priveous 2 scenarios( and many more) , various cryptographic standards were clubbed together to establish a secure communication over an 
   untrusted network and they are known as SSL/TLS. SSL was the initial version which was basically intended for secure communication.ssl 2.0,3.0,TLS 1.0,
   1.2,1.3 various protocols which are used in ssl3.0 could be easily compromised.if we are using secure communication with ssl 3.0 were deemed to be unsaf

Understand HTTPS in an easy way : Every website has a certificate(like passport which is issued by a trusted entity) passport generally gets issued by gov
 before passport gets issued whatever contents are there within that passport it might be name,DOB,address that contents will be verified by the gov.
 similiar way certificate is like a passport it contains lot of information and that certificate is issued by a trusted entity.
- this certificate contains lot of details like domain name(zeal.com)  it is valid for, the public key, validity and others.along with that it has version
  serial number, certificate, signature algorithm,issuer,validity, public key info.

Generally when we open https website the webserver will send us its certificate. now the browser(clients) it verifies if it trusts the certificate issuer.
 if it trusts it will verify all the details of certificate. like when we gpo international there is a channel which will verify evrything. once it verify
 it accepts it bo genuine. then it will take the public key info from certificate it also initiates the negotiation.we know that tls is used for secure 
 communication.in order for secure communication it takes the public key info. Assymmetric key encryption is used to generte a new temporary symmetric key
 which will be used for secure communication. here it takes the public key info a secure pswrd is generated.that secured pawrd which is referred to as symm
 etric key will be used for encrypted and decrypted relation throughout the channel.

- if we open any website that website certificate is verified by any certified entities like comado. 

AWS CERTIFACTE MANAGER : 
earlier : i have a websote and i need to use HTTPS. there are two ways self signed certificte and CA signed certificate.
 when we open self signed certificate it usually shows the site's security certificate is not trusted.it's like creating our own certificate. when we open
 ca certificate it will show trusted.

how to generate a certified which is trusted by a browser , to certified we have to go to a certified authority.which is again certified by browser.
let's say i have a certified authority and it is trusted by browser and browser trust the sites that are issued by certified authority.
 as a user we have to ask certificate authority to generate a certificate for our domain(kplabs.in) . we have to make payment and complete the relevant for
 malities . after formalities the CA will issue the certificate with specific validity(1 year) and private key.all of these we will be adding at the part 
 of our website configuration. then we will have a https certificate. 


- here problem is when validity completes automativally CA won't supportand websites shows it is un secured.
- generally now a days org based on aws there is a new better way in which the certificates are issued and providioned which is through CERTIFICATE MANAGER

** AWS CERTIFICATE MANAGER(ACM) : it alloews us to create certificates which we can use in our websoites and applications.instead of going directly to a CA
   we can get authority though the ACM and we can tell ACM to generate the certificate of our domain(kplabs.in),private key.how ever these things are not
   given to us. the certificate is generated then we can say integfrate the certificate with aws service to which the traffic is coming from like ELB.ww wi
   ll have a https website 
demo : if we open aws acm we can see one certificate is issued for asc.zeal.com. in use : use ; renewal eligibility : eligible we can also associated resou
   rces is ELB.if we look into ELB console , all the traffic comes to ELB , if we look into listeners we have one HTTPS listener of 443 	along with that 
   we can see view/edit certificates of acm.

* if we get certificate and private key from CA any one who has acces to it or any others can decrypt it. but ACM doesn't give us certificate and key it is
  safe to not get in our hands.

Q : 
.A company uses an Amazon RDS MySQL database instance to store customer order data. The security team have requested that SSL/TLS encryption in transit 
   must be used for encrypting connections to the database from application servers. The data in the database is currently encrypted at rest using an AWS
   KMS key.How can a Solutions Architect enable encryption in transit : Download the AWS-provided root certificates. Use the certificates when connecting
   to the RDS DB instance

- Amazon RDS creates an SSL certificate and installs the certificate on the DB instance when Amazon RDS provisions the instance. These certificates are 
  signed by a certificate authority. The SSL certificate includes the DB instance endpoint as the Common Name (CN) for the SSL certificate to guard 
  against spoofing attacks.

  You can download a root certificate from AWS that works for all Regions or you can download Region-specific intermediate certificates.



AMAZON WORKMAIL : 
challenges with managing mail server : configuring a mail server for entire org is challenging risk.
 various aspects related to mail server configuration,Spam detection, security, HA comes into play.
- AMAZON WORKMAIL IS A SECURE AND MANAGED BUSINESS EMAIL SERVICE. this is very similar to our gmail account.we can have our own workmail account.we can 
  recieve and send emails.




***********************************************************************************************************************************************************

CRYPTOGRAPHY & SECURITY 

***********************************************************************************************************************************************************



INTRODUCTION TO CRYPTOGRAPHY ( time to secret out) : 
symmetric encryption : path : knowledge -  encryption - Fskjdnhdalijd - decryption - knowledge
- at encryption we run a algorithm and we feed it in a secret key and we have a result as cypher text data.on the other side we encryption algorithm we do
  decryption by passing the secret key again. and we have a result in plain text data.

Why encryption ? we have a computer and gmail server. in b/w one hacker.when we go to gmail login form it will asks us emailid and passwrord we give them,
  if attacker is sniffing into network and he will be able to get our email address and paswrd in plain text. now after encryption pswrd is encrypted even 
  attacker is sniffing the network he will find the pswrd is not in plain text.

- Now a days in most of the login forms almost all the banks and big org they use HTTPS . S means secure. secure means Encryption.

UNDERSTANDING COMMUNICATION PROTOCOLS : 
use case of protocol : in order for a communication to happen b/w two entities a common language is very imp.that's why humans has languages to communicate
  i speak eng and u must be speak english then only we can understand our communicate each other.when 2 persons don't spoeak a common lang then ideally the
  communication or verbal commun is very diff. and this is same for computers.if 2 computers want to speak to each other they they need to speak comon lang

  if comp 1 speaks diff lang and comp 2 speak diff lang then the communication wouldn't happen. this is why we have a standard protocols(languages),we have
  set of protocols(languages) which are used to inter communicate.any computers which want to communicate b/w each other has to follow a certain protocol.

There are various protocols actively used :
- File Transfer Protocol (FTP)    : will send data natively in plain text. attacker sniffing the network can look into plain data.
- Domain Name System Protocol ( DNS
- Transmission Control Protocol (TCP) : it has it's own lang.
- Secure File Transfer Protocol (SFTP : using sftp everything will be in a encrypted format. even attacker is sniffing traffic he can't understand data.
- Hyper Text Transfer Protocol (HTTP
- Internet Protocol (IP)
* each protocol has its own set of use case.

TCP 3 way handshake : before a communication takes place there is a 3 way handshake. since sinact and act. this 3 way occurs before any communication occur

PLAIN TEXT AND ENCRYPTED PROTOCOLS : ex : FTP AND NFTP :
- in cmd open wireshark(great network sniffing tool)

UNDERSTANDING THE DISK LEVEL ENCRYPTION SCHEMAS : 
- we have seen encrypting individual documents with secret key and decrypting it with secret key again. the problem is it is limited to a individual file.
- what happen we have 1000 files.within a server.we can't encrypt each one in server and decrypt it.

- i have a laptop inside that so many files. i can't encrypt each one.
* in the situations where we have lot of files we can go with the disk level encryption. in disk level encryption encrytping each file we encrypt the entir
  files reside. whenever we start laptop we have to put decrypt pswrd to decrypt entire hdd drive.

AWS CLOUD HSM : (SECURE STORAGE) : 
use case : storing expensive household items : we have expensive jevellary stored in house and are planning to go on a long vacation.
  where will you store jevellary? in the cupboard in house or banlk locker which is secure : A. bank locker beacuse it is secured.

Storing sensitive Digital Keys : we have have sensitive encryption keys that needs to be stored
Where will you prefer to store the keys(encryption keys,ssl,tls keys)? in notepad or Special security devices which or means for storing sensitive data

A. Special security devicses. But in many org sensitive keys are stored in notepad,gmails, because to store keys in security devices we have to configure 
   them also.that's why users prefer notepad etc.  ; however when we go towards the organizations that are much more security oriented even for the org 
   that are following security compliances there is a req  where sensitive and secret data must be stored in security devices.

* Special Security Device - HSM : A HARDWARE SECURITY MODEL(HSM) is a physical device that provides extra security for sensitive data.
- This type of device is used to provision cryptographic keys for critical functions such as encryption, decryption and authentication for the use of 
  applications, identities and databases.

TAMPER RESISTANT : These devices are tamper resistant , that means if anyone tries to tamper, they will automatically delete the keys stored.
- org now migrating to cloud. they have these physical devices in the on premises along with their servers.after few months they are migrating to cloud.
  now how will they move the sensitive device like hsm to cloud, it is not feasible,possible. they will have a connectivity b/w on premises and cloud.

  where the applications that are running in aws , where the appl that are running in aws will connect to the HSM running in the aws location. and the
  transfer happens. but some disadv are there relted to newteork. incase if network is slow the application will stall.in order to overcome this aws has
  one service AWS cloudhsm

AWS CLOUDHSM : AWS CloudHSM is a cloud-based hardware security module (HSM)
- incase if we are migrating to cloud , we don't have to worry about physical hsm which is present in on premises.we can simply use cloude HSM
- we have aws cloud hsm. some appl running inside a vpc.appl can connect to cloud hsm easily for certain oprns.
- With CloudHSM, you can manage your own encryption keys using FIPS 140-2 Level 3 validated HSMs.it is not some random hsm. aws uses very secure and compli
  ance hsm.

AWS KEY MANAGEMENT SERVIVCE(KMS - DO THINGS THE RIGHT WAY) :
- AWS KMS stands for AWS Key Management Service.
- This service provides capability to encrypt and decrypt the data.
 we have a user right side, kms on left.user has a sensitive data " pswrd = 1234 " . user sends that pswrd to kms , kms will encrypt it and send that encry
 pted data back to the user. here instead of storing or sending sensitive data over a communication channels like slack, email others, the user can send en
 crypted data over communications channels.

demo : go to kms console.go to customer managed keys, we can see keys one of them demo key. we can use demo key to perform encryption or decryption oprns.
 to make use of KMS we don't have GUI way, we have to do it using CLI only.

in cli : run command aws kms encrypt key-id - (give key id ) - file(location) . if we have data"123" in a txt file. after running the command we can get
 o/p in cypher text. if we send that cypher text to any friend , he can use that cypher text in kms to decrypt it unless if he has right permissions.
frnd runs " aws kms decrypt --cyphertext -blob (here give cypher text ". we will get o/p as plain text = " some base64 encoded data". here again we need
 a tool that can go ahead and decrypt the data.

INTEGRATION OF KMS : AWS KMS also integrates with various AWS services like S3, DynamoDB, EBS and others.
- big org has so many files.should they need to supply each file individually to KMS and get the encrypted contents? No. Most of the orgs they store sensit
  ve files in s3 or EBS. KMS supports integration with s3,EBS. So KMS can perform encryption or decryption in s3 or ebs directly.


S3 ENCRYPTION : 
Use case : we have a hdd which is portable. during travel we lost it. it has lot of sensitive data.this is risk, because if we store data in plain text in
  hdd it is very risky.That's why companies like Western Digital HDD's come up with hardwarebased encryption. here we can lock the data using h/w encrypt
  even if we lost it any one can't fetch the data.

- S3 also needs encryption because it is a storage service.
- AWS S3 offers multiple approaches to encrypt the data being stored in S3.

i) Server Side Encryption
● Request Amazon S3 to encrypt your object before saving it on disks(behind scene)in its data centers and then decrypt it when you download the objects.  
- Within Server-Side encryption, there are three options that can be used depending on the use-case.

  1.Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)
     - In this approach, each object is encrypted with a unique key. ; if we upload 10 objects to s3,allof that objects be encrypted with unique key(10keys
     - it Uses one of the strongest block ciphers to encrypt the data, AES 256.

  2.Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS)
     - Encrypting data with own CMK allows customers to create, rotate, disable customer managed CMK’s. We can also define access controls on cmk's and 
       enable auditing.
     * we can create our own cmk in KMS , and that cmk can be used to encrypt or decrypt data in s3.

  3.Server-Side Encryption with Customer-Provided Keys (SSE-C) : 
     - Allows customers to set their own encryption keys.
     - Encryption key needs to be provided as part of the request and S3 will manage both the encryption as well as the decryption options.
   ex : we have a plain data and we want to encrypt it in s3 before it is stored. here along with the data we also provide encryption key.key is provided
        by customer.s3 will recieve both key and data. now s3 will encrypt the data using provided key. whenever we want to decrypt the data we again have 
        to send the decrypt operation with the same key(symmetric key).

ii) Client Side Encryption
● Encrypt data client-side and upload the encrypted data to Amazon S3.in this case,you manage the encryption process,the encryption keys,related tools.
- Client-side encryption is the act of encrypting data before sending it to Amazon S3.
- we can encrypt data at client side.and send the encrypted data to s3.here before data goes to s3 it is already in encrypted manner. it is diff from sse-c
  cutomer provided key, because in sse-c we give plain data with key. but now we are giving encrypted data without key.


Q :

- The images must be encrypted at rest in Amazon S3. The company does not want to spend time managing and rotating the keys, but it does want to control 
   who can access those keys.

  SSE-KMS requires that AWS manage the data key but you manage the customer master key (CMK) in AWS KMS. You can choose a customer managed CMK or the AWS
  managed CMK for Amazon S3 in your account.

  Customer managed CMKs are CMKs in your AWS account that you create, own, and manage. You have full control over these CMKs, including establishing and 
  maintaining their key policies, IAM policies, and grants, enabling and disabling them, rotating their cryptographic material, adding tags, creating 
  aliases that refer to the CMK, and scheduling the CMKs for deletion.

  For this scenario, the solutions architect should use SSE-KMS with a customer managed CMK. That way KMS will manage the data key but the company can
  configure key policies defining who can access the keys.

- The solution requires that data keys are encrypted using envelope protection before they are written to disk:AWS KMS API

  When you encrypt your data, your data is protected, but you have to protect your encryption key. One strategy is to encrypt it. Envelope encryption is 
  the practice of encrypting plaintext data with a encryption key(data key), and then encrypting the data key under another key
 
- As the data is stored both in the EBS volumes (temporarily) and the RDS database, both the EBS and RDS volumes must be encrypted at rest. This can be
  achieved by enabling encryption at creation time of the volume and AWS KMS keys can be used to encrypt the data

- The data must be encrypted at rest on both the EC2 instance’s attached EBS volumes and the RDS database. Both storage locations can be encrypted using
  AWS KMS keys. With RDS, KMS uses a customer master key (CMK) to encrypt the DB instance, all logs, backups, and snapshots.


AWS GUARD-DUTY(let's start rolling) : 
- AWS Guard Duty is a threat intelligence service by AWS which monitors for malicious behavior to help customers protect their AWS workloads. 
 above 2 imp words : Threat and intelligence : threat is not just associated with servers,it also associated with humans. anything that can be harmful to 
 particular entity can be regarded as threat. Intelligence is the way in which we can percieve a specific threat. Now guard duty: 

- we have a guard duty service. it basically monitors for multiple log files(cloudtrail events, vpc flow logs, dns logs). once guardduty monitors all of 
  them it will go and analyze it according to the rules that has been configured.and from those it can detect threat.threat can be bitcoin detected,dns
  exfiltration,SSH bruteforce.Now a days hackers do bitcoin mining.if we are able to detect bitcoin mining in our org servers chances are that servers are
  breached. DNS exfiltration( data of dns servers sent via dns packets) 

GuardDuty Findings : ex's :
- unauthorized access ; phishing ; bitcoinrelated alerts ; ssh bruteforce : guardduty alerts
GuardDuty categorizes alerts into multiple categories(critical,imp,informational) . it also shows in which resource threat has detected.
GuardDuty Finding examples
- root credentialusage
- portprobeunprotecyted : if we click on this finding ,it will show more related details like Ip address,actiontype, country,resource id,region
- ec2TorchCliet
* IMP POINTS : 
  Guard Duty will only monitor the Route53 for DNS Logs.
  Lot of organizations makes use of Active Directory DNS. The logs from these servers will not be monitored. those org's uses splunk which has threat intlg

- also guard duty doesn't take into account of all logs. it will take cloudtrail events, vpc flow logs, dns logs into account, but not ec2 logs.
* eventhough guardduty doesn't monitor all log files. but the log files it monitors it does give a lot of insights into potential threats.

Q :

- An application is being monitored using Amazon GuardDuty. A Solutions Architect needs to be notified by email of medium to high severity events. How 
   can this be achieved :  Create an Amazon CloudWatch events rule that triggers an Amazon SNS topic

   A CloudWatch Events rule can be used to set up automatic email notifications for Medium to High Severity findings to the email address of your choice.
   You simply create an Amazon SNS topic and then associate it with an Amazon CloudWatch events rule.

- In order to receive notifications about GuardDuty findings based on CloudWatch Events, you must create a CloudWatch Events rule and a target for 
  GuardDuty. This rule enables CloudWatch to send events for all findings that GuardDuty generates to the target that is specified in the rule.
 
- A systems administrator of a company wants to detect and remediate the compromise of services such as Amazon EC2 instances and Amazon S3 buckets.Which 
   AWS service can the administrator use to protect the company against attacks : Amazon GuardDuty	

  Amazon GuardDuty gives you access to built-in detection techniques that are developed and optimized for the cloud.The detection algorithms are maintained
  and continuously improved upon by AWS Security. The primary detection categories include reconnaissance, instance compromise, account compromise, and
  bucket compromise.

  Amazon GuardDuty offers HTTPS APIs, CLI tools, and Amazon CloudWatch Events to support automated security responses to security findings. For example, 
  you can automate the response workflow by using CloudWatch Events as an event source to trigger an AWS Lambda function.



REFERENCING SECURITY GROUPS( RIGHT WAY ) : 
- In Security Group rules, along with IP addresses, we can also refer to the security group of the destination EC2 instances.
 means most of the org they do , is they create a rule based on ip address,however in sg we can also create a rule based on the security group id of the 
 destination ec2 instances.This is a good practice specifically when using EC2 instances in auto-scaling group.

ex : we have 2 ec2's under web server ASG tier. on the right side 4 ec2's part of  appl tier in another ASG. the 2 web servers need to communicate with 
 4 appl servers. here we have added 2 ip addresses of each ec2's,within the security group associated with the appl.now in future since web server is part
 of ASG, let's assume that a new server is created because of higher load, so the 3rd webserver ip(new) address will not be able to communicate to the appl
  tier.

- so whenever we are referencing via ip addresses is not best way. to avoid this most org allow the whole VPC cidr block within ASG, although this works 
  pretty well even with the dynamic scaling,but this is not best approach.

Best SOLN : 

Create two security groups: 1st for Web Servers and 2nd for Application Servers.
Let’s assume following are the security group id’s assigned:
i)  Web Servers: sg-web ( this sg is common for the webserver tier)
ii)  App Servers: sg-appl ( ( this sg is common for all ec2's under the app tier)

SG rule          role                  allowed rule 

 1           WEb-instances             outbond port 8080 to sg-appl
	
 2           appl-instances            inbound port 8080 from sg-web.

rule 1 : here we are considering that the appl is listening on port 8080. since the web ec2's will have to forward the traffic to the appl tier ec2's we
         add a outbound security rule, which allows port 8080 to all the instances which has sg id of "sg-app" 
rule 2 : here for app ec2s we create a inbound sg which allows the traffic coming to port 8080 from the instances which has sg id "sg-web".

DEMO : i have one ec2 and one rds db.ec2 wants to communicate with rds. now withing rds sg inside that within inbound rule " we can see currently it is all
       owing all the traffic from source" ec2-id". means if there are 10 ec2's where this sg is associated with all the 10 ec2's will be able to communicat
       with each other.[ from start go to rds sg : edit inbound rule : port range 3306,cidr would be the private ip of ec2, since both are within VPC and 
       save. npow if we add on emore ec2, it won't work ] 
- that's why instead of giving ip address in inbound rule better to give security group id. Means any ec2 that has this sg assigned to it will be able to 
  communicate to the rds db on port 3306.

Referencing SG Across VPC Peering : 
Security Group can be referenced across VPC Peering connections as well. 
If there are two VPC’s in a peering connection in the same region, then we can reference by security group id’s from the peer VPC.

EX : i have 2 ec2's(ec21,ec22) belongs to default and custom vpc. now i want ec21 to communicate with ec22 via the icmp protocol.allow the cidr of vpc2 
     within the firewall rule.
- logged into ec21.try to ping to ec22.we can't. now check sg associated with ec22. within ec22 sg we have one rule, no icmp rule which allows the ping 
  request.in ec22 we can create a icmp riule, and specify the cidr of ec21.now we can ping ec22 from ec21. now in a similar way instead of cidr we can 
  give sg-id of the ec21 to ec22 sg.
This will work for vpc's vpc peering connection which is part of same region only.

Cross-Account SG Reference
- To reference a security group in another AWS account, include the account number in Source or Destination field
 format : other-aws-account-id/account-security-group-id ; Example Rule: 123456789012/sg-1a2b3c4d
- You cannot reference the security group of a peer VPC that's in a different region. Instead, use the CIDR block of the peer VPC.




Vulnerability, Exploit, Payload ( Ethical Hacking Terminology ) : 
EX : one house and one guy(robber). robber wants to enter the house. he tried to enter house through all entries, but he found them locked. finally he foun
     d a hole in the side of house through which he can easily gain entry. the hole is reffred to as vulnerability 
- Vulnerability :- Hole on the Side of the House
- Exploit :- The Robber 
- Payload :- What Robber does inside the house ( once robber enters house he can steal something) 
           what rober wants to do once he enters the house is payload. ex : once the computer gets infected by the virus,diff types of virus does diff thin
           some virus will encrypt all data, some virus will generate popups.

Security Terminology
Vulnerability :- Bad S/w Code : we have one appl.we haven't considered security. so the appl has lot of loop holes.since the appl is running inside server,
                 if the appl has loop hole, then the attacker can gain access to the server via appl.
Exploit :- Program that exploits code to get inside : appl can have bad code.but the program that attacker has written through which the attacker can take
           adv of bad appl is considered as exploit.
Payload :- Stealing Data, Ransomwares etc.

vulnerability scanner : it usualy scans the appl or house through which attacker can get inside and shows results. if we discover any vulnerability we have
  to do patch management( where we patch all the velnerabilities means covering holes) 

AWS INSPECTOR (VULNERABILITY SCANNER) : 
- AWS Inspector is similar to a vulnerability scanner which will scan the system for specific vulnerabilities and provide the results.
- It relies on the agent installed on the server to scan the server.
  we have inspector.through inspector we will scan ec2 instance or vm. based on the pckgs installed a velnerability related results will be displayed to u.

  for the inspector to know what are the pckges, or versions that are installed on ec2 , it needs an agent.ssm agent need to be install in ec2. inspector
  service will coordinate with ssm agent,and we will be displayed with the vulnerability results.

- Amazon Inspector gives you the flexibility to enable either EC2 scanning or ECR container image scanning, or both. 
  if we have docker images uploaded in the ecr,inspector can scan that as well.

Q :

- Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS
- Inspector is more about identifying vulnerabilities and evaluating against security best practices. It does not detect compromise.


AMAZON MACIE ( MACHINE LEARNING BASED SECURITY ) : 
- now a days many org shifting from on premises storage to cloud service storages like s3. org's stores petabytes of data in s3. how will we know if anyone
  uploads credentials(security key) in a text file in s3 which is unsecure, how will we identify it ? . this is where macie comes.

- The core feature of macie is to go ahead and search or look for the sensitive(PII dat,SSL keys) data that is stored in s3. along with that aws macie will
  give various finding related to policies,bukcet policies.

* S3 might contain sensitive information like PII data, database backups, SSL private keys and various others.
 Amazon Macie makes use of machine learning to identify sensitive data stored in AWS.

demo : when we open macie, it will shows the buckets which has public accessable,readable access, or not publicly accessable
       macie also shows whether encryption has been used or sharing has been happening.
       many org's use macie to look for sensitive data like ssh keys, aws key or any other credentials.,
       if macie doesn't has the defaul finding identifier, then we can create our own identifier(caught if any s3 bucket keys)

go to aws macie and enalbe it.
we have 2 txt files. file 1 contains credentails or keys [ kadhdfuslpfouafsfol , bccndgfvjyatt8499jdjdj ]
                     file  contains same credentails or keys with names [ aws_access_key : kadhdfuslpfouafsfol , aws_secret_key : bccndgfvjyatt8499jdjdj]
we upload both files to s3 bucket. go to macie console and scan the objects in s3 only once. we can't scan multiple times because costs much cost.
create a job in macie and wait till become active. after making macie active we can see sensitive recording types as findings '1'. macie detecyted only 
one file which is 'file-2'.it couldn't detect file 1.


AMAZON DETECTIVE( THREAT DETECTING SERVICE ) : Amazon Detective makes it easy to investigate, analyze, and quickly identify the root cause of potential 
 security issues or suspicious activities.

- detective usually collect various log sources like cloudtrail,vpc flow logs, it even support guard duty. it takes the logs,and perform some analysis,
  it gives a greate set of info that we can analyze from amazon detective service.


demo : in guarduty i have a alert related to root credentails.if i open that alert it will show all details(ip address, country). now we want to analyge
  in much more way.we can just copy ip address. from detective service specify the ip address we copied and search it. we can seethat ip. open it and withi
  the scope time give 30days. we can see total times observed"300". and count of related user agents"50". it also shows what arethe findings associated wit
  that ip( root credentials are associated".).

it also shows all api volume9succefull,failed cells).if we click on any faiked cells,) we want to see which failed cell from this ip address.if we go down
it shows multiple users saying ip address, has been used by 2 users (alice,demolabs).if we click on alice, we can see that alice is trying to perform descr
ibe s3 and rds and they are failed(call).
we can also sort by services as list of successful or failed calls.


AWS Network Firewall (Another firewall) : 
- AWS Network Firewall is a stateful, managed, network firewall and intrusion detection and prevention service for your virtual private cloud (VPC).

-  2 subnets(public,private) part of vpc. right side one internet gateway for the resources to connect to the internet.b/w both network firewall is there
 what are the things that we can do at network firewall level? intrusion detection and prevention service: like

we can add a set of rules like domain based filetring( block facebook.com , allow twitter.com means any traffic going towards facebook action is blocked,
traffic going towards twitter action is allowed) ,CIDR based filtering(towards any ip addtess range 10.77.0.0/16 action is allowed).

DEMO : go to vpc console,we can see network firewall. click on network firewall.we can see one already with name. if we click on that we can see associated
       vpc and it's subnet (where firewall endpoint lies)
again in vpc console we can see network firewall rule groups. if we click on it we can see domain filtering and ip address filtering

You can use Network Firewall to monitor and protect your Amazon VPC traffic in a number of ways, including the following:
1. Pass traffic through only from known AWS service domains or IP address endpoints, such as Amazon S3.
2. Use custom lists of known bad domains to limit the types of domain names that your applications can access (block unsecure websites)
3. Perform deep packet inspection on traffic entering or leaving your VPC

Q: The AWS Network Firewall is a managed service that makes it easy to deploy essential network protections for all your Amazon Virtual Private Clouds, and
   you can then use domain list rules to block HTTP or HTTPS traffic to domains identified as low-reputation, or that are known or suspected to be
   associated with malware or botnets.

AWS CONFIG ( overview of infrastructure changes ) :  it works at regional level.
- AWS Config is primarily used to record the resource configuration changes over time.

ex :in week 1 we have a specific infrastructure(set of ec2's). in week 2 i have added a lb that integrates with ec2's.in week 3 i have setup rds,s3. aws 
    config allows us to go ahead and record the resource configuration changes that are been made over time.

ex :An EC2 instance was hosting website from past 90 days. Suddenly in last one week, there have been a lot of issues with the requests. What was changed?
    if we have aws config, at infra level we can easily identify what are the configuration changes that are been made to the resources.

AWS config is useful for AUDIT and COMPLIANCE : AWS Config comes with large set of rules that can continuously monitor your AWS env and report the findings
 ex :i have a rule of approvedamisbyid. any time a ec2 instance is launched if that ec2 is not launched from a specific ami id's, then with aws config 
     audit  and compliance rule we can easily identify that resource(it wills show one noncompliant resource).
demo : 
- open config.we can see list of noncomplaint rules by noncompliant resource.
- in aws config we can use managed rules or custom made rules. [ access-keys-rotated,]
- in config we can see timeline of any resource. timeline contains main configuration on the right side, and customized configuration on the left side.
- lke a sg(previous) is given by use earlier, later another sg(customized) is added to that resource.
- earlier type is t2.micro, later it become t2.nano.

A conformance pack : is a collection of AWS Config rules and remediation actions that can be easily deployed.
 - if we got config and add rule.while adding rules we can see many rules we can add. it is diff to add or understand each rule.instead of that we can see 
   confirmation packs,like choosing rule packs based on service.

- You pay $0.003 per configuration item recorded in your AWS account per AWS Region. A configuration item is recorded whenever a resource undergoes a 
  configuration change or a relationship change.
- Based on rule evaluation. A rule evaluation is recorded every time a resource is evaluated for compliance against an AWS Config rule.
- You are charged per conformance pack evaluation in your AWS account per AWS Region based on the tier below. 


FIREWALL MANAGER (CENTRALLY MANAGE RULES) : 
- Most of the organizations are opting for Multi-Account based strategy for separation of environments (dev, stage, prod)
- Security Team needs to create, maintain and update security services across all of the accounts

ex :  challenge we have 3 seperate accnts: dev,stage,prod.we nee dto maintain a seperate set of services,and its associated rules.like WAF need to be in 
      all acnts,network firewall, dns firewall all these might need to be in all acnts.
like in dev waf is configured. now tmrw devlpr wants to configure a new set of appl that appl might not have waf associated with it.then security might 
again need to integrate waf with his appl. to overcome this aws has released firewall manager.

AWS Firewall Manager : is a security management service which allows you to centrally configure and manage firewall rules across your accounts and
  applications in AWS Organizations
ex : i have 3 aws accounts. instead of manually logging in each account and applying the appropriate rules associated with waf,network firewall, we can mak
     use of FIREWALL MANAGER. WHere we centrally configure the rules,and these rules will be centrally common across all acnts.let's say one of the common
     servivce that is used across 3 acnts is ELB. so at afirewall manager level we can set the appropriate waf rules.so now all the lb's will be associate
     with the common set of waf rules, that we apply at a firewall manager level.

Firewall Manager supports wide variety of services

 AWS WAF
 VPC Security Groups
 AWS Network Firewall
 Route53 DNS Firewall
 AWS Shield Advanced
 Palo Alto Cloud Next-generation firewalls

* Since firewall manages at a account level there are certain prerequesities: aws organizations and aws config is pre-req.

Firewall manger benefits :
- Simplify management of firewall rules across your accounts
- Ensure compliance of existing and new applications
- Easily deploy managed rules across accounts
- Centrally deploy protections for your VPCs

*** Now a days many org's use terraform extensively. all of the rules are centrally managed at terraform level. 
 even though we have multiple aws acnts,it doesn't really hastle. the primary challenge where we face is issues related to maintaining rules,if we are not
 maintaining central infra as a service.it can be cloudformation or terraform etc.

AWS SHIELD ( DDOS PROTECTION ) : 
- AWS Shield is a managed Distributed Denial of Service (DDoS) service that safeguards the  workloads running on AWS against DDoS attacks.
- it basically protect our workloads against the Distributed denial of service attacks.

There are two tiers of AWS shield :
- Shield Standard : AWS Shield standard provides basic level protection against most common network and transport layer DDoS attacks.

- Shield Advanced : For a higher level of protection, we can subscribe to the Shield Advanced. Shield Advanced protects against large and sophisticated 
  DDoS attacks with near-real-time visibility into the attacks that might be occurring.
  AWS Shield Advanced also gives customers 24x7 access to the AWS DDoS Response Team (DRT) during ongoing attacks.

let's assume that our org is already goin through a massive ddos attack. we can contact aws drt team , which help in measurements can be taken to protect
 against those attacks.
- AWS Shield Advanced costs 3000$ per organization and requires Business or Enterprise Support.One interesting part about AWS Shield Advanced is that 
  during the attack, if your infrastructure has scaled, AWS will return you the amount occurred during scaling in the form of credits. This is also refer
  red to as Cost protection. amount will be returned for route53,cloudfront, elb ddos atatcks.


VPN ( VIRTUAL PRIVATE NETWORK ) : 
- VPN enables you to route traffic from yourself(your network) towards destination through the vpn server.
- Something similar to Proxy.
ex : we have computer, and destination server in the internet.our computer which is connected to the internet have a unique ip address(192.38.90.03)our 
     computer when connected to the destination server, the destination server will have full access logs related to our connection details.

 in case of vpn server. there is a vpn server(54.89.90.23). when we send the traffic via the vpn to the destination server, the destination server will not
 see my ip address.it will see the ip address of vpn.it is vpn which is routing all the traffic. there are lot of adv's where attacker uses vpn extensivly.

- In Corporate environments, VPN is used to connect to instances in Private Subnet.
- VPN Server resides in the Public Subnet and you route your traffic via VPN server to instances in Public Subnet.
  i have a ec2 in private subnet and it has a private ip.private ip can't be communicated directly via the internet, it is not routable ip.so we put vpn 
  server in the public subnet inside vpc,and we route the traffic from the computer to the vpn,and from vpn towards the private instance.
 if we want to connect to private instance our traffic will be routed from vpn to the private instance.


AWS VPN TUNNELS( SITE TO SITE TUNNEL) : 
- A Site to Site (S2S) VPN allows two networking domains to communicate securely between each other over an untrusted network like Internet.
  two sites can be two diff locations from which we want to communicate securely.(it can be b/w ec2 and datacenter or b/w aws and azure)
- The two sites can be AWS and on-premise data-center or even two different VPC’s.

- once the tunnel is established.we have a ec2 this access the vpn termination,another side we have data center, b/w vpn tunnel is established.
 one of the challenge we face in sitetosite vpn is HA.if we see there is a single tunnel endpoint in each of the sides(we have ec2 which acts as vpn termin
 ation endpoint) if ec2 instance goes down, then the entire tunnel would break.(some time back when aws doesn't have vpc peering , s2s tunnel was common)

- to overcome low availability, org's usually establish multiple tunnels.we have 2 tunnels (active,passive).if one tunnel goes down,we can switch to the 
  passive tunnel for HA.
- there is tunnel istablished b/w mumbai and singapore.we can do this using vpc peering.( if it is aws and azure then we need to use s2s vpn).
s2s vpn architecture: 
PATH : [AWS{VPC[subnet-router]}- virtual private gateway] - vpn connection - customer gateway present in on premises.
- customer gateway can be a firewall or server,it is a vpn termination endpoint on the customer side.
- virtual private gateway : it is not mandatory but it has it's own advantages.here if ec2 instance goes down, then entire tunnel goes down.vpgw is HA.
- A Virtual Private Gateway (VGW) has built-in high-availability for VPN connection
- AWS automatically creates 2 HA endpoints, each in a different AZ.
- vpgw has 2 endpoint ip addresses, and endpoint ip addresses are located in diff AZ.so from customer side we establish 2 vpn tunnel(one to endpoint i1,and
  another to endpoint ip2).together they act as a single vpn connection.


AWS CONTROL TOWER ( AGILITY AND GOVERNANCE ) : 

- Most of the org's follow multi account based architecture.(prod,dev,test,security). when the amount of account increases,it leads to own set of challenge
 Chalenge 1: (identity management). when we have 100's of aws accounts , how will user login to these accounts.will we create a seperate pswrd and username
  for each user in all accounts or we will have  some kind of identity managemnt system. to overcom this we need identity managemnt soln: single sign on

SINGLE SIGN ON : Single sign-on (SSO) is an authentication method that enables users to securely authenticate with multiple applications and websites by 
 using just one set of credentials.
- we have aws sso. user can make use of sso feature to go ahead and login to any of the aws acnts considering that all of them are integrated with the aws
  SSO.

Challenge 2 : (Security hardening).we have 100 aws acnts.in all acnts we need to ensure that there is a req level of securities(enable aws config,aws orgs,
 Service controlled policy,guard duty,cloudtrail logs all should be enabled). doin all this in all 100 acnts is a big challenge.
soln : Security automation : we can write cloudformation template that can enable aws config,enable cloudtrail,guarduty etc.
- AWS CloudFormation StackSets allows you to create, update, or delete stacks across multiple accounts and Regions with a single operation

Challenge 3 : centralized console ; when we have multiple acnts, we need to see which acnt is compliance and which is not compliance.where do we want to
 see compliant and non-compliant of all the acnts ; in a central aws acnt. we login to central aws ,we can see which acnt is compliant and non-compliant.

- to overcome all these challenges we have an aws service control tower.

AWS CONTROL TOWER : AWS CloudFormation StackSets allows you to create, update, or delete stacks across multiple acnts and Regions with a single operation
- behind the scenes it makes use of multiple services(aws organizations, aws sso, cfn stacks,etc). it uses them to configure a acnt.suppose we have dev acn
  control tower make sure that dev acnt is part of aws org's,also check dev is associated with aws sso, cfn stacks etc.
- tmrw if we want to create a new acnt, we can easily create new acnt through control tower.if we need prod acnt, tower create the acnt and prod gets integ
  rates with all the services assocuated with control tower.
- generally in a multi acnt based architecture, the logs associated with the service, we will not be able to store them in the individual acnt itself.the
  best practise states that we need to have a central logging acnt and all of the logs from other aws acnts need to store it in centralaized login acnt.



AMAZON COGNITO (federation): Amazon Cognito provides authentication, authorization, and user management service for your web and mobile apps.

Use case:Alice is a mobile developer in a start-up organization. They have begun with mobile wallet system, and there are specific requirements as follows:
● Users should be able to sign-up with new credentials.
● User should be able to sign-in with social platforms like FB, Twitter, Google.
● There should be a post sign-up process (one-time password) for verification.
● Multi-Factor authentication should be present.
● Account recovery feature should be present.
- the functionalities that we are discussing , it is not limited to start-up org. all most all org will need this kind of functionality.let's say there are
  100 org's.will 100 org's develop these functionalities from scartch,if they do, then this is not best way to utilize time
- above case alice not only need to build a app, she also have to build complete authorization and authentication system. even many org's need this similia
  kind of functionalities. 
- so instead of developing entire code from scratch,alice can make use of the cognito service which basically provides entire authentication,autherazation
  , usermanagemnt service for mobile appl's.

Demo : go to cognito. already a user pool is created. inside that we can see pswrd policies, mfa,

At a high level, there are two major features under Amazon Cognito
i) User Pools : Cognito user pool takes care of the entire authentication, authorization process
ii) Identity Pools : Identity pool provides the functionality of federation for users in user pools.

- in diagram i am on left side. and we have mobile appl.the user can go and perform the authentication through userpool.once the authentication is complete
  they basically get a token.this taken can be exchanged with the identity pool.once succefully authenticated with the identity pool, we get the aws 
  crdentails like access, secret keys through which we can go ahead and connect to variou services in AWS.

Cognito Identity pools also referred to as AWS Cognito Federated Identities allows developers to authorize the users of the application to use various AWS
  services.

Use-Case:We have a quiz based mobile application. At the end of quiz, user’s results should be stored in the DynamoDB table.
- for the mobile appl to store the data in dynamodb table,some kind of access or secret keys is needed.this not good approach, because it is hardcoding
If we hard-code the access/secret keys, chances of reverse engineering are high. here we can make use identity pool where once the user logs into the 
appl,the appl can make use of identity pool to get the access and secret keys.and that keys can be used for the overall access for other aws services,


Q :

- The developers have used Amazon Cognito for authentication, authorization, and user management. Due to the sensitivity of the data, there is a 
 requirement to add another method of authentication in addition to a username and password:  Use multi-factor authentication (MFA) with a Cognito user pool

- A Python application is currently running on Amazon ECS containers using the Fargate launch type.An ALB has been created with a Target Group that routes
   incoming connections to the ECS-based application. The application will be used by consumers who will authenticate using federated OIDC compliant 
   Identity Providers such as Google and Facebook. The users must be securely authenticated on the front-end before they access the secured portions of the
   application :It can be done on the ALB by creating an authentication action on a listener rule that configures an Aws Cognito user pool with the 
   social IdP

  ALB supports authentication from OIDC compliant identity providers such as Google, Facebook and Amazon. It is implemented through an authentication 
  action on a listener rule that integrates with Amazon Cognito to create user pools


BASTION HOST(TIME TO DEFEND) : 
- the org's which uses compliances they use bastion host.
- Bastion hosts also referred as jump box  or jump server which acts like a proxy server and allows the client machines to connect to the remote server in
  the private subnets.
arch : we have multiple servers in private subnet .and we have client in the internet.client cannot directly connect to the servers in the private subnet.
       they will first connect to the bastion server which is in public subnet.and from bastion server they can go ahead to any other server that are part
       of private subnet depending upon the policy we set at a bastion level.
Challenges with this arch is where bastion server is used to connect to all other private ec2's is related to authentication at a bastion and private ec2
level.
 ex : users have the private keys stored at their laptop.now they want to connect to ec2 instance in the private subnet.first they will connect to bastion,
      since the keys are in laptop itself, the connection to bastion is succesful.now from the bastion server they want to connect to a ec2 instance.since
      the private keys are not at abstion server ,connection to private ec2 will not really work.many org's store private keys of every user in bastion ser
      ver(this is a big no-no).incase if bastion server is compromised, essentially all of the keys that are stored at bastion server will also be compromi
      sed,this thing we really don't want.
Q.how will we login from bastion to private ec2 instances? without the keys being transported at a bastion server or without storing at bastion server?
** This is where ssh agent forwarding concept comes.
- SSH Agent forwarding allows users to use their local SSH keys to perform some operation on remote servers without keys being left from your workstation.

- here clients connect to the bastion server. and the keys remains at the workstation(laptop of client) of the client.From the bastion server considering 
  that the client has access to private ec2 instances,they will be able to connect to the private instances successfully even without keys leaving the 
  workstation of client.

DEMO : launch 3 instances(client,bastion host, private instance).
- the client to connect to private ec2 instance , the cleunt need to have permission to both bastion and private ec2. first we need to create a private key
  that private key will have permission to bastion and private.so now connect to client machine and run "ssh-keygen" command to create a private key.
-  now run"ls -l ~/.ssh" . here we can see list of public and private keys. at this stage if we connect to any bastion or private ec2 the connection will 
   fail.because we don't have any entry in authorize keys. 
- so in client ec2 run" cd ~/.ssh " open ssh directory. run "ls " we can see keys.No open public key file by running" cat id_rsa.pub" it will show publckey
  copy the key and add it in both bastion and private as part of authorize keys.
- we added public keys in both bastion and private of authorized keys. any client who is trying to connect to the ec2 instances with one of the authorized
  keys whic is public key the associated private key of that public key would be allowed.







***********************************************************************************************************************************************************

IMPORTANT POINTS :

***********************************************************************************************************************************************************

- VPC Endpoint Services :
 Allows customers to host their own AWS PrivateLink powered service, known as an endpoint service, and share it with other AWS customers.
ex : 
- before endpoint services we discuss about vpc endpoint which allows us to route the traffic to supported services like s3 and others.but what about custo
  m appl's.incase if we have a custom applications where want our traffic to routed this is where vpc endpoint services comes.
diagram : we have endpoint service provider on right side. and left side we have  service consumer it has vpc endpoint. now vpc endpoint can connect to the
          endpoinyt service provider which is hosting the endpoint services. 
 exam tip : keyword : External provider.

- For use-case were consistent dedicated throughput is required, DX is the right answer









































